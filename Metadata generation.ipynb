{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aa89a33",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73be6f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This basic example demostrate the LLM response and ChatModel Response\n",
    "\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "#app.py\n",
    "\n",
    "from langchain import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "482a586d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370e4756680d40a9978934a4f8af3ed9 Azure https://testopenaisaturday.openai.azure.com/ 2023-10-01-preview\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from the .env file\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# Retrieve Azure OpenAI specific configuration from environment variables\n",
    "OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "OPENAI_API_TYPE = \"Azure\"\n",
    "OPENAI_API_BASE = \"https://testopenaisaturday.openai.azure.com/\"\n",
    "OPENAI_API_VERSION = \"2023-10-01-preview\"\n",
    "\n",
    "print(OPENAI_API_KEY, OPENAI_API_TYPE,OPENAI_API_BASE, OPENAI_API_VERSION )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "227e303f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the OpenAI library configuration using the retrieved environment variables\n",
    "openai.api_type = \"Azure\"\n",
    "openai.api_base = \"https://testopenaisaturday.openai.azure.com/\"\n",
    "openai.api_version = \"2023-10-01-preview\"\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "\n",
    "llm = AzureChatOpenAI( \n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    azure_endpoint=OPENAI_API_BASE,\n",
    "    openai_api_version=OPENAI_API_VERSION,\n",
    "    deployment_name=\"assistantPreviewSaturday\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14270711",
   "metadata": {},
   "outputs": [],
   "source": [
    "#app.py\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "from langchain.chat_models import AzureChatOpenAI \n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "790833a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "llm_temp = AzureChatOpenAI( \n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    azure_endpoint=OPENAI_API_BASE,\n",
    "    openai_api_version=OPENAI_API_VERSION,\n",
    "    deployment_name=\"assistantPreviewSaturday\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4b41a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_unstructured.unit_utils import assert_round_trips_through_JSON, example_doc_path\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "from unstructured.documents.elements import ElementType\n",
    "\n",
    "#!pip install unstructured_inference\n",
    "#!pip install -U langchain-unstructured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2104d7a",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6590647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "# Open the PDF file\n",
    "with open('s3.pdf', 'rb') as file:\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    \n",
    "    # Iterate through all the pages and extract text\n",
    "    text = ''\n",
    "    for page_num in range(len(reader.pages)):\n",
    "        page = reader.pages[page_num]\n",
    "        text += page.extract_text()\n",
    "\n",
    "# Print the extracted text\n",
    "#print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "527c6a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document  # Import the Document class\n",
    "\n",
    "# Step 1: Extract text from the PDF using PyPDF2\n",
    "with open('s3.pdf', 'rb') as file:\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    \n",
    "    # Iterate through all the pages and extract text\n",
    "    text = ''\n",
    "    for page_num in range(len(reader.pages)):\n",
    "        page = reader.pages[page_num]\n",
    "        text += page.extract_text()\n",
    "\n",
    "# Step 2: Create a Document object\n",
    "document = Document(page_content=text)\n",
    "\n",
    "# Step 3: Use CharacterTextSplitter to split the extracted text into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=800, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents([document])  # Pass in a list of Document objects\n",
    "\n",
    "# Step 4: Output the split text chunks\n",
    "for chunk in texts:\n",
    "    print(chunk.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8cfb87d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 2405\n",
      "Chunk 1:\n",
      "User Guide\n",
      "Amazon Simple Storage Service\n",
      "API Version 2006-03-01\n",
      "Copyright © 2024 Amazon Web Services, Inc. and/or its aﬃliates. All rights reserved.Amazon Simple Storage Service User Guide\n",
      "Amazon Simple Storage Service: User Guide\n",
      "Copyright © 2024 Amazon Web Services, Inc. and/or its aﬃliates. All rights reserved.\n",
      "Amazon's trademarks and trade dress may not be used in connection with any product or service \n",
      "that is not Amazon's, in any manner that is likely to cause confusion among customers, or\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Chunk 2:\n",
      "Analytics and insights............................................................................................................................5\n",
      "Strong consistency..................................................................................................................................5\n",
      "How Amazon S3 works...............................................................................................................................5\n",
      "Buckets...............................................\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Chunk 3:\n",
      "Amazon S3 data consistency model......................................................................................................10\n",
      "Concurrent applications.......................................................................................................................11\n",
      "Related services..........................................................................................................................................12\n",
      "Accessing Amazon S3..........................................\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document  # Import the Document class\n",
    "\n",
    "# Step 1: Extract text from the PDF using PyPDF2\n",
    "with open('s3.pdf', 'rb') as file:\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    \n",
    "    # Iterate through all the pages and extract text\n",
    "    text = ''\n",
    "    for page_num in range(len(reader.pages)):\n",
    "        page = reader.pages[page_num]\n",
    "        extracted_text = page.extract_text()\n",
    "        if extracted_text:  # Ensure the text extraction is successful\n",
    "            text += extracted_text\n",
    "\n",
    "# Step 2: Create a Document object\n",
    "document = Document(page_content=text)\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Step 3: Use RecursiveCharacterTextSplitter for better splitting\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000, \n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \"]\n",
    ")\n",
    "\n",
    "# Split the document text into smaller chunks\n",
    "texts = text_splitter.split_documents([document])  # Pass in a list of Document objects\n",
    "\n",
    "# Step 4: Print the number of chunks\n",
    "print(f\"Number of chunks: {len(texts)}\")\n",
    "\n",
    "# Step 5: Optionally, output the first few chunks to inspect the splitting\n",
    "for i, chunk in enumerate(texts[:3]):  # Limit to first 3 chunks for display\n",
    "    print(f\"Chunk {i+1}:\")\n",
    "    print(chunk.page_content[:500])  # Print first 500 characters of the chunk\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7d4845da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic Chunker\n",
    "import PyPDF2\n",
    "from langchain.schema import Document  # Import the Document class\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Step 1: Extract text from the PDF using PyPDF2\n",
    "with open('s3.pdf', 'rb') as file:\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    \n",
    "    # Iterate through all the pages and extract text\n",
    "    text = ''\n",
    "    for page_num in range(len(reader.pages)):\n",
    "        page = reader.pages[page_num]\n",
    "        extracted_text = page.extract_text()\n",
    "        if extracted_text:  # Ensure the text extraction is successful\n",
    "            text += extracted_text\n",
    "\n",
    "# Step 2: Create a Document object\n",
    "document = Document(page_content=text)\n",
    "\n",
    "# Initialize the OpenAI embeddings model (required for SemanticChunker)\n",
    "embeddings_model = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    api_key=\"YOUR_API_KEY\"  # Replace with your OpenAI API key\n",
    ")\n",
    "\n",
    "# Step 3: Use SemanticChunker for semantic splitting\n",
    "text_splitter = SemanticChunker(embeddings_model, breakpoint_threshold_type=\"percentile\")\n",
    "\n",
    "# Split the document text into semantically meaningful chunks\n",
    "texts = text_splitter.create_documents([document.page_content])\n",
    "\n",
    "# Step 4: Print the number of chunks\n",
    "print(f\"Number of semantic chunks: {len(texts)}\")\n",
    "\n",
    "# Step 5: Optionally, output the first few chunks to inspect the splitting\n",
    "for i, chunk in enumerate(texts[:3]):  # Limit to first 3 chunks for display\n",
    "    print(f\"Chunk {i+1}:\")\n",
    "    print(chunk.page_content[:500])  # Print first 500 characters of each chunk\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "24e30841",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete directories, and it does not support symbolic links or ﬁle locking. Mountpoint is ideal \n",
      "for applications that do not need all of the features of a shared ﬁle system and POSIX-style \n",
      "permissions but require Amazon S3's elastic throughput to read and write large S3 datasets. For \n",
      "details, see Mountpoint ﬁle system behavior on GitHub. For workloads that require full POSIX \n",
      "support, we recommend Amazon FSx for Lustre and its support for linking S3 buckets.\n",
      "Mountpoint for Amazon S3 is availab\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "wget download-link\n",
      "3. (Optional) Verify the authenticity and integrity of the downloaded ﬁle. First, copy the \n",
      "appropriate signature URL for your architecture.\n",
      "x86_64 :\n",
      "https://s3.amazonaws.com/mountpoint-s3-release/latest/x86_64/mount-s3.rpm.asc\n",
      "Installing Mountpoint API Version 2006-03-01 85Amazon Simple Storage Service User Guide\n",
      "ARM64 (Graviton) :\n",
      "https://s3.amazonaws.com/mountpoint-s3-release/latest/arm64/mount-s3.rpm.asc\n",
      "Next, see Verifying the signature of the Mountpoint for Amazon S3 pac\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Output only the first 2-3 split text chunks with truncation to avoid large data output\n",
    "for chunk in texts[125:127]:  # Display the first 3 chunks\n",
    "    print(chunk.page_content[:500])  # Print only the first 500 characters of each chunk\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95581b58",
   "metadata": {},
   "source": [
    "## Metadata Gen : Topic Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2f10ba47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the function to work with strings (plain text chunks) and generate 2-3 tags\n",
    "def generate_metadata_for_chunks(llm, chunks, max_tokens=2048):\n",
    "    results = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # Truncate the chunk if it exceeds the max_tokens limit\n",
    "        truncated_chunk = chunk[:max_tokens]  # Now 'chunk' is a string, so we handle it directly\n",
    "        \n",
    "        # Create a prompt that requests the LLM to generate only the most relevant 2-3 tags and headings\n",
    "        prompt = f\"\"\"\n",
    "        The following text is a chunk from a larger document. Please generate the 2-3 most relevant topic headings and tags for this text:\n",
    "        Text: \"{truncated_chunk}\"\n",
    "        \"\"\"\n",
    "        \n",
    "        # Call the LLM with the prompt using the invoke method\n",
    "        response = llm.invoke([{\n",
    "            \"role\": \"system\",\n",
    "            \"content\": prompt\n",
    "        }])\n",
    "        \n",
    "        # Store the result (response with tags and headings)\n",
    "        results.append({\n",
    "            \"chunk_index\": i,\n",
    "            \"chunk_text\": truncated_chunk,\n",
    "            \"metadata\": response\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Display the results with truncated chunk text and the most relevant metadata\n",
    "def display_metadata_results(results):\n",
    "    for result in results:\n",
    "        print(f\"Chunk Index: {result['chunk_index']}\")\n",
    "        print(\"Chunk Text (truncated):\")\n",
    "        print(result['chunk_text'][:1000])  # Display only the first 1000 characters of the chunk text\n",
    "        print(\"\\nGenerated Metadata:\")\n",
    "        \n",
    "        # Extract and display the topic headings and tags from the metadata\n",
    "        metadata_content = result['metadata'].content\n",
    "        \n",
    "        # Format the metadata output\n",
    "        print(f\"{metadata_content}\")\n",
    "        print(\"\\n\" + \"-\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4e6ce296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk Index: 0\n",
      "Chunk Text (truncated):\n",
      "\"s3:GetObject\" \n",
      "         ], \n",
      "         \"Resource\": [ \n",
      "            \"arn:aws:s3::: amzn-s3-demo-bucket1 /*\" \n",
      "         ] \n",
      "      }   \n",
      "   ]\n",
      "}\n",
      "Step 2: Do the Account B tasks\n",
      "Now that Account B has permissions to perform operations on Account A's bucket, the Account B \n",
      "administrator does the following:\n",
      "•Uploads an object to Account A's bucket\n",
      "•Adds a grant in the object ACL to allow Account A, the bucket owner, full control\n",
      "Walkthroughs using policies API Version 2006-03-01 1010Amazon Simple Storage Service User Guide\n",
      "Using the AWS CLI\n",
      "1. Using the put-object  AWS CLI command, upload an object. The --body  parameter in the \n",
      "command identiﬁes the source ﬁle to upload. For example, if the ﬁle is on the C: drive of a \n",
      "Windows machine, specify c:\\HappyFace.jpg . The --key parameter provides the key name \n",
      "for the object.\n",
      "aws s3api put-object --bucket amzn-s3-demo-bucket1  --key HappyFace.jpg --body \n",
      " HappyFace.jpg --profile AccountBadmin\n",
      "2. Add a grant to the object ACL to allow the bucket owner fu\n",
      "\n",
      "Generated Metadata:\n",
      "Topic Headings:\n",
      "- Granting Permissions to Access an S3 Bucket Across AWS Accounts\n",
      "- Testing Access Permissions for an S3 Object Across AWS Accounts\n",
      "\n",
      "Tags:\n",
      "- AWS CLI\n",
      "- S3 Bucket\n",
      "- Access Control\n",
      "- AWS Accounts \n",
      "- Permissions\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to pick a specific chunk by index\n",
    "def pick_chunks_by_index(indices, texts):\n",
    "    selected_chunks = [texts[i].page_content for i in indices if i < len(texts)]\n",
    "    return selected_chunks\n",
    "\n",
    "# Example usage: pick chunk(s) by index\n",
    "indices = [1004]  # You can manually update this list with the indices of the chunks you want to select\n",
    "\n",
    "# Get the selected chunks based on the provided indices\n",
    "selected_chunks = pick_chunks_by_index(indices, texts)\n",
    "\n",
    "# Generate metadata for the selected chunks\n",
    "results = generate_metadata_for_chunks(llm, selected_chunks)\n",
    "\n",
    "# Output the results to see how it works\n",
    "display_metadata_results(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7c4988",
   "metadata": {},
   "source": [
    "## Embedings : Split and add metadata with chunks to Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d8f6fa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Assuming `generate_metadata_for_chunks` gives us topics and tags for each chunk\n",
    "def create_documents_with_metadata(chunks, metadata):\n",
    "    documents = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        doc_metadata = {\n",
    "            \"topics\": metadata[i].get(\"topics\", []),  # Assuming metadata contains 'topics'\n",
    "            \"tags\": metadata[i].get(\"tags\", [])  # Assuming metadata contains 'tags'\n",
    "        }\n",
    "        document = Document(page_content=chunk.page_content, metadata=doc_metadata)\n",
    "        documents.append(document)\n",
    "    return documents\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "648b299b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the PDF document (replace with your own PDF file)\n",
    "directory = os.getcwd()\n",
    "file_name = directory + \"/s3.pdf\"  # Example file name\n",
    "loader = PyPDFLoader(file_name)\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# Generate metadata for each chunk (topics and tags)\n",
    "metadata = generate_metadata_for_chunks(llm, [page.page_content for page in pages])\n",
    "\n",
    "# Create documents with metadata\n",
    "documents = create_documents_with_metadata(pages, metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "014cb6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "\n",
    "# Initialize the embeddings model using Azure OpenAI\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    deployment = \"AzureAdaLangchain\",\n",
    "    model = \"text-embedding-ada-002\",\n",
    "    api_key= \"370e4756680d40a9978934a4f8af3ed9\",\n",
    "    openai_api_version=\"2023-10-01-preview\",\n",
    "    azure_endpoint=\"https://testopenaisaturday.openai.azure.com/\",\n",
    "    openai_api_type=\"azure\",\n",
    "    chunk_size=1\n",
    ")\n",
    "\n",
    "# Use FAISS to index the document chunks along with their metadata\n",
    "db = FAISS.from_documents(documents=documents, embedding=embeddings)\n",
    "\n",
    "# Save the FAISS index locally\n",
    "db.save_local(directory + \"/faiss_index_s3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b0bc68",
   "metadata": {},
   "source": [
    "## Integrate LLM with embedded DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f047b7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = AzureChatOpenAI( \n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    azure_endpoint=OPENAI_API_BASE,\n",
    "    openai_api_version=OPENAI_API_VERSION,\n",
    "    deployment_name=\"varelabsAssistant\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Load the FAISS vector store\n",
    "vector_store = FAISS.load_local(directory + \"/faiss_index_s3\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "# Set up a retriever using similarity search\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a6db6d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Create the prompt for condensing the question based on chat history\n",
    "CONDENSE_PROMPT = PromptTemplate.from_template(\"\"\"\n",
    "You are an assistant helping to condense a follow-up question in a conversation about AWS S3 installation and troubleshooting. Use the chat history to rephrase the user's follow-up question into a standalone question that includes references to any specific issues or configurations mentioned.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow-Up Input: {question}\n",
    "Standalone Question:\n",
    "\"\"\")\n",
    "\n",
    "# Create the prompt for generating the answer based on the documentation\n",
    "QA_PROMPT = PromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant providing guidance on AWS S3 installation and troubleshooting. Use the provided documentation to give accurate, concise answers based on the user's questions. Use the conversation history to understand the user's current context and refer to any mentioned topics or configurations.\n",
    "\n",
    "Documentation Context:\n",
    "{context}\n",
    "\n",
    "Conversation History:\n",
    "{chat_history}\n",
    "\n",
    "User's Question: {question}\n",
    "Assistant's Response:\n",
    "\"\"\")\n",
    "\n",
    "# Initialize the Conversational Retrieval Chain with the LLM and retriever\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    condense_question_prompt=CONDENSE_PROMPT,\n",
    "    combine_docs_chain_kwargs={'prompt': QA_PROMPT},\n",
    "    return_source_documents=True,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Function to ask a question and maintain conversation context\n",
    "def ask_question_with_context(qa, question, chat_history):\n",
    "    result = qa({\"question\": question, \"chat_history\": chat_history})\n",
    "    print(\"Assistant's Response:\", result[\"answer\"])\n",
    "    chat_history.append((question, result[\"answer\"]))\n",
    "    return chat_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3e262f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty chat history\n",
    "chat_history = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dab76ed4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prana\\AppData\\Local\\Temp\\ipykernel_16948\\2120289748.py:40: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  result = qa({\"question\": question, \"chat_history\": chat_history})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant's Response: To set up an S3 bucket on AWS, follow these steps:\n",
      "\n",
      "1. **Sign in to the AWS Management Console**:\n",
      "   - Go to the Amazon S3 console at [https://console.aws.amazon.com/s3/](https://console.aws.amazon.com/s3/).\n",
      "\n",
      "2. **Choose the AWS Region**:\n",
      "   - In the navigation bar at the top of the page, select the name of the currently displayed AWS Region.\n",
      "   - Choose the Region where you want to create the bucket. Selecting a Region close to you can help minimize latency and costs and address regulatory requirements.\n",
      "\n",
      "3. **Navigate to Buckets**:\n",
      "   - In the left navigation pane, click on **Buckets**.\n",
      "\n",
      "4. **Create a Bucket**:\n",
      "   - Click on **Create bucket**.\n",
      "   - The Create bucket page will open.\n",
      "\n",
      "5. **Configure Bucket Settings**:\n",
      "   - **Bucket name**: Enter a name for your bucket (e.g., `tutorial-bucket`). Ensure that the name is unique and adheres to the [Bucket naming rules](https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.html).\n",
      "   - **Region**: Select the AWS Region where you want the bucket to reside.\n",
      "   - **Block Public Access settings for this bucket**: Keep the default settings, which are enabled to block all public access. This is recommended for security purposes unless your use case requires otherwise.\n",
      "   - For the remaining settings, you can keep the defaults. If needed, you have the option to configure additional settings according to your specific use case.\n",
      "\n",
      "6. **Create the Bucket**:\n",
      "   - Click on **Create bucket** to finalize the creation of your S3 bucket.\n",
      "\n",
      "Once the bucket is created, you can start uploading objects, such as video files, to it. If you need detailed guidance on uploading files, please let me know!\n",
      "Assistant's Response: If you are encountering a permission error while accessing your S3 bucket, there could be several reasons for this issue. Here are some steps to troubleshoot and resolve the permission error:\n",
      "\n",
      "1. **Review Block Public Access Settings**:\n",
      "   - Ensure that your S3 bucket's public access settings are correctly configured.\n",
      "   - Go to the [Amazon S3 console](https://console.aws.amazon.com/s3/).\n",
      "   - Select your bucket.\n",
      "   - Choose **Permissions**.\n",
      "   - Under **Block public access (bucket settings)**, click **Edit**.\n",
      "   - Ensure that the **Block all public access** setting is configured according to your requirements. If you need public access to your bucket, you may need to clear this setting, but be aware of the security risks involved.\n",
      "\n",
      "2. **Bucket Policy**:\n",
      "   - A bucket policy might be required to grant the necessary permissions.\n",
      "   - In the **Permissions** tab of your bucket, scroll down to the **Bucket policy** section.\n",
      "   - Ensure that the bucket policy grants the required permissions. Here is an example of a bucket policy that allows public read access to all objects in the bucket:\n",
      "     ```json\n",
      "     {\n",
      "       \"Version\": \"2012-10-17\",\n",
      "       \"Statement\": [\n",
      "         {\n",
      "           \"Effect\": \"Allow\",\n",
      "           \"Principal\": \"*\",\n",
      "           \"Action\": \"s3:GetObject\",\n",
      "           \"Resource\": \"arn:aws:s3:::your-bucket-name/*\"\n",
      "         }\n",
      "       ]\n",
      "     }\n",
      "     ```\n",
      "   - Replace `your-bucket-name` with the name of your bucket.\n",
      "\n",
      "3. **IAM User/Role Permissions**:\n",
      "   - Make sure that the IAM user or role you are using has the necessary permissions to access the bucket.\n",
      "   - Go to the [IAM console](https://console.aws.amazon.com/iam/).\n",
      "   - Check the policies attached to your IAM user or role and ensure they include permissions like `s3:ListBucket`, `s3:GetObject`, etc. Here is an example policy:\n",
      "     ```json\n",
      "     {\n",
      "       \"Version\": \"2012-10-17\",\n",
      "       \"Statement\": [\n",
      "         {\n",
      "           \"Effect\": \"Allow\",\n",
      "           \"Action\": [\n",
      "             \"s3:ListBucket\",\n",
      "             \"s3:GetObject\"\n",
      "           ],\n",
      "           \"Resource\": [\n",
      "             \"arn:aws:s3:::your-bucket-name\",\n",
      "             \"arn:aws:s3:::your-bucket-name/*\"\n",
      "           ]\n",
      "         }\n",
      "       ]\n",
      "     }\n",
      "     ```\n",
      "\n",
      "4. **CORS Configuration**:\n",
      "   - If you are accessing the bucket from a web application, ensure that you have configured the Cross-Origin Resource Sharing (CORS) settings.\n",
      "   - In the **Permissions** tab of your bucket, scroll down to the **CORS configuration** section.\n",
      "   - Add a CORS configuration similar to the following if needed:\n",
      "     ```xml\n",
      "     <CORSConfiguration>\n",
      "       <CORSRule>\n",
      "         <AllowedOrigin>*</AllowedOrigin>\n",
      "         <AllowedMethod>GET</AllowedMethod>\n",
      "         <MaxAgeSeconds>3000</MaxAgeSeconds>\n",
      "         <AllowedHeader>*</AllowedHeader>\n",
      "       </CORSRule>\n",
      "     </CORSConfiguration>\n",
      "     ```\n",
      "\n",
      "If you have reviewed and applied the above configurations and still face issues, please provide more details about the specific permission error message you are encountering, and I can assist you further.\n",
      "Assistant's Response: To resolve permission errors for your S3 bucket, you need to configure several access policies and settings. Here's a comprehensive guide covering Block Public Access, Bucket Policy, IAM User/Role Permissions, and CORS configuration:\n",
      "\n",
      "### 1. Block Public Access Settings\n",
      "1. Sign in to the [Amazon S3 console](https://console.aws.amazon.com/s3/).\n",
      "2. Select your bucket.\n",
      "3. Go to the **Permissions** tab.\n",
      "4. Under **Block public access (bucket settings)**, click **Edit**.\n",
      "5. Configure the settings according to your requirements. For example:\n",
      "   - To allow public access, turn off **Block all public access**.\n",
      "   - To restrict public access, ensure that **Block all public access** is turned on.\n",
      "\n",
      "### 2. Bucket Policy\n",
      "To grant permissions via a bucket policy, follow these steps:\n",
      "1. In the **Permissions** tab of your bucket, scroll down to the **Bucket policy** section.\n",
      "2. Click **Edit** to add or modify the bucket policy.\n",
      "3. Example of a bucket policy that grants public read access to all objects in the bucket:\n",
      "   ```json\n",
      "   {\n",
      "     \"Version\": \"2012-10-17\",\n",
      "     \"Statement\": [\n",
      "       {\n",
      "         \"Effect\": \"Allow\",\n",
      "         \"Principal\": \"*\",\n",
      "         \"Action\": \"s3:GetObject\",\n",
      "         \"Resource\": \"arn:aws:s3:::your-bucket-name/*\"\n",
      "       }\n",
      "     ]\n",
      "   }\n",
      "   ```\n",
      "   Replace `your-bucket-name` with the name of your bucket.\n",
      "4. Ensure there are no explicit **Deny** statements that could be causing the permission issue.\n",
      "\n",
      "### 3. IAM User/Role Permissions\n",
      "Ensure the IAM user or role has the necessary permissions:\n",
      "1. Sign in to the [IAM console](https://console.aws.amazon.com/iam/).\n",
      "2. Select the IAM user or role you are using.\n",
      "3. Attach or update the policy to include necessary permissions. Example policy:\n",
      "   ```json\n",
      "   {\n",
      "     \"Version\": \"2012-10-17\",\n",
      "     \"Statement\": [\n",
      "       {\n",
      "         \"Effect\": \"Allow\",\n",
      "         \"Action\": [\n",
      "           \"s3:ListBucket\",\n",
      "           \"s3:GetObject\"\n",
      "         ],\n",
      "         \"Resource\": [\n",
      "           \"arn:aws:s3:::your-bucket-name\",\n",
      "           \"arn:aws:s3:::your-bucket-name/*\"\n",
      "         ]\n",
      "       }\n",
      "     ]\n",
      "   }\n",
      "   ```\n",
      "4. Ensure there are no explicit **Deny** statements in the IAM policy that could be causing the issue.\n",
      "\n",
      "### 4. CORS Configuration\n",
      "If accessing the bucket from a web application, configure CORS settings:\n",
      "1. In the **Permissions** tab of your bucket, scroll down to the **CORS configuration** section.\n",
      "2. Click **Edit** to add or modify the CORS configuration.\n",
      "3. Example CORS configuration:\n",
      "   ```xml\n",
      "   <CORSConfiguration>\n",
      "     <CORSRule>\n",
      "       <AllowedOrigin>*</AllowedOrigin>\n",
      "       <AllowedMethod>GET</AllowedMethod>\n",
      "       <MaxAgeSeconds>3000</MaxAgeSeconds>\n",
      "       <AllowedHeader>*</AllowedHeader>\n",
      "     </CORSRule>\n",
      "   </CORSConfiguration>\n",
      "   ```\n",
      "   Modify the `<AllowedOrigin>` and `<AllowedMethod>` values according to your requirements.\n",
      "\n",
      "### Additional Checks\n",
      "- If the object is owned by an external account, ensure access is granted through object ACLs.\n",
      "- Confirm the bucket name in your bucket policy matches your actual bucket name.\n",
      "- If the bucket uses the **Bucket owner enforced** setting for S3 Object Ownership, use policies exclusively to manage access. Attempts to set or update ACLs will fail.\n",
      "\n",
      "By ensuring these configurations are correctly set up, you should be able to resolve most permission errors related to accessing your S3 bucket. If you encounter specific error messages, please provide those details for further assistance.\n",
      "Assistant's Response: To enable versioning on your S3 bucket, follow these steps:\n",
      "\n",
      "### Step 1: Identify Buckets That Have S3 Versioning Enabled\n",
      "1. **Sign in to the AWS Management Console**:\n",
      "   - Go to the Amazon S3 console at [https://console.aws.amazon.com/s3/](https://console.aws.amazon.com/s3/).\n",
      "\n",
      "2. **Navigate to Storage Lens**:\n",
      "   - In the navigation pane, choose **Storage Lens**, then select **Dashboards**.\n",
      "   - Choose the name of the dashboard that you want to view.\n",
      "\n",
      "3. **View Versioning-Enabled Buckets**:\n",
      "   - In the **Trends and distributions** section, choose **Versioning-enabled bucket count** for the primary metric and **Buckets** for the secondary metric.\n",
      "   - This will display trends for S3 Versioning enabled buckets.\n",
      "\n",
      "4. **Analyze Specific Buckets**:\n",
      "   - Choose a point on the trend chart and select a dimension (e.g., Account, AWS Region, Storage class, or Bucket) for deeper analysis.\n",
      "   - In the **Bubble analysis by buckets for date** section, select the **Versioning-enabled bucket count**, **Buckets**, and **Active buckets** metrics.\n",
      "\n",
      "### Step 2: Enable S3 Versioning\n",
      "Once you have identified the buckets, you can enable versioning if it is not already enabled.\n",
      "\n",
      "1. **Navigate to Your Bucket**:\n",
      "   - In the Amazon S3 console, go to **Buckets**.\n",
      "   - Select the bucket for which you want to enable versioning.\n",
      "\n",
      "2. **Enable Versioning**:\n",
      "   - Go to the **Properties** tab.\n",
      "   - Scroll down to the **Bucket Versioning** section.\n",
      "   - Click **Edit**.\n",
      "   - Select **Enable** and then click **Save changes**.\n",
      "\n",
      "### Step 3: Confirm Versioning Is Enabled\n",
      "1. **Verify Versioning Status**:\n",
      "   - Return to the **Properties** tab of your bucket.\n",
      "   - Under **Bucket Versioning**, ensure that the status is **Enabled**.\n",
      "\n",
      "### Additional Information\n",
      "- After versioning is enabled, every object stored in the bucket will have a unique version ID.\n",
      "- You can add objects to a versioning-enabled bucket using the console, AWS SDKs, and REST API.\n",
      "\n",
      "For more detailed instructions, you can refer to the section \"Enabling versioning on buckets\" in the [Amazon Simple Storage Service User Guide](https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html).\n",
      "\n",
      "By enabling versioning, you can keep track of changes to objects over time, recover from unintended user actions and application failures, and retain multiple versions of objects.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example questions relevant to AWS S3 documentation, installation, and troubleshooting\n",
    "question = \"How do I set up an S3 bucket on AWS?\"\n",
    "chat_history = ask_question_with_context(qa, question, chat_history)\n",
    "\n",
    "question = \"I'm getting a permission error while accessing my S3 bucket. How can I fix this?\"\n",
    "chat_history = ask_question_with_context(qa, question, chat_history)\n",
    "\n",
    "question = \"How do I configure access policies for S3?\"\n",
    "chat_history = ask_question_with_context(qa, question, chat_history)\n",
    "\n",
    "question = \"How do I enable versioning on my S3 bucket?\"\n",
    "chat_history = ask_question_with_context(qa, question, chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cd721d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0059cbea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
