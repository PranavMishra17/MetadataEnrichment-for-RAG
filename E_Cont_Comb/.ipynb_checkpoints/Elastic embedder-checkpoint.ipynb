{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef531a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prana\\anaconda3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# Notebook 1: Setup and Data Loading\n",
    "import json\n",
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import PyPDF2\n",
    "import spacy\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from pathlib import Path\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class FAISSEmbeddingManager:\n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2', base_path: str = 'NEW_faiss'):\n",
    "        \"\"\"\n",
    "        Initialize the FAISS Embedding Manager\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.base_path = Path(base_path)\n",
    "        self.base_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        logging.info(f\"Loading model: {model_name}\")\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.dimension = self.model.get_sentence_embedding_dimension()\n",
    "        \n",
    "        self.index = faiss.IndexFlatL2(self.dimension)\n",
    "        self.chunk_metadata = {}\n",
    "        self.current_id = 0\n",
    "        \n",
    "        # Add NLP and vectorizer\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            max_features=100,\n",
    "            stop_words='english',\n",
    "            min_df=1\n",
    "        )\n",
    "    \n",
    "    def load_chunks(self, json_path: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Load chunks from JSON file with proper UTF-8 encoding\n",
    "        \"\"\"\n",
    "        logging.info(f\"Loading chunks from {json_path}\")\n",
    "        try:\n",
    "            with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            logging.info(f\"Successfully loaded {len(data['chunks'])} chunks\")\n",
    "            return data['chunks']\n",
    "        except UnicodeDecodeError:\n",
    "            logging.warning(\"UTF-8 decoding failed, trying with utf-8-sig encoding\")\n",
    "            try:\n",
    "                with open(json_path, 'r', encoding='utf-8-sig') as f:\n",
    "                    data = json.load(f)\n",
    "                logging.info(f\"Successfully loaded {len(data['chunks'])} chunks with utf-8-sig encoding\")\n",
    "                return data['chunks']\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error loading chunks with utf-8-sig: {str(e)}\")\n",
    "                raise\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading chunks: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def embed_chunk(self, chunk: Dict[str, Any]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create embedding for a single chunk\n",
    "        \"\"\"\n",
    "        text = chunk['content']\n",
    "        try:\n",
    "            embedding = self.model.encode([text])[0]\n",
    "            return embedding\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error embedding chunk {chunk['id']}: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def add_chunk(self, chunk: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Add a single chunk to the FAISS index\n",
    "        \"\"\"\n",
    "        try:\n",
    "            embedding = self.embed_chunk(chunk)\n",
    "            self.index.add(embedding.reshape(1, -1))\n",
    "            \n",
    "            self.chunk_metadata[self.current_id] = {\n",
    "                'chunk_id': chunk['id'],\n",
    "                'content': chunk['content'],\n",
    "                'topics': chunk.get('topics', []),\n",
    "                'tokens': chunk.get('tokens', 0),\n",
    "                'entities': chunk.get('entities', [])\n",
    "            }\n",
    "            \n",
    "            self.current_id += 1\n",
    "            if self.current_id % 10 == 0:  # Log progress every 10 chunks\n",
    "                logging.info(f\"Processed {self.current_id} chunks\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error adding chunk {chunk['id']}: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def add_chunks(self, chunks: List[Dict[str, Any]]):\n",
    "        \"\"\"Add multiple chunks to the FAISS index\"\"\"\n",
    "        logging.info(f\"Adding {len(chunks)} chunks to FAISS index\")\n",
    "        \n",
    "        # First, fit vectorizer on all chunk contents\n",
    "        all_contents = [chunk['content'] for chunk in chunks]\n",
    "        self.vectorizer.fit(all_contents)\n",
    "        \n",
    "        # Then add chunks\n",
    "        for chunk in chunks:\n",
    "            self.add_chunk(chunk)\n",
    "        logging.info(\"Finished adding chunks\")\n",
    "    \n",
    "    def save(self, index_name: str):\n",
    "        \"\"\"\n",
    "        Save FAISS index and metadata\n",
    "        \"\"\"\n",
    "        index_path = self.base_path / f\"{index_name}.index\"\n",
    "        metadata_path = self.base_path / f\"{index_name}_metadata.pkl\"\n",
    "        \n",
    "        logging.info(f\"Saving FAISS index to {index_path}\")\n",
    "        faiss.write_index(self.index, str(index_path))\n",
    "        \n",
    "        logging.info(f\"Saving metadata to {metadata_path}\")\n",
    "        with open(metadata_path, 'wb') as f:\n",
    "            pickle.dump(self.chunk_metadata, f)\n",
    "            \n",
    "        # Also save vectorizer\n",
    "        vectorizer_path = self.base_path / f\"{index_name}_vectorizer.pkl\"\n",
    "        logging.info(f\"Saving vectorizer to {vectorizer_path}\")\n",
    "        with open(vectorizer_path, 'wb') as f:\n",
    "            pickle.dump(self.vectorizer, f)\n",
    "    \n",
    "    def load(self, index_name: str):\n",
    "        \"\"\"\n",
    "        Load FAISS index and metadata\n",
    "        \"\"\"\n",
    "        index_path = self.base_path / f\"{index_name}.index\"\n",
    "        metadata_path = self.base_path / f\"{index_name}_metadata.pkl\"\n",
    "        \n",
    "        logging.info(f\"Loading FAISS index from {index_path}\")\n",
    "        self.index = faiss.read_index(str(index_path))\n",
    "        \n",
    "        logging.info(f\"Loading metadata from {metadata_path}\")\n",
    "        with open(metadata_path, 'rb') as f:\n",
    "            self.chunk_metadata = pickle.load(f)\n",
    "            self.current_id = len(self.chunk_metadata)\n",
    "            \n",
    "        # Also load vectorizer\n",
    "        vectorizer_path = self.base_path / f\"{index_name}_vectorizer.pkl\"\n",
    "        logging.info(f\"Loading vectorizer from {vectorizer_path}\")\n",
    "        with open(vectorizer_path, 'rb') as f:\n",
    "            self.vectorizer = pickle.load(f)\n",
    "\n",
    "    def search(self, query: str, k: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Search for similar chunks\n",
    "        \"\"\"\n",
    "        query_embedding = self.model.encode([query])[0]\n",
    "        distances, indices = self.index.search(query_embedding.reshape(1, -1), k)\n",
    "        \n",
    "        results = []\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            if idx != -1:\n",
    "                result = self.chunk_metadata[idx].copy()\n",
    "                result['distance'] = float(distances[0][i])\n",
    "                results.append(result)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad565b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Import required libraries\n",
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "from rank_bm25 import BM25Okapi\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import os\n",
    "import faiss\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "\n",
    "class BaseRetriever(ABC):\n",
    "    def __init__(self, embedding_manager):\n",
    "        self.manager = embedding_manager\n",
    "        self.model = self.manager.model\n",
    "        self.name = \"base\"\n",
    "        self.hierarchical_stats = {\n",
    "            \"queries_processed\": 0,\n",
    "            \"hierarchical_improvements\": 0\n",
    "        }\n",
    "    \n",
    "    @abstractmethod\n",
    "    def retrieve(self, query: str, k: int = 3) -> List[Dict[str, Any]]:\n",
    "        pass\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, float]:\n",
    "        \"\"\"Return retriever stats (to be implemented by retrievers that track metrics)\"\"\"\n",
    "        return {}\n",
    "    \n",
    "    def get_hierarchical_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return hierarchical exploration statistics\"\"\"\n",
    "        stats = self.hierarchical_stats.copy()\n",
    "        \n",
    "        # Calculate improvement rate\n",
    "        if stats[\"queries_processed\"] > 0:\n",
    "            stats[\"improvement_rate\"] = stats[\"hierarchical_improvements\"] / stats[\"queries_processed\"]\n",
    "        else:\n",
    "            stats[\"improvement_rate\"] = 0.0\n",
    "            \n",
    "        return stats\n",
    "    \n",
    "    def _apply_hierarchical_exploration(self, results: List[Dict[str, Any]], query: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Apply hierarchical exploration to results\"\"\"\n",
    "        # Track query\n",
    "        self.hierarchical_stats[\"queries_processed\"] += 1\n",
    "        \n",
    "        # Check if there's hierarchical information\n",
    "        improved = False\n",
    "        for result in results:\n",
    "            # Example: Check if this result references a parent or child document\n",
    "            # that would be valuable to include\n",
    "            if 'parent_id' in result or 'children_ids' in result:\n",
    "                result['hierarchical_boost'] = True\n",
    "                improved = True\n",
    "            else:\n",
    "                result['hierarchical_boost'] = False\n",
    "        \n",
    "        # Update stats\n",
    "        if improved:\n",
    "            self.hierarchical_stats[\"hierarchical_improvements\"] += 1\n",
    "            \n",
    "        return results\n",
    "\n",
    "class ContentRetriever(BaseRetriever):\n",
    "    \"\"\"Retriever that uses only content embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_manager):\n",
    "        super().__init__(embedding_manager)\n",
    "        self.name = \"content\"\n",
    "        self.avg_similarity = 0\n",
    "        self.query_count = 0\n",
    "    \n",
    "    def retrieve(self, query: str, k: int = 3) -> List[Dict[str, Any]]:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Get query embedding\n",
    "        query_embedding = self.model.encode([query])[0]\n",
    "        \n",
    "        # Search in FAISS index\n",
    "        distances, indices = self.manager.index.search(query_embedding.reshape(1, -1), k)\n",
    "        \n",
    "        # Get results\n",
    "        results = []\n",
    "        total_similarity = 0\n",
    "        \n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            if idx != -1:\n",
    "                result = self.manager.chunk_metadata[str(idx)].copy()\n",
    "                distance = float(distances[0][i])\n",
    "                similarity = 1 - distance\n",
    "                \n",
    "                # Store scores in a separate dictionary for clarity\n",
    "                result['scores'] = {\n",
    "                    'similarity': similarity,\n",
    "                    'distance': distance\n",
    "                }\n",
    "                \n",
    "                total_similarity += similarity\n",
    "                results.append(result)\n",
    "        \n",
    "        # Update statistics\n",
    "        if results:\n",
    "            avg_similarity = total_similarity / len(results)\n",
    "            self.avg_similarity = ((self.avg_similarity * self.query_count) + avg_similarity) / (self.query_count + 1)\n",
    "            self.query_count += 1\n",
    "        \n",
    "        # Apply hierarchical exploration\n",
    "        results = self._apply_hierarchical_exploration(results, query)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        logging.info(f\"Content retrieval took {elapsed:.2f} seconds\")\n",
    "        return results\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, float]:\n",
    "        return {\n",
    "            \"avg_similarity\": self.avg_similarity,\n",
    "            \"query_count\": self.query_count\n",
    "        }\n",
    "\n",
    "class CombinedRetriever(BaseRetriever):\n",
    "    \"\"\"Retriever that combines content and metadata with weighted scoring\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_manager):\n",
    "        super().__init__(embedding_manager)\n",
    "        self.name = \"combined\"\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.metadata_vectorizer = TfidfVectorizer(\n",
    "            max_features=100,\n",
    "            stop_words='english',\n",
    "            min_df=1\n",
    "        )\n",
    "        self._prepare_metadata_vectors()\n",
    "        self.avg_similarity = 0\n",
    "        self.avg_metadata_score = 0\n",
    "        self.query_count = 0\n",
    "    \n",
    "    def _prepare_metadata_vectors(self):\n",
    "        \"\"\"Prepare metadata vectors for all chunks\"\"\"\n",
    "        metadata_texts = []\n",
    "        for chunk_id, chunk_data in self.manager.chunk_metadata.items():\n",
    "            metadata_text = ' '.join(chunk_data.get('topics', []))\n",
    "            if 'entities' in chunk_data:\n",
    "                metadata_text += ' ' + ' '.join([e['text'] for e in chunk_data['entities']])\n",
    "            metadata_texts.append(metadata_text)\n",
    "        \n",
    "        self.metadata_vectors = self.metadata_vectorizer.fit_transform(metadata_texts)\n",
    "    \n",
    "    def retrieve(self, query: str, k: int = 3) -> List[Dict[str, Any]]:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Get content-based results\n",
    "        query_embedding = self.model.encode([query])[0]\n",
    "        distances, indices = self.manager.index.search(query_embedding.reshape(1, -1), k * 2)\n",
    "        \n",
    "        # Extract entities from query\n",
    "        doc = self.nlp(query)\n",
    "        query_entities = [ent.text for ent in doc.ents]\n",
    "        \n",
    "        # Combine query with entities\n",
    "        enhanced_query = query + ' ' + ' '.join(query_entities)\n",
    "        \n",
    "        # Get query vector\n",
    "        query_vector = self.metadata_vectorizer.transform([enhanced_query])\n",
    "        \n",
    "        # Calculate metadata similarities\n",
    "        metadata_similarities = (query_vector @ self.metadata_vectors.T).toarray()[0]\n",
    "        \n",
    "        # Combine scores\n",
    "        combined_scores = {}\n",
    "        \n",
    "        # Add content-based scores (70% weight)\n",
    "        max_distance = max(distances[0]) + 1e-6\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            if idx != -1:\n",
    "                combined_scores[str(idx)] = {\n",
    "                    'combined': 0.7 * (1 - (distances[0][i] / max_distance)),\n",
    "                    'content': 1 - (distances[0][i] / max_distance),\n",
    "                    'metadata': 0.0  # Will be updated if metadata match exists\n",
    "                }\n",
    "        \n",
    "        # Add metadata-based scores (30% weight)\n",
    "        for idx in range(len(metadata_similarities)):\n",
    "            metadata_score = metadata_similarities[idx]\n",
    "            str_idx = str(idx)\n",
    "            if str_idx in combined_scores:\n",
    "                combined_scores[str_idx]['metadata'] = metadata_score\n",
    "                combined_scores[str_idx]['combined'] += 0.3 * metadata_score\n",
    "            else:\n",
    "                combined_scores[str_idx] = {\n",
    "                    'combined': 0.3 * metadata_score,\n",
    "                    'content': 0.0,  # No content match\n",
    "                    'metadata': metadata_score\n",
    "                }\n",
    "        \n",
    "        # Sort by combined score and get top k\n",
    "        top_indices = sorted(combined_scores.items(), key=lambda x: x[1]['combined'], reverse=True)[:k]\n",
    "        \n",
    "        # Prepare results\n",
    "        results = []\n",
    "        total_combined = 0\n",
    "        total_metadata = 0\n",
    "        \n",
    "        for idx, scores in top_indices:\n",
    "            result = self.manager.chunk_metadata[idx].copy()\n",
    "            result['scores'] = scores.copy()\n",
    "            \n",
    "            total_combined += scores['combined']\n",
    "            total_metadata += scores['metadata']\n",
    "            results.append(result)\n",
    "        \n",
    "        # Update statistics\n",
    "        if results:\n",
    "            avg_combined = total_combined / len(results)\n",
    "            avg_metadata = total_metadata / len(results)\n",
    "            \n",
    "            self.avg_similarity = ((self.avg_similarity * self.query_count) + avg_combined) / (self.query_count + 1)\n",
    "            self.avg_metadata_score = ((self.avg_metadata_score * self.query_count) + avg_metadata) / (self.query_count + 1)\n",
    "            self.query_count += 1\n",
    "        \n",
    "        # Apply hierarchical exploration\n",
    "        results = self._apply_hierarchical_exploration(results, query)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        logging.info(f\"Combined retrieval took {elapsed:.2f} seconds\")\n",
    "        return results\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, float]:\n",
    "        return {\n",
    "            \"avg_similarity\": self.avg_similarity,\n",
    "            \"avg_metadata_score\": self.avg_metadata_score,\n",
    "            \"query_count\": self.query_count\n",
    "        }\n",
    "\n",
    "class HybridRetriever(BaseRetriever):\n",
    "    \"\"\"Retriever that applies reranking to combined results\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_manager, reranker_model=None):\n",
    "        super().__init__(embedding_manager)\n",
    "        self.name = \"hybrid\"\n",
    "        self.base_retriever = CombinedRetriever(embedding_manager)\n",
    "        self.reranker = reranker_model if reranker_model else RerankerModel()\n",
    "        self.avg_reranker_score = 0\n",
    "        self.query_count = 0\n",
    "        self.candidate_pool_size = 100  # Number of candidates to get from base retriever\n",
    "    \n",
    "    def retrieve(self, query: str, k: int = 3) -> List[Dict[str, Any]]:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Determine how many documents to return after reranking\n",
    "        if k <= 3:\n",
    "            top_k = 2\n",
    "        elif k <= 5:\n",
    "            top_k = 3\n",
    "        else:\n",
    "            top_k = 4\n",
    "        \n",
    "        # Get a larger candidate pool from base retriever\n",
    "        candidates = self.base_retriever.retrieve(query, k=self.candidate_pool_size)\n",
    "        \n",
    "        # Rerank results\n",
    "        reranked_results = self.reranker.rerank(query, candidates, top_k)\n",
    "        \n",
    "        # Update statistics\n",
    "        if reranked_results:\n",
    "            total_score = sum([doc['scores'].get('reranker', 0) for doc in reranked_results])\n",
    "            avg_score = total_score / len(reranked_results)\n",
    "            self.avg_reranker_score = ((self.avg_reranker_score * self.query_count) + avg_score) / (self.query_count + 1)\n",
    "            self.query_count += 1\n",
    "        \n",
    "        # Apply hierarchical exploration\n",
    "        reranked_results = self._apply_hierarchical_exploration(reranked_results, query)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        logging.info(f\"Hybrid reranked retrieval took {elapsed:.2f} seconds\")\n",
    "        return reranked_results\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, float]:\n",
    "        base_stats = self.base_retriever.get_stats()\n",
    "        return {\n",
    "            **base_stats,\n",
    "            \"avg_reranker_score\": self.avg_reranker_score,\n",
    "            \"query_count\": self.query_count\n",
    "        }\n",
    "\n",
    "class RerankerModel:\n",
    "    \"\"\"Wrapper for a cross-encoder reranker model\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Move model to GPU if available\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "    \n",
    "    def rerank(self, query: str, documents: List[Dict[str, Any]], top_k: Optional[int] = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Rerank documents based on relevance to query\"\"\"\n",
    "        if not documents:\n",
    "            return []\n",
    "        \n",
    "        # Prepare document contents\n",
    "        doc_contents = [doc.get('content', '') for doc in documents]\n",
    "        \n",
    "        # Create input pairs\n",
    "        pairs = []\n",
    "        for doc_content in doc_contents:\n",
    "            pairs.append([query, doc_content])\n",
    "        \n",
    "        # Tokenize\n",
    "        with torch.no_grad():\n",
    "            # Process in batches to avoid OOM\n",
    "            batch_size = 8\n",
    "            all_scores = []\n",
    "            \n",
    "            for i in range(0, len(pairs), batch_size):\n",
    "                batch_pairs = pairs[i:i+batch_size]\n",
    "                features = self.tokenizer(batch_pairs, padding=True, truncation=True, \n",
    "                                          return_tensors=\"pt\", max_length=512)\n",
    "                features = {k: v.to(self.device) for k, v in features.items()}\n",
    "                \n",
    "                # Get embeddings\n",
    "                outputs = self.model(**features)\n",
    "                embeddings = outputs.last_hidden_state[:, 0, :]  # Use CLS token\n",
    "                \n",
    "                # Calculate scores\n",
    "                scores = torch.nn.functional.cosine_similarity(\n",
    "                    embeddings[:, None, :], embeddings[None, :, :], dim=-1\n",
    "                ).cpu().numpy()\n",
    "                \n",
    "                # Get diagonal scores (similarity between query and document)\n",
    "                for j in range(len(batch_pairs)):\n",
    "                    all_scores.append(float(scores[j, j]))\n",
    "        \n",
    "        # Add scores to documents\n",
    "        for i, doc in enumerate(documents):\n",
    "            if 'scores' not in doc:\n",
    "                doc['scores'] = {}\n",
    "            doc['scores']['reranker'] = all_scores[i]\n",
    "            # Update combined score with reranker (give it high weight)\n",
    "            if 'combined' in doc['scores']:\n",
    "                doc['scores']['final'] = 0.3 * doc['scores']['combined'] + 0.7 * all_scores[i]\n",
    "            else:\n",
    "                doc['scores']['final'] = all_scores[i]\n",
    "        \n",
    "        # Sort by reranker score\n",
    "        reranked_docs = sorted(documents, key=lambda x: x['scores'].get('final', 0), reverse=True)\n",
    "        \n",
    "        # Return top-k if specified\n",
    "        if top_k and top_k < len(reranked_docs):\n",
    "            return reranked_docs[:top_k]\n",
    "        \n",
    "        return reranked_docs\n",
    "\n",
    "class EmbeddingManager:\n",
    "    \"\"\"Manager for embeddings and document metadata\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"all-MiniLM-L6-v2\", base_path=None):\n",
    "        # Load embedding model\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.dimension = self.model.get_sentence_embedding_dimension()\n",
    "        \n",
    "        # Initialize empty index and metadata\n",
    "        self.index = None\n",
    "        self.chunk_metadata = {}\n",
    "        \n",
    "        # Load from base_path if provided\n",
    "        if base_path:\n",
    "            self.load(base_path)\n",
    "    \n",
    "    def load(self, base_path):\n",
    "        \"\"\"Load index and metadata from base path\"\"\"\n",
    "        index_path = os.path.join(base_path, \"s3.index\")\n",
    "        metadata_path = os.path.join(base_path, \"s3_metadata.pkl\")\n",
    "        \n",
    "        # Load FAISS index\n",
    "        if os.path.exists(index_path):\n",
    "            self.index = faiss.read_index(index_path)\n",
    "        else:\n",
    "            logging.warning(f\"Index file not found at {index_path}, initializing empty index\")\n",
    "            self.index = faiss.IndexFlatL2(self.dimension)\n",
    "        \n",
    "        # Load metadata\n",
    "        if os.path.exists(metadata_path):\n",
    "            with open(metadata_path, 'r') as f:\n",
    "                self.chunk_metadata = json.load(f)\n",
    "        else:\n",
    "            logging.warning(f\"Metadata file not found at {metadata_path}, initializing empty metadata\")\n",
    "            self.chunk_metadata = {}\n",
    "\n",
    "class RetrieverFactory:\n",
    "    \"\"\"Factory class to create retrievers\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_retrievers(model_name=\"all-MiniLM-L6-v2\", reranker_model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\", \n",
    "                         candidate_pool_size=100, base_path=None):\n",
    "        \"\"\"Create all retrievers\"\"\"\n",
    "        # Create embedding manager\n",
    "        manager = EmbeddingManager(model_name, base_path)\n",
    "        \n",
    "        # Create individual retrievers\n",
    "        content = ContentRetriever(manager)\n",
    "        combined = CombinedRetriever(manager)\n",
    "        \n",
    "        # Create reranker model\n",
    "        reranker = RerankerModel(reranker_model)\n",
    "        \n",
    "        # Create hybrid retriever\n",
    "        hybrid = HybridRetriever(manager, reranker)\n",
    "        hybrid.candidate_pool_size = candidate_pool_size\n",
    "        \n",
    "        return {\n",
    "            'content': content,\n",
    "            'combined': combined,\n",
    "            'hybrid': hybrid\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da80c8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 15:50:31,691 - INFO - Use pytorch device_name: cpu\n",
      "2025-03-03 15:50:31,691 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2025-03-03 15:50:32,281 - WARNING - Index file not found at NEW_faiss\\s3.index, initializing empty index\n",
      "2025-03-03 15:50:32,281 - WARNING - Metadata file not found at NEW_faiss\\s3_metadata.pkl, initializing empty metadata\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create all three retrievers\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m retrievers \u001b[38;5;241m=\u001b[39m RetrieverFactory\u001b[38;5;241m.\u001b[39mcreate_retrievers(\n\u001b[0;32m      3\u001b[0m     model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      4\u001b[0m     reranker_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcross-encoder/ms-marco-MiniLM-L-6-v2\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      5\u001b[0m     candidate_pool_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m,\n\u001b[0;32m      6\u001b[0m     base_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_faiss\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Example query\u001b[39;00m\n\u001b[0;32m     10\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are the prerequisites for locking a vault in S3 Glacier, and how does this feature enforce compliance?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[9], line 412\u001b[0m, in \u001b[0;36mRetrieverFactory.create_retrievers\u001b[1;34m(model_name, reranker_model, candidate_pool_size, base_path)\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;66;03m# Create individual retrievers\u001b[39;00m\n\u001b[0;32m    411\u001b[0m content \u001b[38;5;241m=\u001b[39m ContentRetriever(manager)\n\u001b[1;32m--> 412\u001b[0m combined \u001b[38;5;241m=\u001b[39m CombinedRetriever(manager)\n\u001b[0;32m    414\u001b[0m \u001b[38;5;66;03m# Create reranker model\u001b[39;00m\n\u001b[0;32m    415\u001b[0m reranker \u001b[38;5;241m=\u001b[39m RerankerModel(reranker_model)\n",
      "Cell \u001b[1;32mIn[9], line 141\u001b[0m, in \u001b[0;36mCombinedRetriever.__init__\u001b[1;34m(self, embedding_manager)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata_vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(\n\u001b[0;32m    137\u001b[0m     max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m    138\u001b[0m     stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    139\u001b[0m     min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    140\u001b[0m )\n\u001b[1;32m--> 141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_metadata_vectors()\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavg_similarity \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavg_metadata_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[1;32mIn[9], line 155\u001b[0m, in \u001b[0;36mCombinedRetriever._prepare_metadata_vectors\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    152\u001b[0m         metadata_text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([e[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m chunk_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentities\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[0;32m    153\u001b[0m     metadata_texts\u001b[38;5;241m.\u001b[39mappend(metadata_text)\n\u001b[1;32m--> 155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata_vectors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata_vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(metadata_texts)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2126\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2121\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2122\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2123\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2124\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2125\u001b[0m )\n\u001b[1;32m-> 2126\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit_transform(raw_documents)\n\u001b[0;32m   2127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2128\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2129\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1383\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1375\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1376\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1377\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1378\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1379\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1380\u001b[0m             )\n\u001b[0;32m   1381\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1383\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_vocab(raw_documents, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_vocabulary_)\n\u001b[0;32m   1385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1386\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1289\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1287\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1288\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1289\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1290\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1291\u001b[0m         )\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create all three retrievers\n",
    "retrievers = RetrieverFactory.create_retrievers(\n",
    "    model_name='all-MiniLM-L6-v2',\n",
    "    reranker_model='cross-encoder/ms-marco-MiniLM-L-6-v2',\n",
    "    candidate_pool_size=60,\n",
    "    base_path='FSAII'\n",
    ")\n",
    "\n",
    "# Example query\n",
    "query = \"What are the prerequisites for locking a vault in S3 Glacier, and how does this feature enforce compliance?\"\n",
    "\n",
    "# Function to display results with detailed scores\n",
    "def display_results(retriever_name, results):\n",
    "    print(f\"\\n\\n=== {retriever_name.upper()} RETRIEVER RESULTS ===\")\n",
    "    print(f\"Top {len(results)} results:\")\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"\\n{i+1}. {result.get('id', result.get('chunk_id', 'unknown'))}\")\n",
    "        \n",
    "        # Print scores\n",
    "        if 'scores' in result:\n",
    "            print(\"   Scores:\")\n",
    "            for score_type, score_value in result['scores'].items():\n",
    "                print(f\"     {score_type}: {score_value:.4f}\" if isinstance(score_value, float) \n",
    "                      else f\"     {score_type}: {score_value}\")\n",
    "        \n",
    "        # Print hierarchical information\n",
    "        print(f\"   Hierarchical boost: {result.get('hierarchical_boost', False)}\")\n",
    "        \n",
    "        # Print content snippet\n",
    "        content = result.get('content', '')\n",
    "        if content:\n",
    "            print(f\"   Content snippet: {content[:100]}...\")\n",
    "\n",
    "\n",
    "# Try content retriever\n",
    "content_results = retrievers['content'].retrieve(query, k=5)\n",
    "display_results('Content', content_results)\n",
    "\n",
    "# Try combined retriever\n",
    "combined_results = retrievers['combined'].retrieve(query, k=5)\n",
    "display_results('Combined', combined_results)\n",
    "\n",
    "# Try hybrid retriever\n",
    "hybrid_results = retrievers['hybrid'].retrieve(query, k=5)\n",
    "display_results('Hybrid', hybrid_results)\n",
    "\n",
    "# Compare hierarchical improvement stats\n",
    "print(\"\\n\\n=== HIERARCHICAL EXPLORATION STATS ===\")\n",
    "for name, retriever in retrievers.items():\n",
    "    stats = retriever.get_hierarchical_stats()\n",
    "    print(f\"\\n{name.upper()} RETRIEVER:\")\n",
    "    print(f\"  Queries processed: {stats['queries_processed']}\")\n",
    "    print(f\"  Improvements: {stats['hierarchical_improvements']}\")\n",
    "    print(f\"  Improvement rate: {stats['improvement_rate']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e152698",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0885bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13e6ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
