{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b828a47d",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc2570e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This basic example demostrate the LLM response and ChatModel Response\n",
    "\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "#app.py\n",
    "\n",
    "from langchain import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72e5022b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370e4756680d40a9978934a4f8af3ed9 Azure https://testopenaisaturday.openai.azure.com/ 2023-10-01-preview\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from the .env file\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# Retrieve Azure OpenAI specific configuration from environment variables\n",
    "OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "OPENAI_API_TYPE = \"Azure\"\n",
    "OPENAI_API_BASE = \"https://testopenaisaturday.openai.azure.com/\"\n",
    "OPENAI_API_VERSION = \"2023-10-01-preview\"\n",
    "\n",
    "print(OPENAI_API_KEY, OPENAI_API_TYPE,OPENAI_API_BASE, OPENAI_API_VERSION )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8c76c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the OpenAI library configuration using the retrieved environment variables\n",
    "openai.api_type = \"Azure\"\n",
    "openai.api_base = \"https://testopenaisaturday.openai.azure.com/\"\n",
    "openai.api_version = \"2023-10-01-preview\"\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "\n",
    "llm = AzureChatOpenAI( \n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    azure_endpoint=OPENAI_API_BASE,\n",
    "    openai_api_version=OPENAI_API_VERSION,\n",
    "    deployment_name=\"assistantPreviewSaturday\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29c29bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#app.py\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "from langchain.chat_models import AzureChatOpenAI \n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96010ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "llm_temp = AzureChatOpenAI( \n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    azure_endpoint=OPENAI_API_BASE,\n",
    "    openai_api_version=OPENAI_API_VERSION,\n",
    "    deployment_name=\"assistantPreviewSaturday\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9426e2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_unstructured.unit_utils import assert_round_trips_through_JSON, example_doc_path\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "from unstructured.documents.elements import ElementType\n",
    "\n",
    "#!pip install unstructured_inference\n",
    "#!pip install -U langchain-unstructured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392fa6f3",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5aaafd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "# Open the PDF file\n",
    "with open('s3.pdf', 'rb') as file:\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    \n",
    "    # Iterate through all the pages and extract text\n",
    "    text = ''\n",
    "    for page_num in range(len(reader.pages)):\n",
    "        page = reader.pages[page_num]\n",
    "        text += page.extract_text()\n",
    "\n",
    "# Print the extracted text\n",
    "#print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c21af7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document  # Import the Document class\n",
    "\n",
    "# Step 1: Extract text from the PDF using PyPDF2\n",
    "with open('s3.pdf', 'rb') as file:\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    \n",
    "    # Iterate through all the pages and extract text\n",
    "    text = ''\n",
    "    for page_num in range(len(reader.pages)):\n",
    "        page = reader.pages[page_num]\n",
    "        text += page.extract_text()\n",
    "\n",
    "# Step 2: Create a Document object\n",
    "document = Document(page_content=text)\n",
    "\n",
    "# Step 3: Use CharacterTextSplitter to split the extracted text into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=800, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents([document])  # Pass in a list of Document objects\n",
    "\n",
    "# Step 4: Output the split text chunks\n",
    "for chunk in texts:\n",
    "    print(chunk.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "07ec4cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 2405\n",
      "Chunk 1:\n",
      "User Guide\n",
      "Amazon Simple Storage Service\n",
      "API Version 2006-03-01\n",
      "Copyright © 2024 Amazon Web Services, Inc. and/or its aﬃliates. All rights reserved.Amazon Simple Storage Service User Guide\n",
      "Amazon Simple Storage Service: User Guide\n",
      "Copyright © 2024 Amazon Web Services, Inc. and/or its aﬃliates. All rights reserved.\n",
      "Amazon's trademarks and trade dress may not be used in connection with any product or service \n",
      "that is not Amazon's, in any manner that is likely to cause confusion among customers, or\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Chunk 2:\n",
      "Analytics and insights............................................................................................................................5\n",
      "Strong consistency..................................................................................................................................5\n",
      "How Amazon S3 works...............................................................................................................................5\n",
      "Buckets...............................................\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Chunk 3:\n",
      "Amazon S3 data consistency model......................................................................................................10\n",
      "Concurrent applications.......................................................................................................................11\n",
      "Related services..........................................................................................................................................12\n",
      "Accessing Amazon S3..........................................\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document  # Import the Document class\n",
    "\n",
    "# Step 1: Extract text from the PDF using PyPDF2\n",
    "with open('s3.pdf', 'rb') as file:\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    \n",
    "    # Iterate through all the pages and extract text\n",
    "    text = ''\n",
    "    for page_num in range(len(reader.pages)):\n",
    "        page = reader.pages[page_num]\n",
    "        extracted_text = page.extract_text()\n",
    "        if extracted_text:  # Ensure the text extraction is successful\n",
    "            text += extracted_text\n",
    "\n",
    "# Step 2: Create a Document object\n",
    "document = Document(page_content=text)\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Step 3: Use RecursiveCharacterTextSplitter for better splitting\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000, \n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \"]\n",
    ")\n",
    "\n",
    "# Split the document text into smaller chunks\n",
    "texts = text_splitter.split_documents([document])  # Pass in a list of Document objects\n",
    "\n",
    "# Step 4: Print the number of chunks\n",
    "print(f\"Number of chunks: {len(texts)}\")\n",
    "\n",
    "# Step 5: Optionally, output the first few chunks to inspect the splitting\n",
    "for i, chunk in enumerate(texts[:3]):  # Limit to first 3 chunks for display\n",
    "    print(f\"Chunk {i+1}:\")\n",
    "    print(chunk.page_content[:500])  # Print first 500 characters of the chunk\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "988cc140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete directories, and it does not support symbolic links or ﬁle locking. Mountpoint is ideal \n",
      "for applications that do not need all of the features of a shared ﬁle system and POSIX-style \n",
      "permissions but require Amazon S3's elastic throughput to read and write large S3 datasets. For \n",
      "details, see Mountpoint ﬁle system behavior on GitHub. For workloads that require full POSIX \n",
      "support, we recommend Amazon FSx for Lustre and its support for linking S3 buckets.\n",
      "Mountpoint for Amazon S3 is availab\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "wget download-link\n",
      "3. (Optional) Verify the authenticity and integrity of the downloaded ﬁle. First, copy the \n",
      "appropriate signature URL for your architecture.\n",
      "x86_64 :\n",
      "https://s3.amazonaws.com/mountpoint-s3-release/latest/x86_64/mount-s3.rpm.asc\n",
      "Installing Mountpoint API Version 2006-03-01 85Amazon Simple Storage Service User Guide\n",
      "ARM64 (Graviton) :\n",
      "https://s3.amazonaws.com/mountpoint-s3-release/latest/arm64/mount-s3.rpm.asc\n",
      "Next, see Verifying the signature of the Mountpoint for Amazon S3 pac\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Output only the first 2-3 split text chunks with truncation to avoid large data output\n",
    "for chunk in texts[125:127]:  # Display the first 3 chunks\n",
    "    print(chunk.page_content[:500])  # Print only the first 500 characters of each chunk\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df16e822",
   "metadata": {},
   "source": [
    "## Metadata Gen : Topic Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e295181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the function to work with strings (plain text chunks) and generate 2-3 tags\n",
    "def generate_metadata_for_chunks(llm, chunks, max_tokens=2048):\n",
    "    results = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # Truncate the chunk if it exceeds the max_tokens limit\n",
    "        truncated_chunk = chunk[:max_tokens]  # Now 'chunk' is a string, so we handle it directly\n",
    "        \n",
    "        # Create a prompt that requests the LLM to generate only the most relevant 2-3 tags and headings\n",
    "        prompt = f\"\"\"\n",
    "        The following text is a chunk from a larger document. Please generate the 2-3 most relevant topic headings and tags for this text:\n",
    "        Text: \"{truncated_chunk}\"\n",
    "        \"\"\"\n",
    "        \n",
    "        # Call the LLM with the prompt using the invoke method\n",
    "        response = llm.invoke([{\n",
    "            \"role\": \"system\",\n",
    "            \"content\": prompt\n",
    "        }])\n",
    "        \n",
    "        # Store the result (response with tags and headings)\n",
    "        results.append({\n",
    "            \"chunk_index\": i,\n",
    "            \"chunk_text\": truncated_chunk,\n",
    "            \"metadata\": response\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Display the results with truncated chunk text and the most relevant metadata\n",
    "def display_metadata_results(results):\n",
    "    for result in results:\n",
    "        print(f\"Chunk Index: {result['chunk_index']}\")\n",
    "        print(\"Chunk Text (truncated):\")\n",
    "        print(result['chunk_text'][:1000])  # Display only the first 1000 characters of the chunk text\n",
    "        print(\"\\nGenerated Metadata:\")\n",
    "        \n",
    "        # Extract and display the topic headings and tags from the metadata\n",
    "        metadata_content = result['metadata'].content\n",
    "        \n",
    "        # Format the metadata output\n",
    "        print(f\"{metadata_content}\")\n",
    "        print(\"\\n\" + \"-\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "190097fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk Index: 0\n",
      "Chunk Text (truncated):\n",
      "\"s3:GetObject\" \n",
      "         ], \n",
      "         \"Resource\": [ \n",
      "            \"arn:aws:s3::: amzn-s3-demo-bucket1 /*\" \n",
      "         ] \n",
      "      }   \n",
      "   ]\n",
      "}\n",
      "Step 2: Do the Account B tasks\n",
      "Now that Account B has permissions to perform operations on Account A's bucket, the Account B \n",
      "administrator does the following:\n",
      "•Uploads an object to Account A's bucket\n",
      "•Adds a grant in the object ACL to allow Account A, the bucket owner, full control\n",
      "Walkthroughs using policies API Version 2006-03-01 1010Amazon Simple Storage Service User Guide\n",
      "Using the AWS CLI\n",
      "1. Using the put-object  AWS CLI command, upload an object. The --body  parameter in the \n",
      "command identiﬁes the source ﬁle to upload. For example, if the ﬁle is on the C: drive of a \n",
      "Windows machine, specify c:\\HappyFace.jpg . The --key parameter provides the key name \n",
      "for the object.\n",
      "aws s3api put-object --bucket amzn-s3-demo-bucket1  --key HappyFace.jpg --body \n",
      " HappyFace.jpg --profile AccountBadmin\n",
      "2. Add a grant to the object ACL to allow the bucket owner fu\n",
      "\n",
      "Generated Metadata:\n",
      "Topic Headings:\n",
      "- Granting Permissions to Access an S3 Bucket Across AWS Accounts\n",
      "- Testing Access Permissions for an S3 Object Across AWS Accounts\n",
      "\n",
      "Tags:\n",
      "- AWS CLI\n",
      "- S3 Bucket\n",
      "- Access Control\n",
      "- AWS Accounts \n",
      "- Permissions\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to pick a specific chunk by index\n",
    "def pick_chunks_by_index(indices, texts):\n",
    "    selected_chunks = [texts[i].page_content for i in indices if i < len(texts)]\n",
    "    return selected_chunks\n",
    "\n",
    "# Example usage: pick chunk(s) by index\n",
    "indices = [1004]  # You can manually update this list with the indices of the chunks you want to select\n",
    "\n",
    "# Get the selected chunks based on the provided indices\n",
    "selected_chunks = pick_chunks_by_index(indices, texts)\n",
    "\n",
    "# Generate metadata for the selected chunks\n",
    "results = generate_metadata_for_chunks(llm, selected_chunks)\n",
    "\n",
    "# Output the results to see how it works\n",
    "display_metadata_results(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedd80a3",
   "metadata": {},
   "source": [
    "## Embedings : Split and add metadata with chunks to Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d06dd2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Assuming `generate_metadata_for_chunks` gives us topics and tags for each chunk\n",
    "def create_documents_with_metadata(chunks, metadata):\n",
    "    documents = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        doc_metadata = {\n",
    "            \"topics\": metadata[i].get(\"topics\", []),  # Assuming metadata contains 'topics'\n",
    "            \"tags\": metadata[i].get(\"tags\", [])  # Assuming metadata contains 'tags'\n",
    "        }\n",
    "        document = Document(page_content=chunk.page_content, metadata=doc_metadata)\n",
    "        documents.append(document)\n",
    "    return documents\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e8a6869d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the PDF document (replace with your own PDF file)\n",
    "directory = os.getcwd()\n",
    "file_name = directory + \"/s3.pdf\"  # Example file name\n",
    "loader = PyPDFLoader(file_name)\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# Generate metadata for each chunk (topics and tags)\n",
    "metadata = generate_metadata_for_chunks(llm, [page.page_content for page in pages])\n",
    "\n",
    "# Create documents with metadata\n",
    "documents = create_documents_with_metadata(pages, metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4b6aab1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "\n",
    "# Initialize the embeddings model using Azure OpenAI\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    deployment = \"AzureAdaLangchain\",\n",
    "    model = \"text-embedding-ada-002\",\n",
    "    api_key= \"370e4756680d40a9978934a4f8af3ed9\",\n",
    "    openai_api_version=\"2023-10-01-preview\",\n",
    "    azure_endpoint=\"https://testopenaisaturday.openai.azure.com/\",\n",
    "    openai_api_type=\"azure\",\n",
    "    chunk_size=1\n",
    ")\n",
    "\n",
    "# Use FAISS to index the document chunks along with their metadata\n",
    "db = FAISS.from_documents(documents=documents, embedding=embeddings)\n",
    "\n",
    "# Save the FAISS index locally\n",
    "db.save_local(directory + \"/faiss_index_s3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c35131",
   "metadata": {},
   "source": [
    "## Integrate LLM with embedded DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "50d7a191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = AzureChatOpenAI( \n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    azure_endpoint=OPENAI_API_BASE,\n",
    "    openai_api_version=OPENAI_API_VERSION,\n",
    "    deployment_name=\"varelabsAssistant\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Load the FAISS vector store\n",
    "vector_store = FAISS.load_local(directory + \"/faiss_index_s3\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "# Set up a retriever using similarity search\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n",
    "\n",
    "\n",
    "# GPT 4-o\n",
    "\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "QA_PROMPT = PromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant specialized in providing support for installation, configuration, and troubleshooting issues. Please use the relevant documentation to provide concise and clear instructions based on the user's question. Stick to the information in the provided documentation.\n",
    "\n",
    "Documentation Context:\n",
    "{context}\n",
    "\n",
    "User's Question: {question}\n",
    "Assistant's Response:\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # 'stuff' chain is simple and suitable here\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": QA_PROMPT},\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "def ask_question(qa, question):\n",
    "    result = qa({\"query\": question})\n",
    "    print(\"Answer:\", result[\"result\"])\n",
    "    return result\n",
    "\n",
    "\n",
    "# Function to ask a question and maintain conversation context\n",
    "def ask_question_with_context(qa, question, chat_history):\n",
    "    result = qa({\"question\": question, \"chat_history\": chat_history})\n",
    "    print(\"Answer:\", result[\"answer\"])\n",
    "    chat_history.append((question, result[\"answer\"]))\n",
    "    return chat_history\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db91c06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty chat history\n",
    "chat_history = []\n",
    "\n",
    "\n",
    "# Ask your questions\n",
    "question = \"How frequent should a child brush their teeth?\"\n",
    "response = ask_question(qa, question)\n",
    "\n",
    "question = \"My kid is Gavin. Gavin is 8 years old. He refuses to brush before bed. What should I do?\"\n",
    "response = ask_question(qa, question)\n",
    "\n",
    "question = \"What should Gavin correct?\"\n",
    "response = ask_question(qa, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5087c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48fc804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ebe2a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
