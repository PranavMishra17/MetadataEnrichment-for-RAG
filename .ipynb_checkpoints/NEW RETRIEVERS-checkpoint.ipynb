{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f05f146",
   "metadata": {},
   "source": [
    "# Advanced Retriever with BM25, Reranking, and Hierarchical Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e432ef3",
   "metadata": {},
   "source": [
    "### Initial Embedder manager - for fetching from index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255f7a39",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43da9bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prana\\anaconda3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# Notebook 1: Setup and Data Loading\n",
    "import json\n",
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import PyPDF2\n",
    "import spacy\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from pathlib import Path\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class FAISSEmbeddingManager:\n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2', base_path: str = 'NEW_faiss'):\n",
    "        \"\"\"\n",
    "        Initialize the FAISS Embedding Manager\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.base_path = Path(base_path)\n",
    "        self.base_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        logging.info(f\"Loading model: {model_name}\")\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.dimension = self.model.get_sentence_embedding_dimension()\n",
    "        \n",
    "        self.index = faiss.IndexFlatL2(self.dimension)\n",
    "        self.chunk_metadata = {}\n",
    "        self.current_id = 0\n",
    "        \n",
    "        # Add NLP and vectorizer\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            max_features=100,\n",
    "            stop_words='english',\n",
    "            min_df=1\n",
    "        )\n",
    "    \n",
    "    def load_chunks(self, json_path: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Load chunks from JSON file with proper UTF-8 encoding\n",
    "        \"\"\"\n",
    "        logging.info(f\"Loading chunks from {json_path}\")\n",
    "        try:\n",
    "            with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            logging.info(f\"Successfully loaded {len(data['chunks'])} chunks\")\n",
    "            return data['chunks']\n",
    "        except UnicodeDecodeError:\n",
    "            logging.warning(\"UTF-8 decoding failed, trying with utf-8-sig encoding\")\n",
    "            try:\n",
    "                with open(json_path, 'r', encoding='utf-8-sig') as f:\n",
    "                    data = json.load(f)\n",
    "                logging.info(f\"Successfully loaded {len(data['chunks'])} chunks with utf-8-sig encoding\")\n",
    "                return data['chunks']\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error loading chunks with utf-8-sig: {str(e)}\")\n",
    "                raise\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading chunks: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def embed_chunk(self, chunk: Dict[str, Any]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create embedding for a single chunk\n",
    "        \"\"\"\n",
    "        text = chunk['content']\n",
    "        try:\n",
    "            embedding = self.model.encode([text])[0]\n",
    "            return embedding\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error embedding chunk {chunk['id']}: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def add_chunk(self, chunk: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Add a single chunk to the FAISS index\n",
    "        \"\"\"\n",
    "        try:\n",
    "            embedding = self.embed_chunk(chunk)\n",
    "            self.index.add(embedding.reshape(1, -1))\n",
    "            \n",
    "            self.chunk_metadata[self.current_id] = {\n",
    "                'chunk_id': chunk['id'],\n",
    "                'content': chunk['content'],\n",
    "                'topics': chunk.get('topics', []),\n",
    "                'tokens': chunk.get('tokens', 0),\n",
    "                'entities': chunk.get('entities', [])\n",
    "            }\n",
    "            \n",
    "            self.current_id += 1\n",
    "            if self.current_id % 10 == 0:  # Log progress every 10 chunks\n",
    "                logging.info(f\"Processed {self.current_id} chunks\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error adding chunk {chunk['id']}: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def add_chunks(self, chunks: List[Dict[str, Any]]):\n",
    "        \"\"\"Add multiple chunks to the FAISS index\"\"\"\n",
    "        logging.info(f\"Adding {len(chunks)} chunks to FAISS index\")\n",
    "        \n",
    "        # First, fit vectorizer on all chunk contents\n",
    "        all_contents = [chunk['content'] for chunk in chunks]\n",
    "        self.vectorizer.fit(all_contents)\n",
    "        \n",
    "        # Then add chunks\n",
    "        for chunk in chunks:\n",
    "            self.add_chunk(chunk)\n",
    "        logging.info(\"Finished adding chunks\")\n",
    "    \n",
    "    def save(self, index_name: str):\n",
    "        \"\"\"\n",
    "        Save FAISS index and metadata\n",
    "        \"\"\"\n",
    "        index_path = self.base_path / f\"{index_name}.index\"\n",
    "        metadata_path = self.base_path / f\"{index_name}_metadata.pkl\"\n",
    "        \n",
    "        logging.info(f\"Saving FAISS index to {index_path}\")\n",
    "        faiss.write_index(self.index, str(index_path))\n",
    "        \n",
    "        logging.info(f\"Saving metadata to {metadata_path}\")\n",
    "        with open(metadata_path, 'wb') as f:\n",
    "            pickle.dump(self.chunk_metadata, f)\n",
    "            \n",
    "        # Also save vectorizer\n",
    "        vectorizer_path = self.base_path / f\"{index_name}_vectorizer.pkl\"\n",
    "        logging.info(f\"Saving vectorizer to {vectorizer_path}\")\n",
    "        with open(vectorizer_path, 'wb') as f:\n",
    "            pickle.dump(self.vectorizer, f)\n",
    "    \n",
    "    def load(self, index_name: str):\n",
    "        \"\"\"\n",
    "        Load FAISS index and metadata\n",
    "        \"\"\"\n",
    "        index_path = self.base_path / f\"{index_name}.index\"\n",
    "        metadata_path = self.base_path / f\"{index_name}_metadata.pkl\"\n",
    "        \n",
    "        logging.info(f\"Loading FAISS index from {index_path}\")\n",
    "        self.index = faiss.read_index(str(index_path))\n",
    "        \n",
    "        logging.info(f\"Loading metadata from {metadata_path}\")\n",
    "        with open(metadata_path, 'rb') as f:\n",
    "            self.chunk_metadata = pickle.load(f)\n",
    "            self.current_id = len(self.chunk_metadata)\n",
    "            \n",
    "        # Also load vectorizer\n",
    "        vectorizer_path = self.base_path / f\"{index_name}_vectorizer.pkl\"\n",
    "        logging.info(f\"Loading vectorizer from {vectorizer_path}\")\n",
    "        with open(vectorizer_path, 'rb') as f:\n",
    "            self.vectorizer = pickle.load(f)\n",
    "\n",
    "    def search(self, query: str, k: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Search for similar chunks\n",
    "        \"\"\"\n",
    "        query_embedding = self.model.encode([query])[0]\n",
    "        distances, indices = self.index.search(query_embedding.reshape(1, -1), k)\n",
    "        \n",
    "        results = []\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            if idx != -1:\n",
    "                result = self.chunk_metadata[idx].copy()\n",
    "                result['distance'] = float(distances[0][i])\n",
    "                results.append(result)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d9d212",
   "metadata": {},
   "source": [
    "## Custom++ Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f864961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Dict, Any, Tuple, Set\n",
    "import logging\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import CrossEncoder\n",
    "import spacy\n",
    "import faiss\n",
    "import math\n",
    "from collections import Counter, defaultdict\n",
    "from abc import ABC, abstractmethod\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class BaseRetriever(ABC):\n",
    "    \"\"\"Base abstract class for all retrievers\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_manager=None):\n",
    "        self.manager = embedding_manager\n",
    "        self.name = \"base\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def retrieve(self, query: str, k: int = 3) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant chunks for a given query\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            k: Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List of chunk results with relevance scores\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class BM25Scorer:\n",
    "    \"\"\"BM25 scoring implementation for keyword-based relevance\"\"\"\n",
    "    \n",
    "    def __init__(self, k1=1.5, b=0.75):\n",
    "        \"\"\"\n",
    "        Initialize BM25 scorer\n",
    "        \n",
    "        Args:\n",
    "            k1: Term frequency saturation parameter\n",
    "            b: Length normalization parameter\n",
    "        \"\"\"\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.doc_freqs = defaultdict(int)  # Document frequencies\n",
    "        self.doc_lengths = []  # Length of each document\n",
    "        self.avg_doc_length = 0\n",
    "        self.total_docs = 0\n",
    "        self.chunk_keywords = []  # Processed keywords for each chunk\n",
    "        self.chunk_ids = []  # Original chunk IDs\n",
    "        self.id_to_idx = {}  # Mapping from chunk_id to index\n",
    "        \n",
    "    def extract_keywords(self, chunk: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract keywords from a chunk\n",
    "        \n",
    "        Args:\n",
    "            chunk: The chunk to process\n",
    "            \n",
    "        Returns:\n",
    "            List of keywords\n",
    "        \"\"\"\n",
    "        keywords = []\n",
    "        \n",
    "        # Add topics if available\n",
    "        if 'topics' in chunk and chunk['topics']:\n",
    "            keywords.extend(chunk['topics'])\n",
    "            \n",
    "        # Add entity texts if available\n",
    "        if 'entities' in chunk and chunk['entities']:\n",
    "            keywords.extend([entity['text'].lower() for entity in chunk['entities']])\n",
    "            \n",
    "        # Add important words from content\n",
    "        if 'content' in chunk and chunk['content']:\n",
    "            # Simple content-based keywords\n",
    "            content_words = chunk['content'].lower().split()\n",
    "            # Filter out short words and common stopwords\n",
    "            content_words = [w for w in content_words if len(w) > 3]\n",
    "            keywords.extend(content_words)\n",
    "            \n",
    "        return list(set(keywords))  # Remove duplicates\n",
    "        \n",
    "    def index_chunks(self, chunks: List[Dict[str, Any]]):\n",
    "        \"\"\"\n",
    "        Index chunks for BM25 scoring\n",
    "        \n",
    "        Args:\n",
    "            chunks: List of chunks to index\n",
    "        \"\"\"\n",
    "        logging.info(f\"Indexing {len(chunks)} chunks for BM25 scoring\")\n",
    "        self.total_docs = len(chunks)\n",
    "        self.chunk_ids = [chunk.get('id', chunk.get('chunk_id', f'unknown_{i}')) for i, chunk in enumerate(chunks)]\n",
    "        \n",
    "        # Create mapping from chunk_id to index\n",
    "        self.id_to_idx = {chunk_id: i for i, chunk_id in enumerate(self.chunk_ids)}\n",
    "        \n",
    "        # Process each chunk\n",
    "        for chunk in chunks:\n",
    "            # Extract keywords\n",
    "            keywords = self.extract_keywords(chunk)\n",
    "            self.chunk_keywords.append(keywords)\n",
    "            \n",
    "            # Update document frequencies\n",
    "            for keyword in set(keywords):  # Use set to count each term once per document\n",
    "                self.doc_freqs[keyword] += 1\n",
    "            \n",
    "            # Update document lengths\n",
    "            self.doc_lengths.append(len(keywords))\n",
    "            \n",
    "        # Calculate average document length\n",
    "        self.avg_doc_length = sum(self.doc_lengths) / max(1, self.total_docs)\n",
    "        logging.info(f\"BM25 indexing complete. Average document length: {self.avg_doc_length:.2f}\")\n",
    "        \n",
    "    def score(self, query_keywords: List[str], doc_idx: int) -> float:\n",
    "        \"\"\"\n",
    "        Calculate BM25 score for a document\n",
    "        \n",
    "        Args:\n",
    "            query_keywords: List of keywords from the query\n",
    "            doc_idx: Index of the document to score\n",
    "            \n",
    "        Returns:\n",
    "            BM25 score\n",
    "        \"\"\"\n",
    "        score = 0\n",
    "        doc_length = self.doc_lengths[doc_idx]\n",
    "        doc_keywords = self.chunk_keywords[doc_idx]\n",
    "        \n",
    "        # Count query terms in document\n",
    "        doc_term_counts = Counter(doc_keywords)\n",
    "        \n",
    "        for query_term in query_keywords:\n",
    "            if query_term not in doc_term_counts:\n",
    "                continue\n",
    "                \n",
    "            # Calculate IDF\n",
    "            df = self.doc_freqs.get(query_term, 0)\n",
    "            if df == 0:\n",
    "                continue\n",
    "            idf = math.log((self.total_docs - df + 0.5) / (df + 0.5) + 1)\n",
    "            \n",
    "            # Calculate TF with saturation and length normalization\n",
    "            tf = doc_term_counts[query_term]\n",
    "            tf_normalized = ((tf * (self.k1 + 1)) / \n",
    "                           (tf + self.k1 * (1 - self.b + self.b * doc_length / self.avg_doc_length)))\n",
    "            \n",
    "            score += idf * tf_normalized\n",
    "            \n",
    "        return score\n",
    "    \n",
    "    def get_scores(self, query: str) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Get BM25 scores for all documents\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            \n",
    "        Returns:\n",
    "            List of (chunk_id, score) tuples\n",
    "        \"\"\"\n",
    "        # Process query keywords\n",
    "        query_keywords = query.lower().split()\n",
    "        \n",
    "        # Calculate scores for all documents\n",
    "        scores = []\n",
    "        for i in range(len(self.chunk_keywords)):\n",
    "            score = self.score(query_keywords, i)\n",
    "            scores.append((self.chunk_ids[i], score))\n",
    "        \n",
    "        # Normalize scores\n",
    "        max_score = max([score for _, score in scores]) if scores else 1\n",
    "        if max_score > 0:\n",
    "            normalized_scores = [(chunk_id, score/max_score) for chunk_id, score in scores]\n",
    "        else:\n",
    "            normalized_scores = scores\n",
    "            \n",
    "        return normalized_scores\n",
    "    \n",
    "    def get_top_k(self, query: str, k: int = 5) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Get top-k chunks by BM25 score\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            k: Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List of (chunk_id, score) tuples for top-k results\n",
    "        \"\"\"\n",
    "        scores = self.get_scores(query)\n",
    "        # Sort by score (descending)\n",
    "        sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "        return sorted_scores[:k]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9a2f72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FAISSIndexManager:\n",
    "    \"\"\"Manager for FAISS indices\"\"\"\n",
    "    \n",
    "    def __init__(self, base_path: str = 'NEW_faiss'):\n",
    "        \"\"\"\n",
    "        Initialize FAISS index manager\n",
    "        \n",
    "        Args:\n",
    "            base_path: Path to FAISS index directory\n",
    "        \"\"\"\n",
    "        self.base_path = Path(base_path)\n",
    "        self.indices = {}\n",
    "        self.metadata = {}\n",
    "        \n",
    "    def load_index(self, index_name: str):\n",
    "        \"\"\"\n",
    "        Load a FAISS index and its metadata\n",
    "        \n",
    "        Args:\n",
    "            index_name: Name of the index to load\n",
    "        \"\"\"\n",
    "        index_path = self.base_path / f\"{index_name}.index\"\n",
    "        metadata_path = self.base_path / f\"{index_name}_metadata.pkl\"\n",
    "        \n",
    "        if not index_path.exists() or not metadata_path.exists():\n",
    "            raise FileNotFoundError(f\"Index {index_name} not found at {self.base_path}\")\n",
    "        \n",
    "        logging.info(f\"Loading index {index_name}\")\n",
    "        self.indices[index_name] = faiss.read_index(str(index_path))\n",
    "        \n",
    "        with open(metadata_path, 'rb') as f:\n",
    "            self.metadata[index_name] = pickle.load(f)\n",
    "            \n",
    "        logging.info(f\"Loaded index {index_name} with {len(self.metadata[index_name])} chunks\")\n",
    "        \n",
    "    def get_metadata_by_id(self, index_name: str, chunk_id: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get metadata for a chunk by its ID\n",
    "        \n",
    "        Args:\n",
    "            index_name: Name of the index\n",
    "            chunk_id: ID of the chunk\n",
    "            \n",
    "        Returns:\n",
    "            Chunk metadata\n",
    "        \"\"\"\n",
    "        # Find metadata by chunk_id\n",
    "        for idx, metadata in self.metadata[index_name].items():\n",
    "            if metadata.get('id', metadata.get('chunk_id')) == chunk_id:\n",
    "                return metadata\n",
    "        return None\n",
    "    \n",
    "    def get_faiss_id_by_chunk_id(self, index_name: str, chunk_id: str) -> int:\n",
    "        \"\"\"\n",
    "        Get FAISS index ID for a chunk\n",
    "        \n",
    "        Args:\n",
    "            index_name: Name of the index\n",
    "            chunk_id: ID of the chunk\n",
    "            \n",
    "        Returns:\n",
    "            FAISS index ID\n",
    "        \"\"\"\n",
    "        for idx, metadata in self.metadata[index_name].items():\n",
    "            if metadata.get('id', metadata.get('chunk_id')) == chunk_id:\n",
    "                return idx\n",
    "        return -1\n",
    "    \n",
    "    def search(self, index_name: str, query_embedding: np.ndarray, k: int = 5) -> List[Tuple[int, float]]:\n",
    "        \"\"\"\n",
    "        Search FAISS index\n",
    "        \n",
    "        Args:\n",
    "            index_name: Name of the index to search\n",
    "            query_embedding: Query embedding vector\n",
    "            k: Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List of (idx, distance) tuples\n",
    "        \"\"\"\n",
    "        if index_name not in self.indices:\n",
    "            raise ValueError(f\"Index {index_name} not loaded\")\n",
    "            \n",
    "        distances, indices = self.indices[index_name].search(\n",
    "            query_embedding.reshape(1, -1), k\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            if idx != -1:\n",
    "                results.append((idx, float(distances[0][i])))\n",
    "                \n",
    "        return results\n",
    "    \n",
    "    def search_by_ids(self, index_name: str, query_embedding: np.ndarray, chunk_ids: List[str], \n",
    "                      k: int = 5) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Search only within specified chunk IDs\n",
    "        \n",
    "        Args:\n",
    "            index_name: Name of the index to search\n",
    "            query_embedding: Query embedding vector\n",
    "            chunk_ids: List of chunk IDs to search within\n",
    "            k: Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List of (chunk_id, similarity) tuples\n",
    "        \"\"\"\n",
    "        if index_name not in self.indices:\n",
    "            raise ValueError(f\"Index {index_name} not loaded\")\n",
    "            \n",
    "        # Get FAISS IDs for chunk IDs\n",
    "        faiss_ids = []\n",
    "        for chunk_id in chunk_ids:\n",
    "            faiss_id = self.get_faiss_id_by_chunk_id(index_name, chunk_id)\n",
    "            if faiss_id != -1:\n",
    "                faiss_ids.append(faiss_id)\n",
    "                \n",
    "        if not faiss_ids:\n",
    "            return []\n",
    "            \n",
    "        # Get embeddings for these IDs\n",
    "        embeddings = np.vstack([\n",
    "            self.indices[index_name].reconstruct(faiss_id) \n",
    "            for faiss_id in faiss_ids\n",
    "        ])\n",
    "        \n",
    "        # Compute similarities\n",
    "        query_norm = np.linalg.norm(query_embedding)\n",
    "        embedding_norms = np.linalg.norm(embeddings, axis=1)\n",
    "        similarities = np.dot(embeddings, query_embedding) / (embedding_norms * query_norm)\n",
    "        \n",
    "        # Create result tuples\n",
    "        results = []\n",
    "        for i, faiss_id in enumerate(faiss_ids):\n",
    "            chunk_id = self.metadata[index_name][faiss_id].get('id', self.metadata[index_name][faiss_id].get('chunk_id'))\n",
    "            results.append((chunk_id, float(similarities[i])))\n",
    "            \n",
    "        # Sort by similarity (descending)\n",
    "        results = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "        return results[:k]\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "492d9624",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mFAISSIndexManager\u001b[39;00m:\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Manager for FAISS indices\"\"\"\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, base_path: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_faiss\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "Cell \u001b[1;32mIn[1], line 36\u001b[0m, in \u001b[0;36mFAISSIndexManager\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata[index_name] \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     34\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata[index_name])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_metadata_by_id\u001b[39m(\u001b[38;5;28mself\u001b[39m, index_name: \u001b[38;5;28mstr\u001b[39m, chunk_id: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    Get metadata for a chunk by its ID\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;124;03m        Chunk metadata\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;66;03m# Find metadata by chunk_id\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Dict' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "class FAISSIndexManager:\n",
    "    \"\"\"Manager for FAISS indices\"\"\"\n",
    "    \n",
    "    def __init__(self, base_path: str = 'NEW_faiss'):\n",
    "        \"\"\"\n",
    "        Initialize FAISS index manager\n",
    "        \n",
    "        Args:\n",
    "            base_path: Path to FAISS index directory\n",
    "        \"\"\"\n",
    "        self.base_path = Path(base_path)\n",
    "        self.indices = {}\n",
    "        self.metadata = {}\n",
    "        \n",
    "    def load_index(self, index_name: str):\n",
    "        \"\"\"\n",
    "        Load a FAISS index and its metadata\n",
    "        \n",
    "        Args:\n",
    "            index_name: Name of the index to load\n",
    "        \"\"\"\n",
    "        index_path = self.base_path / f\"{index_name}.index\"\n",
    "        metadata_path = self.base_path / f\"{index_name}_metadata.pkl\"\n",
    "        \n",
    "        if not index_path.exists() or not metadata_path.exists():\n",
    "            raise FileNotFoundError(f\"Index {index_name} not found at {self.base_path}\")\n",
    "        \n",
    "        logging.info(f\"Loading index {index_name}\")\n",
    "        self.indices[index_name] = faiss.read_index(str(index_path))\n",
    "        \n",
    "        with open(metadata_path, 'rb') as f:\n",
    "            self.metadata[index_name] = pickle.load(f)\n",
    "            \n",
    "        logging.info(f\"Loaded index {index_name} with {len(self.metadata[index_name])} chunks\")\n",
    "        \n",
    "    def get_metadata_by_id(self, index_name: str, chunk_id: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get metadata for a chunk by its ID\n",
    "        \n",
    "        Args:\n",
    "            index_name: Name of the index\n",
    "            chunk_id: ID of the chunk\n",
    "            \n",
    "        Returns:\n",
    "            Chunk metadata\n",
    "        \"\"\"\n",
    "        # Find metadata by chunk_id\n",
    "        for idx, metadata in self.metadata[index_name].items():\n",
    "            if metadata.get('id', metadata.get('chunk_id')) == chunk_id:\n",
    "                return metadata\n",
    "        return None\n",
    "    \n",
    "    def get_faiss_id_by_chunk_id(self, index_name: str, chunk_id: str) -> int:\n",
    "        \"\"\"\n",
    "        Get FAISS index ID for a chunk\n",
    "        \n",
    "        Args:\n",
    "            index_name: Name of the index\n",
    "            chunk_id: ID of the chunk\n",
    "            \n",
    "        Returns:\n",
    "            FAISS index ID\n",
    "        \"\"\"\n",
    "        for idx, metadata in self.metadata[index_name].items():\n",
    "            if metadata.get('id', metadata.get('chunk_id')) == chunk_id:\n",
    "                return idx\n",
    "        return -1\n",
    "    \n",
    "    def search(self, index_name: str, query_embedding: np.ndarray, k: int = 5) -> List[Tuple[int, float]]:\n",
    "        \"\"\"\n",
    "        Search FAISS index\n",
    "        \n",
    "        Args:\n",
    "            index_name: Name of the index to search\n",
    "            query_embedding: Query embedding vector\n",
    "            k: Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List of (idx, distance) tuples\n",
    "        \"\"\"\n",
    "        if index_name not in self.indices:\n",
    "            raise ValueError(f\"Index {index_name} not loaded\")\n",
    "            \n",
    "        distances, indices = self.indices[index_name].search(\n",
    "            query_embedding.reshape(1, -1), k\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            if idx != -1:\n",
    "                results.append((idx, float(distances[0][i])))\n",
    "                \n",
    "        return results\n",
    "    \n",
    "    def search_by_ids(self, index_name: str, query_embedding: np.ndarray, chunk_ids: List[str], \n",
    "                      k: int = 5) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Search only within specified chunk IDs\n",
    "        \n",
    "        Args:\n",
    "            index_name: Name of the index to search\n",
    "            query_embedding: Query embedding vector\n",
    "            chunk_ids: List of chunk IDs to search within\n",
    "            k: Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List of (chunk_id, similarity) tuples\n",
    "        \"\"\"\n",
    "        if index_name not in self.indices:\n",
    "            raise ValueError(f\"Index {index_name} not loaded\")\n",
    "            \n",
    "        # Get FAISS IDs for chunk IDs\n",
    "        faiss_ids = []\n",
    "        for chunk_id in chunk_ids:\n",
    "            faiss_id = self.get_faiss_id_by_chunk_id(index_name, chunk_id)\n",
    "            if faiss_id != -1:\n",
    "                faiss_ids.append(faiss_id)\n",
    "                \n",
    "        if not faiss_ids:\n",
    "            return []\n",
    "            \n",
    "        # Get embeddings for these IDs\n",
    "        embeddings = np.vstack([\n",
    "            self.indices[index_name].reconstruct(faiss_id) \n",
    "            for faiss_id in faiss_ids\n",
    "        ])\n",
    "        \n",
    "        # Compute similarities\n",
    "        query_norm = np.linalg.norm(query_embedding)\n",
    "        embedding_norms = np.linalg.norm(embeddings, axis=1)\n",
    "        similarities = np.dot(embeddings, query_embedding) / (embedding_norms * query_norm)\n",
    "        \n",
    "        # Create result tuples\n",
    "        results = []\n",
    "        for i, faiss_id in enumerate(faiss_ids):\n",
    "            chunk_id = self.metadata[index_name][faiss_id].get('id', self.metadata[index_name][faiss_id].get('chunk_id'))\n",
    "            results.append((chunk_id, float(similarities[i])))\n",
    "            \n",
    "        # Sort by similarity (descending)\n",
    "        results = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "        return results[:k]\n",
    "\n",
    "   \n",
    "\n",
    "class AdvancedHybridRetriever(BaseRetriever):\n",
    "    \"\"\"\n",
    "    Advanced hybrid retriever with BM25, reranking, and hierarchical exploration\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2', \n",
    "                 reranker_model: str = 'cross-encoder/ms-marco-MiniLM-L-6-v2',\n",
    "                 candidate_pool_size: int = 100,\n",
    "                 base_path: str = 'NEW_faiss'):\n",
    "        \"\"\"\n",
    "        Initialize the advanced hybrid retriever\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name of the sentence transformer model\n",
    "            reranker_model: Name of the cross-encoder model for reranking\n",
    "            candidate_pool_size: Number of candidate chunks to retrieve initially\n",
    "            base_path: Path to FAISS indices\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.name = \"advanced_hybrid\"\n",
    "        self.model_name = model_name\n",
    "        self.candidate_pool_size = candidate_pool_size\n",
    "        \n",
    "        # Set up components\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        self.encoder = SentenceTransformer(model_name)\n",
    "        self.reranker = CrossEncoder(reranker_model)\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "        # Set up FAISS index manager\n",
    "        self.index_manager = FAISSIndexManager(base_path)\n",
    "        \n",
    "        # Set up BM25 scorer\n",
    "        self.bm25_scorer = BM25Scorer()\n",
    "        \n",
    "        # Indices that we'll use\n",
    "        self.metadata_index = 's3_metadata'  # For initial retrieval\n",
    "        self.content_index = 's3_content'    # For content-based comparisons\n",
    "        self.combined_index = 's3_combined'  # For final results\n",
    "        \n",
    "        # Tracking variables for performance analysis\n",
    "        self.hierarchical_improvements = 0\n",
    "        self.queries_processed = 0\n",
    "        \n",
    "    def initialize(self):\n",
    "        \"\"\"Load indices and prepare BM25 scorer\"\"\"\n",
    "        # Load indices\n",
    "        self.index_manager.load_index(self.metadata_index)\n",
    "        self.index_manager.load_index(self.content_index)\n",
    "        self.index_manager.load_index(self.combined_index)\n",
    "        \n",
    "        # Initialize BM25 scorer with chunks from combined index\n",
    "        chunks = [\n",
    "            metadata for metadata in self.index_manager.metadata[self.combined_index].values()\n",
    "        ]\n",
    "        self.bm25_scorer.index_chunks(chunks)\n",
    "        \n",
    "        logging.info(\"AdvancedHybridRetriever initialized successfully\")\n",
    "        \n",
    "    def get_hierarchical_chunks(self, chunk_ids: List[str]) -> Set[str]:\n",
    "        \"\"\"\n",
    "        Get parent and child chunks for a set of chunk IDs\n",
    "        \n",
    "        Args:\n",
    "            chunk_ids: List of chunk IDs\n",
    "            \n",
    "        Returns:\n",
    "            Set of related chunk IDs (parents and children)\n",
    "        \"\"\"\n",
    "        related_chunks = set()\n",
    "        \n",
    "        for chunk_id in chunk_ids:\n",
    "            # Get chunk metadata\n",
    "            chunk = self.index_manager.get_metadata_by_id(self.combined_index, chunk_id)\n",
    "            if not chunk:\n",
    "                continue\n",
    "                \n",
    "            # Add parent chunks\n",
    "            if 'parent' in chunk and chunk['parent']:\n",
    "                if isinstance(chunk['parent'], list):\n",
    "                    related_chunks.update(chunk['parent'])\n",
    "                else:\n",
    "                    related_chunks.add(chunk['parent'])\n",
    "                    \n",
    "            # Add child chunks\n",
    "            if 'children' in chunk and chunk['children']:\n",
    "                if isinstance(chunk['children'], list):\n",
    "                    related_chunks.update(chunk['children'])\n",
    "                else:\n",
    "                    related_chunks.add(chunk['children'])\n",
    "                    \n",
    "        # Remove any IDs that were in the original set\n",
    "        related_chunks = related_chunks - set(chunk_ids)\n",
    "        return related_chunks\n",
    "        \n",
    "    def retrieve(self, query: str, k: int = 3, use_hierarchical: bool = True) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant chunks using the advanced hybrid approach\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            k: Number of results to return\n",
    "            use_hierarchical: Whether to use hierarchical exploration\n",
    "            \n",
    "        Returns:\n",
    "            List of chunk results with relevance scores\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        self.queries_processed += 1\n",
    "        \n",
    "        # Step 1: Get query embedding\n",
    "        query_embedding = self.encoder.encode(query)\n",
    "        \n",
    "        # Step 2: Use BM25 to get initial candidates\n",
    "        logging.info(f\"Getting initial candidates using BM25 (pool size: {self.candidate_pool_size})\")\n",
    "        bm25_results = self.bm25_scorer.get_top_k(query, k=self.candidate_pool_size)\n",
    "        candidate_ids = [chunk_id for chunk_id, _ in bm25_results]\n",
    "        \n",
    "        # Step 3: Rerank candidates using dense embeddings\n",
    "        logging.info(f\"Reranking {len(candidate_ids)} candidates using dense embeddings\")\n",
    "        vector_results = self.index_manager.search_by_ids(\n",
    "            self.combined_index, query_embedding, candidate_ids, k=self.candidate_pool_size\n",
    "        )\n",
    "        \n",
    "        # Step 4: Get top-k chunks for further reranking\n",
    "        top_k_candidates = vector_results[:k*2]  # Get 2*k for reranking\n",
    "        top_k_ids = [chunk_id for chunk_id, _ in top_k_candidates]\n",
    "        \n",
    "        # Step 5: Prepare texts for reranker\n",
    "        texts_to_rerank = []\n",
    "        for chunk_id in top_k_ids:\n",
    "            chunk = self.index_manager.get_metadata_by_id(self.combined_index, chunk_id)\n",
    "            if chunk and 'content' in chunk:\n",
    "                texts_to_rerank.append(chunk['content'])\n",
    "            else:\n",
    "                texts_to_rerank.append(\"\")  # Empty string as placeholder\n",
    "                \n",
    "        # Step 6: Rerank using cross-encoder\n",
    "        logging.info(f\"Reranking top {len(texts_to_rerank)} candidates using cross-encoder\")\n",
    "        rerank_inputs = [[query, text] for text in texts_to_rerank]\n",
    "        rerank_scores = self.reranker.predict(rerank_inputs)\n",
    "        \n",
    "        # Combine chunk IDs with rerank scores\n",
    "        reranked_results = list(zip(top_k_ids, rerank_scores))\n",
    "        reranked_results.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Step 7: Get top-k results\n",
    "        top_k_results = reranked_results[:k]\n",
    "        final_chunk_ids = [chunk_id for chunk_id, _ in top_k_results]\n",
    "        \n",
    "        # Step 8: Hierarchical exploration (optional)\n",
    "        hierarchical_helped = False\n",
    "        if use_hierarchical:\n",
    "            logging.info(\"Performing hierarchical exploration\")\n",
    "            # Get hierarchically related chunks\n",
    "            related_ids = self.get_hierarchical_chunks(final_chunk_ids)\n",
    "            \n",
    "            if related_ids:\n",
    "                logging.info(f\"Found {len(related_ids)} related chunks\")\n",
    "                # Get similarity scores for related chunks\n",
    "                related_results = self.index_manager.search_by_ids(\n",
    "                    self.combined_index, query_embedding, list(related_ids), k=len(related_ids)\n",
    "                )\n",
    "                \n",
    "                # Only keep related chunks with scores above threshold\n",
    "                min_score = min([score for _, score in top_k_results])\n",
    "                good_related = [(chunk_id, score) for chunk_id, score in related_results if score > min_score]\n",
    "                \n",
    "                if good_related:\n",
    "                    logging.info(f\"Found {len(good_related)} related chunks with scores above threshold\")\n",
    "                    # Add to final results\n",
    "                    combined_results = top_k_results + good_related\n",
    "                    combined_results.sort(key=lambda x: x[1], reverse=True)\n",
    "                    \n",
    "                    # Check if hierarchical exploration helped\n",
    "                    if combined_results[0][0] not in final_chunk_ids:\n",
    "                        hierarchical_helped = True\n",
    "                        self.hierarchical_improvements += 1\n",
    "                        \n",
    "                    # Update final chunk IDs\n",
    "                    final_chunk_ids = [chunk_id for chunk_id, _ in combined_results[:k]]\n",
    "        \n",
    "        # Step 9: Prepare final results\n",
    "        final_results = []\n",
    "        for chunk_id in final_chunk_ids:\n",
    "            chunk = self.index_manager.get_metadata_by_id(self.combined_index, chunk_id)\n",
    "            if chunk:\n",
    "                result = chunk.copy()\n",
    "                result['hierarchical_boost'] = hierarchical_helped\n",
    "                final_results.append(result)\n",
    "                \n",
    "        elapsed = time.time() - start_time\n",
    "        logging.info(f\"Retrieval completed in {elapsed:.2f} seconds\")\n",
    "        if hierarchical_helped:\n",
    "            logging.info(\"Hierarchical exploration improved results!\")\n",
    "            \n",
    "        return final_results\n",
    "    \n",
    "    def get_hierarchical_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get statistics on hierarchical exploration effectiveness\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with statistics\n",
    "        \"\"\"\n",
    "        if self.queries_processed == 0:\n",
    "            return {\n",
    "                \"queries_processed\": 0,\n",
    "                \"hierarchical_improvements\": 0,\n",
    "                \"improvement_rate\": 0\n",
    "            }\n",
    "            \n",
    "        return {\n",
    "            \"queries_processed\": self.queries_processed,\n",
    "            \"hierarchical_improvements\": self.hierarchical_improvements,\n",
    "            \"improvement_rate\": self.hierarchical_improvements / self.queries_processed\n",
    "        }\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "302e16af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:17:56,205 - INFO - Use pytorch device_name: cpu\n",
      "2025-02-25 19:17:56,205 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "C:\\Users\\prana\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "286e59343d864e14a6575135316b2f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prana\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\prana\\.cache\\huggingface\\hub\\models--cross-encoder--ms-marco-MiniLM-L-6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "338500298c004f84b77aec71b8cea863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c168031b102e4348afe0f80005f9040e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49363dfba0d247b6ae282e52c297ed2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84df5e435f084c57addf9c823a8e31cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:18:01,704 - INFO - Use pytorch device: cpu\n",
      "2025-02-25 19:18:02,287 - INFO - Loading index s3_metadata\n",
      "2025-02-25 19:18:02,337 - INFO - Loaded index s3_metadata with 6214 chunks\n",
      "2025-02-25 19:18:02,338 - INFO - Loading index s3_content\n",
      "2025-02-25 19:18:02,396 - INFO - Loaded index s3_content with 6214 chunks\n",
      "2025-02-25 19:18:02,398 - INFO - Loading index s3_combined\n",
      "2025-02-25 19:18:02,471 - INFO - Loaded index s3_combined with 6214 chunks\n",
      "2025-02-25 19:18:02,472 - INFO - Indexing 6214 chunks for BM25 scoring\n",
      "2025-02-25 19:18:02,764 - INFO - BM25 indexing complete. Average document length: 82.43\n",
      "2025-02-25 19:18:02,765 - INFO - AdvancedHybridRetriever initialized successfully\n"
     ]
    }
   ],
   "source": [
    "# Initialize the retriever\n",
    "retriever = AdvancedHybridRetriever(\n",
    "    model_name='all-MiniLM-L6-v2',\n",
    "    reranker_model='cross-encoder/ms-marco-MiniLM-L-6-v2',\n",
    "    candidate_pool_size=100,\n",
    "    base_path='NEW_faiss'\n",
    ")\n",
    "\n",
    "# Load indices and prepare BM25\n",
    "retriever.initialize()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "084f9b14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85fa714b819f463d9826d0ea24254029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:19:19,593 - INFO - Getting initial candidates using BM25 (pool size: 100)\n",
      "2025-02-25 19:19:19,720 - INFO - Reranking 100 candidates using dense embeddings\n",
      "2025-02-25 19:19:19,734 - INFO - Reranking top 6 candidates using cross-encoder\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb16b61b329b42ef90b61b765f3a0cec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:19:20,511 - INFO - Performing hierarchical exploration\n",
      "2025-02-25 19:19:20,512 - INFO - Found 18 related chunks\n",
      "2025-02-25 19:19:20,516 - INFO - Found 18 related chunks with scores above threshold\n",
      "2025-02-25 19:19:20,517 - INFO - Retrieval completed in 4.71 seconds\n",
      "2025-02-25 19:19:20,518 - INFO - Hierarchical exploration improved results!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hierarchical improvement rate: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Make a query\n",
    "results = retriever.retrieve(\n",
    "    \"What are the prerequisites for locking a vault in S3 Glacier, and how does this feature enforce compliance?\", \n",
    "    k=3, \n",
    "    use_hierarchical=True\n",
    ")\n",
    "\n",
    "# Check if hierarchical exploration helped\n",
    "stats = retriever.get_hierarchical_stats()\n",
    "print(f\"Hierarchical improvement rate: {stats['improvement_rate']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "583cea30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'chunk_1843',\n",
       "  'content': 'Amazon Simple Storage Service User Guide\\nthen transitions objects to S3 Glacier Deep Archive after 20 days. In this case the S3 Glacier Deep \\nArchive transition must occur after at least 94 days.\\nYou can specify two rules to accomplish this, but you pay the minimum duration storage charges. \\nFor more information about cost considerations, see Amazon S3 pricing.\\nFor more information about creating a S3 Lifecycle, see Setting an S3 Lifecycle conguration on a \\nbucket.\\nTransitioning to the S3 Glacier Flexible Retrieval and S3 Glacier Deep Archive \\nstorage classes (object archival)\\nBy using an S3 Lifecycle conguration, you can transition objects to the S3 Glacier Flexible \\nRetrieval or S3 Glacier Deep Archive storage classes for archiving.\\nBefore you archive objects, review the following sections for relevant considerations.\\nGeneral considerations\\nThe following are the general considerations for you to consider before you archive objects:\\nEncrypted objects remain encrypted throughout the storage class transition process.\\nObjects that are stored in the S3 Glacier Flexible Retrieval or S3 Glacier Deep Archive storage \\nclasses are not available in real time.\\nArchived objects are Amazon S3 objects, but before you can access an archived object, you must \\nrst restore a temporary copy of it. The restored object copy is available only for the duration \\nthat you specify in the restore request. After that, Amazon S3 deletes the temporary copy, and \\nthe object remains archived in S3 Glacier Flexible Retrieval.\\nYou can restore an object by using the Amazon S3 console or programmatically by using the \\nAWS SDK wrapper libraries or the Amazon S3 REST API in your code. For more information, see\\nRestoring an archived object.\\nObjects that are stored in the S3 Glacier Flexible Retrieval storage class can only be transitioned \\nto the S3 Glacier Deep Archive storage class.\\nYou can use an S3 Lifecycle conguration rule to convert the storage class of an object from S3 \\nGlacier Flexible Retrieval to the S3 Glacier Deep Archive storage class only. If you want to change \\nthe storage class of an object that is stored in S3 Glacier Flexible Retrieval to a storage class \\nTransitioning objects API Version 2006-03-01 1837',\n",
       "  'topics': ['s3', 'class', 'objects', 'lifecycle', 'object'],\n",
       "  'summary': 'Amazon S3 Lifecycle configurations enable transitioning objects to archival storage classes like S3 Glacier Flexible Retrieval or Deep Archive based on specific retention rules and considerations.',\n",
       "  'parent': ['chunk_1820', 'chunk_1822', 'chunk_1821'],\n",
       "  'children': ['chunk_1844', 'chunk_1846', 'chunk_1845'],\n",
       "  'embedded_features': ['content', 'topics', 'summary', 'parent', 'children'],\n",
       "  'hierarchical_boost': True},\n",
       " {'id': 'chunk_1818',\n",
       "  'content': \"Amazon Simple Storage Service User Guide\\nSome S3 Glacier storage classes are archival, which means the objects stored in those classes are \\narchived and not available for real-time access. For more information, see Understanding archival \\nstorage in S3 Glacier Flexible Retrieval and S3 Glacier Deep Archive.\\nStorage classes designed for less frequent access patterns with longer retrieval times oer lower \\nstorage costs. For pricing information, see https://aws.amazon.com/s3/pricing/.\\nThe following table summarizes the key points to consider when choosing a S3 Glacier storage \\nclass:\\nS3 Glacier Instant Retrieval\\nWe recommend using S3 Glacier Instant Retrieval for long-term data that's accessed once per \\nquarter and requires millisecond retrieval times. This storage class is ideal for performance-\\nsensitive use cases such as image hosting, le-sharing applications, and storing medical records for \\naccess during appointments.\\nS3 Glacier Instant Retrieval storage class oers real-time access to your objects with the same \\nlatency and throughput performance as the S3 Standard-IA storage class. When compared to S3 \\nStandard-IA, S3 Glacier Instant Retrieval has lower storage costs but higher data access costs.\\nThere is a minimum object size of 128 KB for data stored in the S3 Glacier Instant Retrieval storage \\nclass. This storage class also has a minimum storage duration period of 90 days.\\nS3 Glacier Flexible Retrieval\\nWe recommend using S3 Glacier Flexible Retrieval for archive data that's accessed one to two times \\na year and doesn't require immediate access. S3 Glacier Flexible Retrieval oers exible retrieval \\ntimes to help you balance costs, with access times ranging from a few minutes to hours, and free \\nbulk retrievals. This storage class is ideal for backup and disaster recovery.\\nObjects stored in S3 Glacier Flexible Retrieval are archived and not available for real-time access. \\nFor more information, see Understanding archival storage in S3 Glacier Flexible Retrieval and S3 \\nGlacier Deep Archive. To access these objects, you rst initiate a restore request which creates a \\ntemporary copy of the object that you can access when the request completes. For information, see\\nWorking with archived objects. When you restore an object, you can choose a retrieval tier to meet \\nyour use case, with lower costs for longer restore times.\\nThe following retrieval tiers are available for S3 Glacier Flexible Retrieval:\\nAmazon S3 Glacier storage classes API Version 2006-03-01 1812\",\n",
       "  'topics': ['s3', 'class', 'access', 'storage', 'data'],\n",
       "  'summary': 'Amazon S3 Glacier storage classes provide cost-effective archival solutions with varying retrieval times, from real-time (Instant Retrieval) to delayed access (Flexible Retrieval) for infrequent data access needs.',\n",
       "  'parent': ['chunk_644', 'chunk_679', 'chunk_1429'],\n",
       "  'children': ['chunk_1819', 'chunk_1845', 'chunk_1821'],\n",
       "  'embedded_features': ['content', 'topics', 'summary', 'parent', 'children'],\n",
       "  'hierarchical_boost': True},\n",
       " {'id': 'chunk_1820',\n",
       "  'content': \"Amazon Simple Storage Service User Guide\\nThe minimum storage duration for objects in S3 Glacier Deep Archive storage class is 180 days.\\nS3 Glacier Deep Archive requires 40 KB of additional metadata for each object. This includes 32 KB \\nof metadata required to identify and retrieve your data, which is charged at the default rate for S3 \\nGlacier Deep Archive. An additional 8 KB data is required to maintain the user-dened name and \\nmetadata for archived objects, and is charged at the S3 Standard rate.\\nUnderstanding archival storage in S3 Glacier Flexible Retrieval and S3 Glacier \\nDeep Archive\\nS3 Glacier Flexible Retrieval and S3 Glacier Deep Archive are archival storage classes. This means \\nthat when you store an object in these storage classes that object is archived, and cannot be \\naccessed directly. To access an archived object, you submit a restore request for it, and then \\nwait for the service to restore the object. The restore request restores a temporary copy of the \\nobject, and that copy is deleted when the duration you specied in the request expires. For more \\ninformation see Working with archived objects.\\nThe transition of objects to the S3 Glacier Deep Archive storage class can go only one way.\\nIf you want to change the storage class of an archived object to another storage class, you \\nmust use the restore operation to make a temporary copy of the object rst. Then use the copy \\noperation to overwrite the object specifying S3 Standard, S3 Intelligent-Tiering, S3 Standard-IA, \\nS3 One Zone-IA, S3 Glacier Instant Retrieval, S3 Glacier Flexible Retrieval, or Reduced Redundancy \\nStorage as the storage class.\\nNote\\nThe Copy operation for restored objects isn't supported in the Amazon S3 console for \\nobjects in the S3 Glacier Flexible Retrieval or S3 Glacier Deep Archive storage classes. For \\nthis type of Copy operation, use the AWS Command Line Interface (AWS CLI), the AWS \\nSDKs, or the REST API.\\nYou can restore archived objects in these storage classes with up to 1,000 transactions per second \\n(TPS) of object restore requests per account per AWS Region.\\nCost considerations\\nIf you are planning to archive infrequently accessed data for a period of months or years, the S3 \\nGlacier Flexible Retrieval and S3 Glacier Deep Archive storage classes can reduce your storage costs. \\nAmazon S3 Glacier storage classes API Version 2006-03-01 1814\",\n",
       "  'topics': ['s3', 'copy', 'storage', 'object', 'class'],\n",
       "  'summary': 'S3 Glacier Flexible Retrieval and Deep Archive are cost-effective archival storage classes requiring restore requests for access, with Deep Archive having a 180-day minimum duration and additional metadata overhead.',\n",
       "  'parent': ['chunk_1817', 'chunk_492', 'chunk_1793'],\n",
       "  'children': ['chunk_1821', 'chunk_1845', 'chunk_1827'],\n",
       "  'embedded_features': ['content', 'topics', 'summary', 'parent', 'children'],\n",
       "  'hierarchical_boost': True}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50da1437",
   "metadata": {},
   "source": [
    "# 3 Custom retrievers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b36cf1",
   "metadata": {},
   "source": [
    "## With BM25 + Reranking + Hierarchical adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce27466a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Dict, Any, Tuple, Set\n",
    "import logging\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import CrossEncoder\n",
    "import spacy\n",
    "import faiss\n",
    "import math\n",
    "from collections import Counter, defaultdict\n",
    "from abc import ABC, abstractmethod\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class BaseRetriever(ABC):\n",
    "    \"\"\"Base abstract class for all retrievers\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_manager=None):\n",
    "        self.manager = embedding_manager\n",
    "        self.name = \"base\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def retrieve(self, query: str, k: int = 3) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant chunks for a given query\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            k: Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List of chunk results with relevance scores\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class BM25Scorer:\n",
    "    \"\"\"BM25 scoring implementation for keyword-based relevance\"\"\"\n",
    "    \n",
    "    def __init__(self, k1=1.5, b=0.75):\n",
    "        \"\"\"\n",
    "        Initialize BM25 scorer\n",
    "        \n",
    "        Args:\n",
    "            k1: Term frequency saturation parameter\n",
    "            b: Length normalization parameter\n",
    "        \"\"\"\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.doc_freqs = defaultdict(int)  # Document frequencies\n",
    "        self.doc_lengths = []  # Length of each document\n",
    "        self.avg_doc_length = 0\n",
    "        self.total_docs = 0\n",
    "        self.chunk_keywords = []  # Processed keywords for each chunk\n",
    "        self.chunk_ids = []  # Original chunk IDs\n",
    "        self.id_to_idx = {}  # Mapping from chunk_id to index\n",
    "        \n",
    "    def extract_keywords(self, chunk: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract keywords from a chunk\n",
    "        \n",
    "        Args:\n",
    "            chunk: The chunk to process\n",
    "            \n",
    "        Returns:\n",
    "            List of keywords\n",
    "        \"\"\"\n",
    "        keywords = []\n",
    "        \n",
    "        # Add topics if available\n",
    "        if 'topics' in chunk and chunk['topics']:\n",
    "            keywords.extend(chunk['topics'])\n",
    "            \n",
    "        # Add entity texts if available\n",
    "        if 'entities' in chunk and chunk['entities']:\n",
    "            keywords.extend([entity['text'].lower() for entity in chunk['entities']])\n",
    "            \n",
    "        # Add important words from content\n",
    "        if 'content' in chunk and chunk['content']:\n",
    "            # Simple content-based keywords\n",
    "            content_words = chunk['content'].lower().split()\n",
    "            # Filter out short words and common stopwords\n",
    "            content_words = [w for w in content_words if len(w) > 3]\n",
    "            keywords.extend(content_words)\n",
    "            \n",
    "        return list(set(keywords))  # Remove duplicates\n",
    "        \n",
    "    def index_chunks(self, chunks: List[Dict[str, Any]]):\n",
    "        \"\"\"\n",
    "        Index chunks for BM25 scoring\n",
    "        \n",
    "        Args:\n",
    "            chunks: List of chunks to index\n",
    "        \"\"\"\n",
    "        logging.info(f\"Indexing {len(chunks)} chunks for BM25 scoring\")\n",
    "        self.total_docs = len(chunks)\n",
    "        self.chunk_ids = [chunk.get('id', chunk.get('chunk_id', f'unknown_{i}')) for i, chunk in enumerate(chunks)]\n",
    "        \n",
    "        # Create mapping from chunk_id to index\n",
    "        self.id_to_idx = {chunk_id: i for i, chunk_id in enumerate(self.chunk_ids)}\n",
    "        \n",
    "        # Process each chunk\n",
    "        for chunk in chunks:\n",
    "            # Extract keywords\n",
    "            keywords = self.extract_keywords(chunk)\n",
    "            self.chunk_keywords.append(keywords)\n",
    "            \n",
    "            # Update document frequencies\n",
    "            for keyword in set(keywords):  # Use set to count each term once per document\n",
    "                self.doc_freqs[keyword] += 1\n",
    "            \n",
    "            # Update document lengths\n",
    "            self.doc_lengths.append(len(keywords))\n",
    "            \n",
    "        # Calculate average document length\n",
    "        self.avg_doc_length = sum(self.doc_lengths) / max(1, self.total_docs)\n",
    "        logging.info(f\"BM25 indexing complete. Average document length: {self.avg_doc_length:.2f}\")\n",
    "        \n",
    "    def score(self, query_keywords: List[str], doc_idx: int) -> float:\n",
    "        \"\"\"\n",
    "        Calculate BM25 score for a document\n",
    "        \n",
    "        Args:\n",
    "            query_keywords: List of keywords from the query\n",
    "            doc_idx: Index of the document to score\n",
    "            \n",
    "        Returns:\n",
    "            BM25 score\n",
    "        \"\"\"\n",
    "        score = 0\n",
    "        doc_length = self.doc_lengths[doc_idx]\n",
    "        doc_keywords = self.chunk_keywords[doc_idx]\n",
    "        \n",
    "        # Count query terms in document\n",
    "        doc_term_counts = Counter(doc_keywords)\n",
    "        \n",
    "        for query_term in query_keywords:\n",
    "            if query_term not in doc_term_counts:\n",
    "                continue\n",
    "                \n",
    "            # Calculate IDF\n",
    "            df = self.doc_freqs.get(query_term, 0)\n",
    "            if df == 0:\n",
    "                continue\n",
    "            idf = math.log((self.total_docs - df + 0.5) / (df + 0.5) + 1)\n",
    "            \n",
    "            # Calculate TF with saturation and length normalization\n",
    "            tf = doc_term_counts[query_term]\n",
    "            tf_normalized = ((tf * (self.k1 + 1)) / \n",
    "                           (tf + self.k1 * (1 - self.b + self.b * doc_length / self.avg_doc_length)))\n",
    "            \n",
    "            score += idf * tf_normalized\n",
    "            \n",
    "        return score\n",
    "    \n",
    "    def get_scores(self, query: str) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Get BM25 scores for all documents\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            \n",
    "        Returns:\n",
    "            List of (chunk_id, score) tuples\n",
    "        \"\"\"\n",
    "        # Process query keywords\n",
    "        query_keywords = query.lower().split()\n",
    "        \n",
    "        # Calculate scores for all documents\n",
    "        scores = []\n",
    "        for i in range(len(self.chunk_keywords)):\n",
    "            score = self.score(query_keywords, i)\n",
    "            scores.append((self.chunk_ids[i], score))\n",
    "        \n",
    "        # Normalize scores\n",
    "        max_score = max([score for _, score in scores]) if scores else 1\n",
    "        if max_score > 0:\n",
    "            normalized_scores = [(chunk_id, score/max_score) for chunk_id, score in scores]\n",
    "        else:\n",
    "            normalized_scores = scores\n",
    "            \n",
    "        return normalized_scores\n",
    "    \n",
    "    def get_top_k(self, query: str, k: int = 5) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Get top-k chunks by BM25 score\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            k: Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List of (chunk_id, score) tuples for top-k results\n",
    "        \"\"\"\n",
    "        scores = self.get_scores(query)\n",
    "        # Sort by score (descending)\n",
    "        sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "        return sorted_scores[:k]\n",
    "\n",
    "\n",
    "class FAISSIndexManager:\n",
    "    \"\"\"Manager for FAISS indices\"\"\"\n",
    "    \n",
    "    def __init__(self, base_path: str = 'NEW_faiss'):\n",
    "        \"\"\"\n",
    "        Initialize FAISS index manager\n",
    "        \n",
    "        Args:\n",
    "            base_path: Path to FAISS index directory\n",
    "        \"\"\"\n",
    "        self.base_path = Path(base_path)\n",
    "        self.indices = {}\n",
    "        self.metadata = {}\n",
    "        \n",
    "    def load_index(self, index_name: str):\n",
    "        \"\"\"\n",
    "        Load a FAISS index and its metadata\n",
    "        \n",
    "        Args:\n",
    "            index_name: Name of the index to load\n",
    "        \"\"\"\n",
    "        index_path = self.base_path / f\"{index_name}.index\"\n",
    "        metadata_path = self.base_path / f\"{index_name}_metadata.pkl\"\n",
    "        \n",
    "        if not index_path.exists() or not metadata_path.exists():\n",
    "            raise FileNotFoundError(f\"Index {index_name} not found at {self.base_path}\")\n",
    "        \n",
    "        logging.info(f\"Loading index {index_name}\")\n",
    "        self.indices[index_name] = faiss.read_index(str(index_path))\n",
    "        \n",
    "        with open(metadata_path, 'rb') as f:\n",
    "            self.metadata[index_name] = pickle.load(f)\n",
    "            \n",
    "        logging.info(f\"Loaded index {index_name} with {len(self.metadata[index_name])} chunks\")\n",
    "        \n",
    "    def get_metadata_by_id(self, index_name: str, chunk_id: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get metadata for a chunk by its ID\n",
    "        \n",
    "        Args:\n",
    "            index_name: Name of the index\n",
    "            chunk_id: ID of the chunk\n",
    "            \n",
    "        Returns:\n",
    "            Chunk metadata\n",
    "        \"\"\"\n",
    "        # Find metadata by chunk_id\n",
    "        for idx, metadata in self.metadata[index_name].items():\n",
    "            if metadata.get('id', metadata.get('chunk_id')) == chunk_id:\n",
    "                return metadata\n",
    "        return None\n",
    "    \n",
    "    def get_faiss_id_by_chunk_id(self, index_name: str, chunk_id: str) -> int:\n",
    "        \"\"\"\n",
    "        Get FAISS index ID for a chunk\n",
    "        \n",
    "        Args:\n",
    "            index_name: Name of the index\n",
    "            chunk_id: ID of the chunk\n",
    "            \n",
    "        Returns:\n",
    "            FAISS index ID\n",
    "        \"\"\"\n",
    "        for idx, metadata in self.metadata[index_name].items():\n",
    "            if metadata.get('id', metadata.get('chunk_id')) == chunk_id:\n",
    "                return idx\n",
    "        return -1\n",
    "    \n",
    "    def search(self, index_name: str, query_embedding: np.ndarray, k: int = 5) -> List[Tuple[int, float]]:\n",
    "        \"\"\"\n",
    "        Search FAISS index\n",
    "        \n",
    "        Args:\n",
    "            index_name: Name of the index to search\n",
    "            query_embedding: Query embedding vector\n",
    "            k: Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List of (idx, distance) tuples\n",
    "        \"\"\"\n",
    "        if index_name not in self.indices:\n",
    "            raise ValueError(f\"Index {index_name} not loaded\")\n",
    "            \n",
    "        distances, indices = self.indices[index_name].search(\n",
    "            query_embedding.reshape(1, -1), k\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            if idx != -1:\n",
    "                results.append((idx, float(distances[0][i])))\n",
    "                \n",
    "        return results\n",
    "    \n",
    "    def search_by_ids(self, index_name: str, query_embedding: np.ndarray, chunk_ids: List[str], \n",
    "                      k: int = 5) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Search only within specified chunk IDs\n",
    "        \n",
    "        Args:\n",
    "            index_name: Name of the index to search\n",
    "            query_embedding: Query embedding vector\n",
    "            chunk_ids: List of chunk IDs to search within\n",
    "            k: Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List of (chunk_id, similarity) tuples\n",
    "        \"\"\"\n",
    "        if index_name not in self.indices:\n",
    "            raise ValueError(f\"Index {index_name} not loaded\")\n",
    "            \n",
    "        # Get FAISS IDs for chunk IDs\n",
    "        faiss_ids = []\n",
    "        for chunk_id in chunk_ids:\n",
    "            faiss_id = self.get_faiss_id_by_chunk_id(index_name, chunk_id)\n",
    "            if faiss_id != -1:\n",
    "                faiss_ids.append(faiss_id)\n",
    "                \n",
    "        if not faiss_ids:\n",
    "            return []\n",
    "            \n",
    "        # Get embeddings for these IDs\n",
    "        embeddings = np.vstack([\n",
    "            self.indices[index_name].reconstruct(faiss_id) \n",
    "            for faiss_id in faiss_ids\n",
    "        ])\n",
    "        \n",
    "        # Compute similarities\n",
    "        query_norm = np.linalg.norm(query_embedding)\n",
    "        embedding_norms = np.linalg.norm(embeddings, axis=1)\n",
    "        similarities = np.dot(embeddings, query_embedding) / (embedding_norms * query_norm)\n",
    "        \n",
    "        # Create result tuples\n",
    "        results = []\n",
    "        for i, faiss_id in enumerate(faiss_ids):\n",
    "            chunk_id = self.metadata[index_name][faiss_id].get('id', self.metadata[index_name][faiss_id].get('chunk_id'))\n",
    "            results.append((chunk_id, float(similarities[i])))\n",
    "            \n",
    "        # Sort by similarity (descending)\n",
    "        results = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "        return results[:k]\n",
    "\n",
    "\n",
    "class AdvancedRetriever(BaseRetriever):\n",
    "    \"\"\"\n",
    "    Advanced retriever with BM25, reranking, and hierarchical exploration\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model_name: str = 'all-MiniLM-L6-v2', \n",
    "                 reranker_model: str = 'cross-encoder/ms-marco-MiniLM-L-6-v2',\n",
    "                 candidate_pool_size: int = 100,\n",
    "                 base_path: str = 'NEW_faiss',\n",
    "                 retriever_type: str = 'combined'):\n",
    "        \"\"\"\n",
    "        Initialize the advanced retriever\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name of the sentence transformer model\n",
    "            reranker_model: Name of the cross-encoder model for reranking\n",
    "            candidate_pool_size: Number of candidate chunks to retrieve initially\n",
    "            base_path: Path to FAISS indices\n",
    "            retriever_type: Type of retriever ('content', 'combined', or 'hybrid')\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.retriever_type = retriever_type\n",
    "        self.name = f\"advanced_{retriever_type}\"\n",
    "        self.model_name = model_name\n",
    "        self.candidate_pool_size = candidate_pool_size\n",
    "        \n",
    "        # Set up components\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        self.encoder = SentenceTransformer(model_name)\n",
    "        self.reranker = CrossEncoder(reranker_model)\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "        # Set up FAISS index manager\n",
    "        self.index_manager = FAISSIndexManager(base_path)\n",
    "        \n",
    "        # Set up BM25 scorer\n",
    "        self.bm25_scorer = BM25Scorer()\n",
    "        \n",
    "        # Indices that we'll use\n",
    "        self.metadata_index = 's3_metadata'  # For initial metadata exploration\n",
    "        self.content_index = 's3_content'    # For content-based retrieval\n",
    "        self.combined_index = 's3_combined'  # For combined retrieval\n",
    "        \n",
    "        # Set primary index based on retriever type\n",
    "        if retriever_type == 'content':\n",
    "            self.primary_index = self.content_index\n",
    "        elif retriever_type == 'combined':\n",
    "            self.primary_index = self.combined_index\n",
    "        elif retriever_type == 'hybrid':\n",
    "            # Hybrid will use metadata for initial retrieval, then rerank with combined\n",
    "            self.primary_index = self.combined_index\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown retriever type: {retriever_type}\")\n",
    "        \n",
    "        # Tracking variables for performance analysis\n",
    "        self.hierarchical_improvements = 0\n",
    "        self.queries_processed = 0\n",
    "        \n",
    "    # Fix for the AdvancedRetriever class\n",
    "    def initialize(self):\n",
    "        \"\"\"Load indices and prepare BM25 scorer\"\"\"\n",
    "        # Load only the indices you need\n",
    "        needed_indices = set([self.primary_index, self.metadata_index])\n",
    "        for index_name in needed_indices:\n",
    "            self.index_manager.load_index(index_name)\n",
    "\n",
    "        # Debug logging\n",
    "        logging.info(f\"Loaded primary index: {self.primary_index}\")\n",
    "        logging.info(f\"Primary index memory address: {id(self.index_manager.indices[self.primary_index])}\")\n",
    "\n",
    "        # Check metadata differences\n",
    "        sample_metadata = list(self.index_manager.metadata[self.primary_index].values())[0]\n",
    "        logging.info(f\"Sample metadata keys: {sample_metadata.keys()}\")\n",
    "\n",
    "        # Initialize BM25 scorer with chunks from primary index\n",
    "        chunks = [\n",
    "            metadata for metadata in self.index_manager.metadata[self.primary_index].values()\n",
    "        ]\n",
    "        self.bm25_scorer.index_chunks(chunks)\n",
    "\n",
    "        logging.info(f\"{self.retriever_type} retriever initialized successfully with {len(chunks)} chunks\")\n",
    "        \n",
    "    def get_hierarchical_chunks(self, chunk_ids: List[str]) -> Set[str]:\n",
    "        \"\"\"\n",
    "        Get parent and child chunks for a set of chunk IDs\n",
    "        \n",
    "        Args:\n",
    "            chunk_ids: List of chunk IDs\n",
    "            \n",
    "        Returns:\n",
    "            Set of related chunk IDs (parents and children)\n",
    "        \"\"\"\n",
    "        related_chunks = set()\n",
    "        \n",
    "        for chunk_id in chunk_ids:\n",
    "            # Get chunk metadata\n",
    "            chunk = self.index_manager.get_metadata_by_id(self.combined_index, chunk_id)\n",
    "            if not chunk:\n",
    "                continue\n",
    "                \n",
    "            # Add parent chunks\n",
    "            if 'parent' in chunk and chunk['parent']:\n",
    "                if isinstance(chunk['parent'], list):\n",
    "                    related_chunks.update(chunk['parent'])\n",
    "                else:\n",
    "                    related_chunks.add(chunk['parent'])\n",
    "                    \n",
    "            # Add child chunks\n",
    "            if 'children' in chunk and chunk['children']:\n",
    "                if isinstance(chunk['children'], list):\n",
    "                    related_chunks.update(chunk['children'])\n",
    "                else:\n",
    "                    related_chunks.add(chunk['children'])\n",
    "                    \n",
    "        # Remove any IDs that were in the original set\n",
    "        related_chunks = related_chunks - set(chunk_ids)\n",
    "        return related_chunks\n",
    "        \n",
    "    def retrieve(self, query: str, k: int = 3, use_hierarchical: bool = True) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant chunks using the advanced approach\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            k: Number of results to return\n",
    "            use_hierarchical: Whether to use hierarchical exploration\n",
    "            \n",
    "        Returns:\n",
    "            List of chunk results with relevance scores\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        self.queries_processed += 1\n",
    "        \n",
    "        # Step 1: Get query embedding\n",
    "        query_embedding = self.encoder.encode(query)\n",
    "        \n",
    "        # Step 2: Initial retrieval based on retriever type\n",
    "        if self.retriever_type == 'hybrid':\n",
    "            # Hybrid approach: First get BM25 candidates, then use vector search\n",
    "            logging.info(f\"Getting initial candidates using BM25 (pool size: {self.candidate_pool_size})\")\n",
    "            bm25_results = self.bm25_scorer.get_top_k(query, k=self.candidate_pool_size)\n",
    "            bm25_scores = {chunk_id: score for chunk_id, score in bm25_results}\n",
    "            candidate_ids = [chunk_id for chunk_id, _ in bm25_results]\n",
    "            \n",
    "            # Rerank candidates using dense embeddings\n",
    "            logging.info(f\"Reranking {len(candidate_ids)} candidates using dense embeddings\")\n",
    "            vector_results = self.index_manager.search_by_ids(\n",
    "                self.primary_index, query_embedding, candidate_ids, k=self.candidate_pool_size\n",
    "            )\n",
    "            \n",
    "            # Keep track of both scores\n",
    "            initial_scores = {}\n",
    "            for chunk_id, score in vector_results:\n",
    "                initial_scores[chunk_id] = {\n",
    "                    'vector': score,\n",
    "                    'bm25': bm25_scores.get(chunk_id, 0.0)\n",
    "                }\n",
    "            \n",
    "            # Get top candidates for further reranking\n",
    "            top_k_candidates = vector_results[:k*2]  # Get 2*k for reranking\n",
    "        else:\n",
    "            # Content or Combined: Use the primary index directly\n",
    "            logging.info(f\"Using {self.retriever_type} index for initial retrieval\")\n",
    "            \n",
    "            # First get BM25 scores for all chunks\n",
    "            bm25_results = self.bm25_scorer.get_top_k(query, k=self.candidate_pool_size)\n",
    "            bm25_scores = {chunk_id: score for chunk_id, score in bm25_results}\n",
    "            \n",
    "            # Then get vector search results\n",
    "            distances, indices = self.index_manager.indices[self.primary_index].search(\n",
    "                query_embedding.reshape(1, -1), self.candidate_pool_size\n",
    "            )\n",
    "            \n",
    "            # Convert to chunk IDs and scores\n",
    "            vector_results = []\n",
    "            initial_scores = {}\n",
    "            for i, idx in enumerate(indices[0]):\n",
    "                if idx != -1:\n",
    "                    chunk_id = self.index_manager.metadata[self.primary_index][idx].get('id', \n",
    "                                 self.index_manager.metadata[self.primary_index][idx].get('chunk_id'))\n",
    "                    similarity = 1 - (distances[0][i] / 2)  # Convert L2 distance to similarity\n",
    "                    vector_results.append((chunk_id, similarity))\n",
    "                    \n",
    "                    # Store both scores\n",
    "                    initial_scores[chunk_id] = {\n",
    "                        'vector': similarity,\n",
    "                        'bm25': bm25_scores.get(chunk_id, 0.0)\n",
    "                    }\n",
    "            \n",
    "            # Get top candidates for reranking\n",
    "            top_k_candidates = vector_results[:k*2]\n",
    "            \n",
    "        # Get chunk IDs for reranking\n",
    "        top_k_ids = [chunk_id for chunk_id, _ in top_k_candidates]\n",
    "        \n",
    "        # Step 3: Prepare texts for reranker\n",
    "        texts_to_rerank = []\n",
    "        for chunk_id in top_k_ids:\n",
    "            chunk = self.index_manager.get_metadata_by_id(self.primary_index, chunk_id)\n",
    "            if chunk and 'content' in chunk:\n",
    "                texts_to_rerank.append(chunk['content'])\n",
    "            else:\n",
    "                texts_to_rerank.append(\"\")  # Empty string as placeholder\n",
    "                \n",
    "        # Step 4: Rerank using cross-encoder\n",
    "        logging.info(f\"Reranking top {len(texts_to_rerank)} candidates using cross-encoder\")\n",
    "        rerank_inputs = [[query, text] for text in texts_to_rerank]\n",
    "        rerank_scores = self.reranker.predict(rerank_inputs)\n",
    "        \n",
    "        # Combine chunk IDs with rerank scores and update scores dictionary\n",
    "        reranked_results = []\n",
    "        for i, (chunk_id, score) in enumerate(zip(top_k_ids, rerank_scores)):\n",
    "            reranked_results.append((chunk_id, score))\n",
    "            if chunk_id in initial_scores:\n",
    "                initial_scores[chunk_id]['reranker'] = score\n",
    "        \n",
    "        # Sort by reranker score\n",
    "        reranked_results.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Step 5: Get top-k results\n",
    "        top_k_results = reranked_results[:k]\n",
    "        final_chunk_ids = [chunk_id for chunk_id, _ in top_k_results]\n",
    "        \n",
    "        # Step 6: Hierarchical exploration (optional)\n",
    "        hierarchical_helped = False\n",
    "        hierarchical_scores = {}\n",
    "        \n",
    "        if use_hierarchical:\n",
    "            logging.info(\"Performing hierarchical exploration\")\n",
    "            # Get hierarchically related chunks\n",
    "            related_ids = self.get_hierarchical_chunks(final_chunk_ids)\n",
    "            \n",
    "            if related_ids:\n",
    "                logging.info(f\"Found {len(related_ids)} related chunks\")\n",
    "                # Get similarity scores for related chunks\n",
    "                related_results = self.index_manager.search_by_ids(\n",
    "                    self.primary_index, query_embedding, list(related_ids), k=len(related_ids)\n",
    "                )\n",
    "                \n",
    "                # Get reranker scores for related chunks\n",
    "                related_texts = []\n",
    "                related_chunk_ids = [chunk_id for chunk_id, _ in related_results]\n",
    "                for chunk_id in related_chunk_ids:\n",
    "                    chunk = self.index_manager.get_metadata_by_id(self.primary_index, chunk_id)\n",
    "                    if chunk and 'content' in chunk:\n",
    "                        related_texts.append(chunk['content'])\n",
    "                    else:\n",
    "                        related_texts.append(\"\")\n",
    "                \n",
    "                if related_texts:\n",
    "                    # Only run reranker if we have texts\n",
    "                    related_inputs = [[query, text] for text in related_texts]\n",
    "                    related_rerank_scores = self.reranker.predict(related_inputs)\n",
    "                    \n",
    "                    # Add scores to dictionary\n",
    "                    for i, (chunk_id, score) in enumerate(zip(related_chunk_ids, related_rerank_scores)):\n",
    "                        hierarchical_scores[chunk_id] = {\n",
    "                            'reranker': score,\n",
    "                            'relationship': 'hierarchical'\n",
    "                        }\n",
    "                    \n",
    "                    # Only keep related chunks with scores above threshold\n",
    "                    min_score = min([score for _, score in top_k_results])\n",
    "                    good_related = [(chunk_id, score) for i, (chunk_id, _) in enumerate(related_results) \n",
    "                                   if related_rerank_scores[i] > min_score]\n",
    "                    \n",
    "                    if good_related:\n",
    "                        logging.info(f\"Found {len(good_related)} related chunks with scores above threshold\")\n",
    "                        # Add to final results\n",
    "                        combined_results = top_k_results + good_related\n",
    "                        combined_results.sort(key=lambda x: x[1], reverse=True)\n",
    "                        \n",
    "                        # Check if hierarchical exploration helped\n",
    "                        if combined_results[0][0] not in final_chunk_ids:\n",
    "                            hierarchical_helped = True\n",
    "                            self.hierarchical_improvements += 1\n",
    "                            logging.info(\"Hierarchical exploration improved results!\")\n",
    "                            \n",
    "                        # Update final chunk IDs (keeping within k limit)\n",
    "                        final_chunk_ids = [chunk_id for chunk_id, _ in combined_results[:k]]\n",
    "        \n",
    "        # Step 7: Prepare final results with all scores\n",
    "        final_results = []\n",
    "        \n",
    "        for i, chunk_id in enumerate(final_chunk_ids):\n",
    "            chunk = self.index_manager.get_metadata_by_id(self.primary_index, chunk_id)\n",
    "            if chunk:\n",
    "                result = chunk.copy()\n",
    "                \n",
    "                # Add scoring information\n",
    "                result['scores'] = {\n",
    "                    'rank': i + 1\n",
    "                }\n",
    "                \n",
    "                # Add initial scores if available\n",
    "                if chunk_id in initial_scores:\n",
    "                    result['scores'].update(initial_scores[chunk_id])\n",
    "                \n",
    "                # Add hierarchical information\n",
    "                if chunk_id in hierarchical_scores:\n",
    "                    result['scores'].update(hierarchical_scores[chunk_id])\n",
    "                    result['hierarchical_boost'] = True\n",
    "                else:\n",
    "                    result['hierarchical_boost'] = False\n",
    "                \n",
    "                # Add overall hierarchical helped flag\n",
    "                result['hierarchical_improved_results'] = hierarchical_helped\n",
    "                \n",
    "                final_results.append(result)\n",
    "                \n",
    "        elapsed = time.time() - start_time\n",
    "        logging.info(f\"Retrieval completed in {elapsed:.2f} seconds\")\n",
    "            \n",
    "        return final_results\n",
    "    \n",
    "    def get_hierarchical_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get statistics on hierarchical exploration effectiveness\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with statistics\n",
    "        \"\"\"\n",
    "        if self.queries_processed == 0:\n",
    "            return {\n",
    "                \"queries_processed\": 0,\n",
    "                \"hierarchical_improvements\": 0,\n",
    "                \"improvement_rate\": 0\n",
    "            }\n",
    "            \n",
    "        return {\n",
    "            \"queries_processed\": self.queries_processed,\n",
    "            \"hierarchical_improvements\": self.hierarchical_improvements,\n",
    "            \"improvement_rate\": self.hierarchical_improvements / self.queries_processed\n",
    "        }\n",
    "        \n",
    "\n",
    "class RetrieverFactory:\n",
    "    \"\"\"Factory for creating different types of retrievers\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_retrievers(\n",
    "        model_name: str = 'all-MiniLM-L6-v2',\n",
    "        reranker_model: str = 'cross-encoder/ms-marco-MiniLM-L-6-v2',\n",
    "        candidate_pool_size: int = 100,\n",
    "        base_path: str = 'NEW_faiss'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Create all three retriever types\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name of the encoder model\n",
    "            reranker_model: Name of the reranker model\n",
    "            candidate_pool_size: Size of the candidate pool\n",
    "            base_path: Path to FAISS indices\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of retrievers\n",
    "        \"\"\"\n",
    "        # Create the three retriever types\n",
    "        content_retriever = AdvancedRetriever(\n",
    "            model_name=model_name,\n",
    "            reranker_model=reranker_model,\n",
    "            candidate_pool_size=candidate_pool_size,\n",
    "            base_path=base_path,\n",
    "            retriever_type='content'\n",
    "        )\n",
    "        \n",
    "        combined_retriever = AdvancedRetriever(\n",
    "            model_name=model_name,\n",
    "            reranker_model=reranker_model,\n",
    "            candidate_pool_size=candidate_pool_size,\n",
    "            base_path=base_path,\n",
    "            retriever_type='combined'\n",
    "        )\n",
    "        \n",
    "        hybrid_retriever = AdvancedRetriever(\n",
    "            model_name=model_name,\n",
    "            reranker_model=reranker_model,\n",
    "            candidate_pool_size=candidate_pool_size,\n",
    "            base_path=base_path,\n",
    "            retriever_type='hybrid'\n",
    "        )\n",
    "        \n",
    "        # Initialize all retrievers\n",
    "        content_retriever.initialize()\n",
    "        combined_retriever.initialize()\n",
    "        hybrid_retriever.initialize()\n",
    "        \n",
    "        return {\n",
    "            'content': content_retriever,\n",
    "            'combined': combined_retriever,\n",
    "            'hybrid': hybrid_retriever\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "610d51c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:26:39,477 - INFO - Use pytorch device_name: cpu\n",
      "2025-02-25 19:26:39,477 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "C:\\Users\\prana\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "2025-02-25 19:26:40,288 - INFO - Use pytorch device: cpu\n",
      "2025-02-25 19:26:40,772 - INFO - Use pytorch device_name: cpu\n",
      "2025-02-25 19:26:40,773 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2025-02-25 19:26:41,487 - INFO - Use pytorch device: cpu\n",
      "2025-02-25 19:26:41,987 - INFO - Use pytorch device_name: cpu\n",
      "2025-02-25 19:26:41,988 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2025-02-25 19:26:42,900 - INFO - Use pytorch device: cpu\n",
      "2025-02-25 19:26:43,360 - INFO - Loading index s3_metadata\n",
      "2025-02-25 19:26:43,383 - INFO - Loaded index s3_metadata with 6214 chunks\n",
      "2025-02-25 19:26:43,384 - INFO - Loading index s3_content\n",
      "2025-02-25 19:26:43,415 - INFO - Loaded index s3_content with 6214 chunks\n",
      "2025-02-25 19:26:43,416 - INFO - Loading index s3_combined\n",
      "2025-02-25 19:26:43,450 - INFO - Loaded index s3_combined with 6214 chunks\n",
      "2025-02-25 19:26:43,451 - INFO - Indexing 6214 chunks for BM25 scoring\n",
      "2025-02-25 19:26:43,785 - INFO - BM25 indexing complete. Average document length: 82.43\n",
      "2025-02-25 19:26:43,786 - INFO - AdvancedHybridRetriever initialized successfully\n",
      "2025-02-25 19:26:43,787 - INFO - Loading index s3_metadata\n",
      "2025-02-25 19:26:43,810 - INFO - Loaded index s3_metadata with 6214 chunks\n",
      "2025-02-25 19:26:43,811 - INFO - Loading index s3_content\n",
      "2025-02-25 19:26:43,839 - INFO - Loaded index s3_content with 6214 chunks\n",
      "2025-02-25 19:26:43,840 - INFO - Loading index s3_combined\n",
      "2025-02-25 19:26:43,875 - INFO - Loaded index s3_combined with 6214 chunks\n",
      "2025-02-25 19:26:43,876 - INFO - Indexing 6214 chunks for BM25 scoring\n",
      "2025-02-25 19:26:44,198 - INFO - BM25 indexing complete. Average document length: 82.43\n",
      "2025-02-25 19:26:44,199 - INFO - AdvancedHybridRetriever initialized successfully\n",
      "2025-02-25 19:26:44,200 - INFO - Loading index s3_metadata\n",
      "2025-02-25 19:26:44,222 - INFO - Loaded index s3_metadata with 6214 chunks\n",
      "2025-02-25 19:26:44,223 - INFO - Loading index s3_content\n",
      "2025-02-25 19:26:44,524 - INFO - Loaded index s3_content with 6214 chunks\n",
      "2025-02-25 19:26:44,525 - INFO - Loading index s3_combined\n",
      "2025-02-25 19:26:44,561 - INFO - Loaded index s3_combined with 6214 chunks\n",
      "2025-02-25 19:26:44,562 - INFO - Indexing 6214 chunks for BM25 scoring\n",
      "2025-02-25 19:26:44,898 - INFO - BM25 indexing complete. Average document length: 82.43\n",
      "2025-02-25 19:26:44,898 - INFO - AdvancedHybridRetriever initialized successfully\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Import the AdvancedRetriever class\n",
    "# (This should be the whole code from the artifact)\n",
    "\n",
    "# Create all three retrievers\n",
    "retrievers = RetrieverFactory.create_retrievers(\n",
    "    model_name='all-MiniLM-L6-v2',\n",
    "    reranker_model='cross-encoder/ms-marco-MiniLM-L-6-v2',\n",
    "    candidate_pool_size=100,\n",
    "    base_path='NEW_faiss'\n",
    ")\n",
    "\n",
    "# Example query\n",
    "query = \"What are the prerequisites for locking a vault in S3 Glacier, and how does this feature enforce compliance?\"\n",
    "\n",
    "# Function to display results with detailed scores\n",
    "def display_results(retriever_name, results):\n",
    "    print(f\"\\n\\n=== {retriever_name.upper()} RETRIEVER RESULTS ===\")\n",
    "    print(f\"Top {len(results)} results:\")\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"\\n{i+1}. {result.get('id', result.get('chunk_id', 'unknown'))}\")\n",
    "        \n",
    "        # Print scores\n",
    "        if 'scores' in result:\n",
    "            print(\"   Scores:\")\n",
    "            for score_type, score_value in result['scores'].items():\n",
    "                print(f\"     {score_type}: {score_value:.4f}\" if isinstance(score_value, float) \n",
    "                      else f\"     {score_type}: {score_value}\")\n",
    "        \n",
    "        # Print hierarchical information\n",
    "        print(f\"   Hierarchical boost: {result.get('hierarchical_boost', False)}\")\n",
    "        \n",
    "        # Print content snippet\n",
    "        content = result.get('content', '')\n",
    "        if content:\n",
    "            print(f\"   Content snippet: {content[:100]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d97dd75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c7045f739f141fb91c90e4b5fecb20e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:26:59,414 - INFO - Using content index for initial retrieval\n",
      "2025-02-25 19:26:59,516 - INFO - Reranking top 10 candidates using cross-encoder\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f1ed8e52cac44d79b7c62bd4396d040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:27:00,523 - INFO - Performing hierarchical exploration\n",
      "2025-02-25 19:27:00,524 - INFO - Found 28 related chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdbb19f71e954bbea17bdbf81297bb00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:27:03,746 - INFO - Found 20 related chunks with scores above threshold\n",
      "2025-02-25 19:27:03,747 - INFO - Retrieval completed in 4.38 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== CONTENT RETRIEVER RESULTS ===\n",
      "Top 5 results:\n",
      "\n",
      "1. chunk_411\n",
      "   Scores:\n",
      "     rank: 1\n",
      "     vector: 0.7570\n",
      "     bm25: 0.5957\n",
      "     reranker: -8.184192657470703\n",
      "   Hierarchical boost: False\n",
      "   Content snippet: Amazon Simple Storage Service User Guide\n",
      "5. In the Bucket name eld, enter the name of the bucket th...\n",
      "\n",
      "2. chunk_109\n",
      "   Scores:\n",
      "     rank: 2\n",
      "     vector: 0.7919\n",
      "     bm25: 0.0000\n",
      "     reranker: -9.281982421875\n",
      "   Hierarchical boost: False\n",
      "   Content snippet: Amazon Simple Storage Service User Guide\n",
      "You can't use all of your available bandwidth over the int...\n",
      "\n",
      "3. chunk_375\n",
      "   Scores:\n",
      "     rank: 3\n",
      "     vector: 0.8085\n",
      "     bm25: 0.5619\n",
      "     reranker: -9.411375045776367\n",
      "   Hierarchical boost: False\n",
      "   Content snippet: Amazon Simple Storage Service User Guide\n",
      "inputS3Url   A presigned URL that can be used to fetch th...\n",
      "\n",
      "4. chunk_111\n",
      "   Scores:\n",
      "     rank: 4\n",
      "     vector: 0.7528\n",
      "     bm25: 0.6339\n",
      "     reranker: -10.308664321899414\n",
      "   Hierarchical boost: False\n",
      "   Content snippet: Amazon Simple Storage Service User Guide\n",
      "Use the AWS CLI and AWS SDKs. For more information, see De...\n",
      "\n",
      "5. chunk_112\n",
      "   Scores:\n",
      "     rank: 5\n",
      "     vector: 0.7969\n",
      "     bm25: 0.8760\n",
      "     reranker: -10.443737983703613\n",
      "   Hierarchical boost: False\n",
      "   Content snippet: Amazon Simple Storage Service User Guide\n",
      "After Transfer Acceleration is enabled, it can take up to 2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84ac02bc037942a091fd38a6543b3ec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:27:03,777 - INFO - Using combined index for initial retrieval\n",
      "2025-02-25 19:27:03,854 - INFO - Reranking top 10 candidates using cross-encoder\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cdd91d1310c4b38bdfc74c7983243b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:27:04,926 - INFO - Performing hierarchical exploration\n",
      "2025-02-25 19:27:04,927 - INFO - Found 28 related chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd9b7a49a1ed4dc2847f1dce78a9d848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:27:07,976 - INFO - Found 20 related chunks with scores above threshold\n",
      "2025-02-25 19:27:07,977 - INFO - Retrieval completed in 4.23 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== COMBINED RETRIEVER RESULTS ===\n",
      "Top 5 results:\n",
      "\n",
      "1. chunk_411\n",
      "   Scores:\n",
      "     rank: 1\n",
      "     vector: 0.7570\n",
      "     bm25: 0.5957\n",
      "     reranker: -8.184192657470703\n",
      "   Hierarchical boost: False\n",
      "   Content snippet: Amazon Simple Storage Service User Guide\n",
      "5. In the Bucket name eld, enter the name of the bucket th...\n",
      "\n",
      "2. chunk_109\n",
      "   Scores:\n",
      "     rank: 2\n",
      "     vector: 0.7919\n",
      "     bm25: 0.0000\n",
      "     reranker: -9.281982421875\n",
      "   Hierarchical boost: False\n",
      "   Content snippet: Amazon Simple Storage Service User Guide\n",
      "You can't use all of your available bandwidth over the int...\n",
      "\n",
      "3. chunk_375\n",
      "   Scores:\n",
      "     rank: 3\n",
      "     vector: 0.8085\n",
      "     bm25: 0.5619\n",
      "     reranker: -9.411375045776367\n",
      "   Hierarchical boost: False\n",
      "   Content snippet: Amazon Simple Storage Service User Guide\n",
      "inputS3Url   A presigned URL that can be used to fetch th...\n",
      "\n",
      "4. chunk_111\n",
      "   Scores:\n",
      "     rank: 4\n",
      "     vector: 0.7528\n",
      "     bm25: 0.6339\n",
      "     reranker: -10.308664321899414\n",
      "   Hierarchical boost: False\n",
      "   Content snippet: Amazon Simple Storage Service User Guide\n",
      "Use the AWS CLI and AWS SDKs. For more information, see De...\n",
      "\n",
      "5. chunk_112\n",
      "   Scores:\n",
      "     rank: 5\n",
      "     vector: 0.7969\n",
      "     bm25: 0.8760\n",
      "     reranker: -10.443737983703613\n",
      "   Hierarchical boost: False\n",
      "   Content snippet: Amazon Simple Storage Service User Guide\n",
      "After Transfer Acceleration is enabled, it can take up to 2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc10f94ad7e04e0683dd53d305c08c74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:27:08,008 - INFO - Getting initial candidates using BM25 (pool size: 100)\n",
      "2025-02-25 19:27:08,085 - INFO - Reranking 100 candidates using dense embeddings\n",
      "2025-02-25 19:27:08,096 - INFO - Reranking top 10 candidates using cross-encoder\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca7a6891a2fb4ccd95c228704d0c4422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:27:09,336 - INFO - Performing hierarchical exploration\n",
      "2025-02-25 19:27:09,337 - INFO - Found 28 related chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61634e85a801483dabfa2419422344a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:27:12,706 - INFO - Found 3 related chunks with scores above threshold\n",
      "2025-02-25 19:27:12,708 - INFO - Retrieval completed in 4.73 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== HYBRID RETRIEVER RESULTS ===\n",
      "Top 5 results:\n",
      "\n",
      "1. chunk_17\n",
      "   Scores:\n",
      "     rank: 1\n",
      "     vector: 0.4083\n",
      "     bm25: 1.0000\n",
      "     reranker: -0.7180487513542175\n",
      "   Hierarchical boost: False\n",
      "   Content snippet: Amazon Simple Storage Service User Guide\n",
      "data at the lowest costs in S3 Glacier Instant Retrieval, S...\n",
      "\n",
      "2. chunk_1429\n",
      "   Scores:\n",
      "     rank: 2\n",
      "     vector: 0.5160\n",
      "     bm25: 0.6031\n",
      "     reranker: -1.519490122795105\n",
      "   Hierarchical boost: False\n",
      "   Content snippet: Amazon Simple Storage Service User Guide\n",
      "that require WORM storage or simply to add an additional la...\n",
      "\n",
      "3. chunk_2483\n",
      "   Scores:\n",
      "     rank: 3\n",
      "     vector: 0.4655\n",
      "     bm25: 0.6965\n",
      "     reranker: -3.6777374744415283\n",
      "   Hierarchical boost: False\n",
      "   Content snippet: Amazon Simple Storage Service User Guide\n",
      "Change Description Date\n",
      "Support for MFA-\n",
      "protected API acce...\n",
      "\n",
      "4. chunk_1845\n",
      "   Scores:\n",
      "     rank: 4\n",
      "     vector: 0.4109\n",
      "     bm25: 0.5180\n",
      "     reranker: -4.652083873748779\n",
      "   Hierarchical boost: False\n",
      "   Content snippet: Amazon Simple Storage Service User Guide\n",
      "Storage overhead charges  When you transition objects to ...\n",
      "\n",
      "5. chunk_315\n",
      "   Scores:\n",
      "     rank: 5\n",
      "     vector: 0.4112\n",
      "     bm25: 0.8931\n",
      "     reranker: -4.767022609710693\n",
      "   Hierarchical boost: False\n",
      "   Content snippet: Amazon Simple Storage Service User Guide\n",
      "AWS Signature Version 4 (SigV4)\n",
      "To enforce specic behavior...\n",
      "\n",
      "\n",
      "=== HIERARCHICAL EXPLORATION STATS ===\n",
      "\n",
      "CONTENT RETRIEVER:\n",
      "  Queries processed: 1\n",
      "  Improvements: 0\n",
      "  Improvement rate: 0.00%\n",
      "\n",
      "COMBINED RETRIEVER:\n",
      "  Queries processed: 1\n",
      "  Improvements: 0\n",
      "  Improvement rate: 0.00%\n",
      "\n",
      "HYBRID RETRIEVER:\n",
      "  Queries processed: 1\n",
      "  Improvements: 0\n",
      "  Improvement rate: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Try content retriever\n",
    "content_results = retrievers['content'].retrieve(query, k=5)\n",
    "display_results('Content', content_results)\n",
    "\n",
    "# Try combined retriever\n",
    "combined_results = retrievers['combined'].retrieve(query, k=5)\n",
    "display_results('Combined', combined_results)\n",
    "\n",
    "# Try hybrid retriever\n",
    "hybrid_results = retrievers['hybrid'].retrieve(query, k=5)\n",
    "display_results('Hybrid', hybrid_results)\n",
    "\n",
    "# Compare hierarchical improvement stats\n",
    "print(\"\\n\\n=== HIERARCHICAL EXPLORATION STATS ===\")\n",
    "for name, retriever in retrievers.items():\n",
    "    stats = retriever.get_hierarchical_stats()\n",
    "    print(f\"\\n{name.upper()} RETRIEVER:\")\n",
    "    print(f\"  Queries processed: {stats['queries_processed']}\")\n",
    "    print(f\"  Improvements: {stats['hierarchical_improvements']}\")\n",
    "    print(f\"  Improvement rate: {stats['improvement_rate']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbcc999b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(retrievers, query):\n",
    "    # Run each retriever\n",
    "    results = {}\n",
    "    stats = {}\n",
    "    \n",
    "    for name, retriever in retrievers.items():\n",
    "        results[name] = retriever.retrieve(query, k=5)\n",
    "        \n",
    "        # Calculate average scores\n",
    "        vector_scores = [r['scores'].get('vector', 0) for r in results[name] if 'scores' in r]\n",
    "        bm25_scores = [r['scores'].get('bm25', 0) for r in results[name] if 'scores' in r]\n",
    "        reranker_scores = [r['scores'].get('reranker', 0) for r in results[name] if 'scores' in r]\n",
    "        \n",
    "        stats[name] = {\n",
    "            'avg_vector': sum(vector_scores) / len(vector_scores) if vector_scores else 0,\n",
    "            'avg_bm25': sum(bm25_scores) / len(bm25_scores) if bm25_scores else 0,\n",
    "            'avg_reranker': sum(reranker_scores) / len(reranker_scores) if reranker_scores else 0,\n",
    "        }\n",
    "    \n",
    "    # Print stats\n",
    "    print(\"\\n=== RETRIEVAL SCORE STATISTICS ===\")\n",
    "    for name, retriever_stats in stats.items():\n",
    "        print(f\"\\n{name.upper()} RETRIEVER:\")\n",
    "        print(f\"  Average Vector Score: {retriever_stats['avg_vector']:.4f}\")\n",
    "        print(f\"  Average BM25 Score: {retriever_stats['avg_bm25']:.4f}\")\n",
    "        print(f\"  Average Reranker Score: {retriever_stats['avg_reranker']:.4f}\")\n",
    "    \n",
    "    return results, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f88df09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, define the analyze_results function\n",
    "def analyze_results(retrievers, query):\n",
    "    # Run each retriever\n",
    "    results = {}\n",
    "    stats = {}\n",
    "    \n",
    "    for name, retriever in retrievers.items():\n",
    "        results[name] = retriever.retrieve(query, k=5)\n",
    "        \n",
    "        # Calculate average scores\n",
    "        vector_scores = [r['scores'].get('vector', 0) for r in results[name] if 'scores' in r]\n",
    "        bm25_scores = [r['scores'].get('bm25', 0) for r in results[name] if 'scores' in r]\n",
    "        reranker_scores = [r['scores'].get('reranker', 0) for r in results[name] if 'scores' in r]\n",
    "        \n",
    "        stats[name] = {\n",
    "            'avg_vector': sum(vector_scores) / len(vector_scores) if vector_scores else 0,\n",
    "            'avg_bm25': sum(bm25_scores) / len(bm25_scores) if bm25_scores else 0,\n",
    "            'avg_reranker': sum(reranker_scores) / len(reranker_scores) if reranker_scores else 0,\n",
    "        }\n",
    "    \n",
    "    # Print stats\n",
    "    print(\"\\n=== RETRIEVAL SCORE STATISTICS ===\")\n",
    "    for name, retriever_stats in stats.items():\n",
    "        print(f\"\\n{name.upper()} RETRIEVER:\")\n",
    "        print(f\"  Average Vector Score: {retriever_stats['avg_vector']:.4f}\")\n",
    "        print(f\"  Average BM25 Score: {retriever_stats['avg_bm25']:.4f}\")\n",
    "        print(f\"  Average Reranker Score: {retriever_stats['avg_reranker']:.4f}\")\n",
    "    \n",
    "    return results, stats\n",
    "\n",
    "# Then, use it in your main code\n",
    "def run_retrieval_analysis():\n",
    "    # Create all three retrievers\n",
    "    retrievers = RetrieverFactory.create_retrievers(\n",
    "        model_name='all-MiniLM-L6-v2',\n",
    "        reranker_model='cross-encoder/ms-marco-MiniLM-L-6-v2',\n",
    "        candidate_pool_size=100,\n",
    "        base_path='NEW_faiss'\n",
    "    )\n",
    "    \n",
    "    # Example queries to test\n",
    "    test_queries = [\n",
    "        \"What are the prerequisites for locking a vault in S3 Glacier, and how does this feature enforce compliance?\",\n",
    "        \"How does S3 Object Lock help meet regulatory compliance, and what are the prerequisites for enabling it?\",\n",
    "        \"What are the different checksum algorithms supported by S3, and how can they be used to ensure data integrity?\",\n",
    "        \"A developer's Java SDK script to upload an archive to S3 Glacier fails due to a `Missing Authentication Token` error. What could be the cause, and how can it be resolved?\"\n",
    "    ]\n",
    "    \n",
    "    # Analyze results for each query\n",
    "    all_stats = {}\n",
    "    for query in test_queries:\n",
    "        print(f\"\\n\\n======= QUERY: {query} =======\")\n",
    "        results, stats = analyze_results(retrievers, query)\n",
    "        \n",
    "        # Store stats for this query\n",
    "        all_stats[query] = stats\n",
    "        \n",
    "        # Display detailed results if needed\n",
    "        for name, retriever_results in results.items():\n",
    "            display_results(name, retriever_results)\n",
    "    \n",
    "    # Calculate overall average scores across all queries\n",
    "    print(\"\\n\\n====== OVERALL AVERAGE SCORES ======\")\n",
    "    overall_stats = {}\n",
    "    \n",
    "    for retriever_name in retrievers.keys():\n",
    "        vector_sum = sum(all_stats[q][retriever_name]['avg_vector'] for q in test_queries)\n",
    "        bm25_sum = sum(all_stats[q][retriever_name]['avg_bm25'] for q in test_queries)\n",
    "        reranker_sum = sum(all_stats[q][retriever_name]['avg_reranker'] for q in test_queries)\n",
    "        \n",
    "        overall_stats[retriever_name] = {\n",
    "            'avg_vector': vector_sum / len(test_queries),\n",
    "            'avg_bm25': bm25_sum / len(test_queries),\n",
    "            'avg_reranker': reranker_sum / len(test_queries)\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{retriever_name.upper()} RETRIEVER:\")\n",
    "        print(f\"  Overall Average Vector Score: {overall_stats[retriever_name]['avg_vector']:.4f}\")\n",
    "        print(f\"  Overall Average BM25 Score: {overall_stats[retriever_name]['avg_bm25']:.4f}\")\n",
    "        print(f\"  Overall Average Reranker Score: {overall_stats[retriever_name]['avg_reranker']:.4f}\")\n",
    "    \n",
    "    # Print hierarchical stats\n",
    "    print(\"\\n\\n=== HIERARCHICAL EXPLORATION STATS ===\")\n",
    "    for name, retriever in retrievers.items():\n",
    "        stats = retriever.get_hierarchical_stats()\n",
    "        print(f\"\\n{name.upper()} RETRIEVER:\")\n",
    "        print(f\"  Queries processed: {stats['queries_processed']}\")\n",
    "        print(f\"  Improvements: {stats['hierarchical_improvements']}\")\n",
    "        print(f\"  Improvement rate: {stats['improvement_rate']:.2%}\")\n",
    "\n",
    "# You can also modify the existing display_results function to include less detailed output\n",
    "def display_results(retriever_name, results):\n",
    "    print(f\"\\n{retriever_name.upper()} RETRIEVER TOP RESULTS:\")\n",
    "    for i, result in enumerate(results[:3]):  # Show only top 3 for brevity\n",
    "        print(f\"  {i+1}. {result.get('id', result.get('chunk_id', 'unknown'))}\")\n",
    "        if 'content' in result:\n",
    "            print(f\"     {result['content'][:100]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b05b9d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:35:05,819 - INFO - Use pytorch device_name: cpu\n",
      "2025-02-25 19:35:05,820 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2025-02-25 19:35:06,409 - INFO - Use pytorch device: cpu\n",
      "2025-02-25 19:35:06,870 - INFO - Use pytorch device_name: cpu\n",
      "2025-02-25 19:35:06,871 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2025-02-25 19:35:07,425 - INFO - Use pytorch device: cpu\n",
      "2025-02-25 19:35:07,901 - INFO - Use pytorch device_name: cpu\n",
      "2025-02-25 19:35:07,902 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2025-02-25 19:35:08,472 - INFO - Use pytorch device: cpu\n",
      "2025-02-25 19:35:08,947 - INFO - Loading index s3_metadata\n",
      "2025-02-25 19:35:08,969 - INFO - Loaded index s3_metadata with 6214 chunks\n",
      "2025-02-25 19:35:08,971 - INFO - Loading index s3_content\n",
      "2025-02-25 19:35:09,008 - INFO - Loaded index s3_content with 6214 chunks\n",
      "2025-02-25 19:35:09,009 - INFO - Loading index s3_combined\n",
      "2025-02-25 19:35:09,055 - INFO - Loaded index s3_combined with 6214 chunks\n",
      "2025-02-25 19:35:09,056 - INFO - Indexing 6214 chunks for BM25 scoring\n",
      "2025-02-25 19:35:09,382 - INFO - BM25 indexing complete. Average document length: 82.43\n",
      "2025-02-25 19:35:09,383 - INFO - AdvancedHybridRetriever initialized successfully\n",
      "2025-02-25 19:35:09,384 - INFO - Loading index s3_metadata\n",
      "2025-02-25 19:35:09,418 - INFO - Loaded index s3_metadata with 6214 chunks\n",
      "2025-02-25 19:35:09,419 - INFO - Loading index s3_content\n",
      "2025-02-25 19:35:09,452 - INFO - Loaded index s3_content with 6214 chunks\n",
      "2025-02-25 19:35:09,453 - INFO - Loading index s3_combined\n",
      "2025-02-25 19:35:09,859 - INFO - Loaded index s3_combined with 6214 chunks\n",
      "2025-02-25 19:35:09,860 - INFO - Indexing 6214 chunks for BM25 scoring\n",
      "2025-02-25 19:35:10,175 - INFO - BM25 indexing complete. Average document length: 82.43\n",
      "2025-02-25 19:35:10,175 - INFO - AdvancedHybridRetriever initialized successfully\n",
      "2025-02-25 19:35:10,176 - INFO - Loading index s3_metadata\n",
      "2025-02-25 19:35:10,206 - INFO - Loaded index s3_metadata with 6214 chunks\n",
      "2025-02-25 19:35:10,207 - INFO - Loading index s3_content\n",
      "2025-02-25 19:35:10,235 - INFO - Loaded index s3_content with 6214 chunks\n",
      "2025-02-25 19:35:10,236 - INFO - Loading index s3_combined\n",
      "2025-02-25 19:35:10,271 - INFO - Loaded index s3_combined with 6214 chunks\n",
      "2025-02-25 19:35:10,271 - INFO - Indexing 6214 chunks for BM25 scoring\n",
      "2025-02-25 19:35:10,568 - INFO - BM25 indexing complete. Average document length: 82.43\n",
      "2025-02-25 19:35:10,569 - INFO - AdvancedHybridRetriever initialized successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======= QUERY: What are the prerequisites for locking a vault in S3 Glacier, and how does this feature enforce compliance? =======\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "848bbd0c59b844de84c1693067ecbe8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:35:10,600 - INFO - Using content index for initial retrieval\n",
      "2025-02-25 19:35:10,683 - INFO - Reranking top 10 candidates using cross-encoder\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfaec693b03d48329bd6d4a67fc6f72c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:35:11,687 - INFO - Performing hierarchical exploration\n",
      "2025-02-25 19:35:11,688 - INFO - Found 28 related chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beadaff8e40e43e9b0f00e4ae4eb6e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:35:14,862 - INFO - Found 20 related chunks with scores above threshold\n",
      "2025-02-25 19:35:14,863 - INFO - Retrieval completed in 4.29 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c86f94111e444329ae487b2ca1f5254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:35:14,905 - INFO - Using combined index for initial retrieval\n",
      "2025-02-25 19:35:14,990 - INFO - Reranking top 10 candidates using cross-encoder\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5aae1fc4aae4cd582bfa4714134a638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:35:16,018 - INFO - Performing hierarchical exploration\n",
      "2025-02-25 19:35:16,019 - INFO - Found 28 related chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2e3bd1e7e645828e2f8ecabef871cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:35:19,191 - INFO - Found 20 related chunks with scores above threshold\n",
      "2025-02-25 19:35:19,192 - INFO - Retrieval completed in 4.32 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c449e67636be4cabb10cb28e9df5f08c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:35:19,224 - INFO - Getting initial candidates using BM25 (pool size: 100)\n",
      "2025-02-25 19:35:19,308 - INFO - Reranking 100 candidates using dense embeddings\n",
      "2025-02-25 19:35:19,321 - INFO - Reranking top 10 candidates using cross-encoder\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e8ff34756f7437488548e132fd95352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:35:20,627 - INFO - Performing hierarchical exploration\n",
      "2025-02-25 19:35:20,629 - INFO - Found 28 related chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "215d557eee99475dbb40c9efdf300c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:35:23,855 - INFO - Found 3 related chunks with scores above threshold\n",
      "2025-02-25 19:35:23,857 - INFO - Retrieval completed in 4.66 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RETRIEVAL SCORE STATISTICS ===\n",
      "\n",
      "CONTENT RETRIEVER:\n",
      "  Average Vector Score: 0.7814\n",
      "  Average BM25 Score: 0.5335\n",
      "  Average Reranker Score: -9.5260\n",
      "\n",
      "COMBINED RETRIEVER:\n",
      "  Average Vector Score: 0.7814\n",
      "  Average BM25 Score: 0.5335\n",
      "  Average Reranker Score: -9.5260\n",
      "\n",
      "HYBRID RETRIEVER:\n",
      "  Average Vector Score: 0.4424\n",
      "  Average BM25 Score: 0.7421\n",
      "  Average Reranker Score: -3.0669\n",
      "\n",
      "CONTENT RETRIEVER TOP RESULTS:\n",
      "  1. chunk_411\n",
      "     Amazon Simple Storage Service User Guide\n",
      "5. In the Bucket name eld, enter the name of the bucket th...\n",
      "  2. chunk_109\n",
      "     Amazon Simple Storage Service User Guide\n",
      "You can't use all of your available bandwidth over the int...\n",
      "  3. chunk_375\n",
      "     Amazon Simple Storage Service User Guide\n",
      "inputS3Url   A presigned URL that can be used to fetch th...\n",
      "\n",
      "COMBINED RETRIEVER TOP RESULTS:\n",
      "  1. chunk_411\n",
      "     Amazon Simple Storage Service User Guide\n",
      "5. In the Bucket name eld, enter the name of the bucket th...\n",
      "  2. chunk_109\n",
      "     Amazon Simple Storage Service User Guide\n",
      "You can't use all of your available bandwidth over the int...\n",
      "  3. chunk_375\n",
      "     Amazon Simple Storage Service User Guide\n",
      "inputS3Url   A presigned URL that can be used to fetch th...\n",
      "\n",
      "HYBRID RETRIEVER TOP RESULTS:\n",
      "  1. chunk_17\n",
      "     Amazon Simple Storage Service User Guide\n",
      "data at the lowest costs in S3 Glacier Instant Retrieval, S...\n",
      "  2. chunk_1429\n",
      "     Amazon Simple Storage Service User Guide\n",
      "that require WORM storage or simply to add an additional la...\n",
      "  3. chunk_2483\n",
      "     Amazon Simple Storage Service User Guide\n",
      "Change Description Date\n",
      "Support for MFA-\n",
      "protected API acce...\n",
      "\n",
      "\n",
      "======= QUERY: How does S3 Object Lock help meet regulatory compliance, and what are the prerequisites for enabling it? =======\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "616aac017f644422a223ea95e365ac78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:35:23,898 - INFO - Using content index for initial retrieval\n",
      "2025-02-25 19:35:23,975 - INFO - Reranking top 10 candidates using cross-encoder\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d424dfcc7d4fc4b16011a430cee214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:35:25,121 - INFO - Performing hierarchical exploration\n",
      "2025-02-25 19:35:25,122 - INFO - Found 23 related chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca0baedb4a047d3be238e55107b495a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:35:27,773 - INFO - Found 2 related chunks with scores above threshold\n",
      "2025-02-25 19:35:27,775 - INFO - Retrieval completed in 3.92 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9a1ec249d114b248e38d5e25eae00c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:35:27,799 - INFO - Using combined index for initial retrieval\n",
      "2025-02-25 19:35:27,885 - INFO - Reranking top 10 candidates using cross-encoder\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "471ff5bb036e4735a47e6452f9c74be0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:35:29,060 - INFO - Performing hierarchical exploration\n",
      "2025-02-25 19:35:29,062 - INFO - Found 23 related chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9842bcccbf0444197399edaad12aa53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:35:31,661 - INFO - Found 2 related chunks with scores above threshold\n",
      "2025-02-25 19:35:31,663 - INFO - Retrieval completed in 3.89 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da108222c0bf4725a9e107d353009952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:35:31,690 - INFO - Getting initial candidates using BM25 (pool size: 100)\n",
      "2025-02-25 19:35:31,774 - INFO - Reranking 100 candidates using dense embeddings\n",
      "2025-02-25 19:35:31,793 - INFO - Reranking top 10 candidates using cross-encoder\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbc95fa827be42ee8867a8b1f747ca08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:35:32,816 - INFO - Performing hierarchical exploration\n",
      "2025-02-25 19:35:32,818 - INFO - Found 30 related chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6715ebd1d4b7406bb87900ac4bf37060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:35:36,224 - INFO - Found 3 related chunks with scores above threshold\n",
      "2025-02-25 19:35:36,226 - INFO - Retrieval completed in 4.56 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RETRIEVAL SCORE STATISTICS ===\n",
      "\n",
      "CONTENT RETRIEVER:\n",
      "  Average Vector Score: 0.6165\n",
      "  Average BM25 Score: 0.1693\n",
      "  Average Reranker Score: 1.5524\n",
      "\n",
      "COMBINED RETRIEVER:\n",
      "  Average Vector Score: 0.6165\n",
      "  Average BM25 Score: 0.1693\n",
      "  Average Reranker Score: 1.5524\n",
      "\n",
      "HYBRID RETRIEVER:\n",
      "  Average Vector Score: 0.5787\n",
      "  Average BM25 Score: 0.4151\n",
      "  Average Reranker Score: 1.8069\n",
      "\n",
      "CONTENT RETRIEVER TOP RESULTS:\n",
      "  1. chunk_1426\n",
      "     Amazon Simple Storage Service User Guide\n",
      "Compliance validation for Amazon S3\n",
      "The security and compli...\n",
      "  2. chunk_1716\n",
      "     Amazon Simple Storage Service User Guide\n",
      "Conguring S3 Object Lock\n",
      "With Amazon S3 Object Lock, you c...\n",
      "  3. chunk_1720\n",
      "     Amazon Simple Storage Service User Guide\n",
      "Using the AWS SDKs\n",
      "For examples of how to enable Object Loc...\n",
      "\n",
      "COMBINED RETRIEVER TOP RESULTS:\n",
      "  1. chunk_1426\n",
      "     Amazon Simple Storage Service User Guide\n",
      "Compliance validation for Amazon S3\n",
      "The security and compli...\n",
      "  2. chunk_1716\n",
      "     Amazon Simple Storage Service User Guide\n",
      "Conguring S3 Object Lock\n",
      "With Amazon S3 Object Lock, you c...\n",
      "  3. chunk_1720\n",
      "     Amazon Simple Storage Service User Guide\n",
      "Using the AWS SDKs\n",
      "For examples of how to enable Object Loc...\n",
      "\n",
      "HYBRID RETRIEVER TOP RESULTS:\n",
      "  1. chunk_1426\n",
      "     Amazon Simple Storage Service User Guide\n",
      "Compliance validation for Amazon S3\n",
      "The security and compli...\n",
      "  2. chunk_533\n",
      "     Amazon Simple Storage Service User Guide\n",
      "To use S3 Batch Operations with Object Lock to add legal ho...\n",
      "  3. chunk_1719\n",
      "     Amazon Simple Storage Service User Guide\n",
      "2. In the left navigation pane, choose Buckets.\n",
      "3. In the B...\n",
      "\n",
      "\n",
      "======= QUERY: What are the different checksum algorithms supported by S3, and how can they be used to ensure data integrity? =======\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebb2d3c1e3624e69a2507fdd921a0237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:35:36,257 - INFO - Using content index for initial retrieval\n",
      "2025-02-25 19:35:36,361 - INFO - Reranking top 10 candidates using cross-encoder\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac662d99660f4218a41b1953773cc657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:35:37,769 - INFO - Performing hierarchical exploration\n",
      "2025-02-25 19:35:37,770 - INFO - Found 30 related chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9ff2f94b0a74ed6aa49c4f12f0af95a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:35:41,217 - INFO - Found 25 related chunks with scores above threshold\n",
      "2025-02-25 19:35:41,218 - INFO - Retrieval completed in 4.99 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b187932bb3a44028ab98b2607cf8691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:35:41,241 - INFO - Using combined index for initial retrieval\n",
      "2025-02-25 19:35:41,319 - INFO - Reranking top 10 candidates using cross-encoder\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79d8cacb9544be58b37f6f36a3f5faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:35:42,604 - INFO - Performing hierarchical exploration\n",
      "2025-02-25 19:35:42,606 - INFO - Found 30 related chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1e495aeafe342d38388c02af3c46696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:35:45,996 - INFO - Found 25 related chunks with scores above threshold\n",
      "2025-02-25 19:35:45,998 - INFO - Retrieval completed in 4.78 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6460332cfb954d3f8268f89ecf6898e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:35:46,023 - INFO - Getting initial candidates using BM25 (pool size: 100)\n",
      "2025-02-25 19:35:46,096 - INFO - Reranking 100 candidates using dense embeddings\n",
      "2025-02-25 19:35:46,107 - INFO - Reranking top 10 candidates using cross-encoder\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c6ee145fbb949bd8507ede2cf571a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:35:47,155 - INFO - Performing hierarchical exploration\n",
      "2025-02-25 19:35:47,156 - INFO - Found 26 related chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c054ac7997fa4314bbf44e06db78e527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:35:49,969 - INFO - Retrieval completed in 3.97 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RETRIEVAL SCORE STATISTICS ===\n",
      "\n",
      "CONTENT RETRIEVER:\n",
      "  Average Vector Score: 0.6792\n",
      "  Average BM25 Score: 0.5904\n",
      "  Average Reranker Score: 1.3683\n",
      "\n",
      "COMBINED RETRIEVER:\n",
      "  Average Vector Score: 0.6792\n",
      "  Average BM25 Score: 0.5904\n",
      "  Average Reranker Score: 1.3683\n",
      "\n",
      "HYBRID RETRIEVER:\n",
      "  Average Vector Score: 0.6638\n",
      "  Average BM25 Score: 0.7448\n",
      "  Average Reranker Score: 5.4556\n",
      "\n",
      "CONTENT RETRIEVER TOP RESULTS:\n",
      "  1. chunk_740\n",
      "     Amazon Simple Storage Service User Guide\n",
      "ETags\n",
      "Entity tags (ETags) for S3 Express One Zone are rando...\n",
      "  2. chunk_808\n",
      "     Amazon Simple Storage Service User Guide\n",
      "S3 additional checksum best practices\n",
      "S3 Express One Zone o...\n",
      "  3. chunk_259\n",
      "     Amazon Simple Storage Service User Guide\n",
      "When you're using the AWS Management Console, you select t...\n",
      "\n",
      "COMBINED RETRIEVER TOP RESULTS:\n",
      "  1. chunk_740\n",
      "     Amazon Simple Storage Service User Guide\n",
      "ETags\n",
      "Entity tags (ETags) for S3 Express One Zone are rando...\n",
      "  2. chunk_808\n",
      "     Amazon Simple Storage Service User Guide\n",
      "S3 additional checksum best practices\n",
      "S3 Express One Zone o...\n",
      "  3. chunk_259\n",
      "     Amazon Simple Storage Service User Guide\n",
      "When you're using the AWS Management Console, you select t...\n",
      "\n",
      "HYBRID RETRIEVER TOP RESULTS:\n",
      "  1. chunk_740\n",
      "     Amazon Simple Storage Service User Guide\n",
      "ETags\n",
      "Entity tags (ETags) for S3 Express One Zone are rando...\n",
      "  2. chunk_808\n",
      "     Amazon Simple Storage Service User Guide\n",
      "S3 additional checksum best practices\n",
      "S3 Express One Zone o...\n",
      "  3. chunk_258\n",
      "     Amazon Simple Storage Service User Guide\n",
      "Troubleshooting downloading objects\n",
      "Insucient permissions ...\n",
      "\n",
      "\n",
      "======= QUERY: A developer's Java SDK script to upload an archive to S3 Glacier fails due to a `Missing Authentication Token` error. What could be the cause, and how can it be resolved? =======\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "561822af55714a90bc957957559ce6ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:35:50,012 - INFO - Using content index for initial retrieval\n",
      "2025-02-25 19:35:50,104 - INFO - Reranking top 10 candidates using cross-encoder\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23f9839fca384ca5aebaa202d15faee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:35:51,236 - INFO - Performing hierarchical exploration\n",
      "2025-02-25 19:35:51,237 - INFO - Found 29 related chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ebca7b2750a44f2908cafdd08205ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:35:54,548 - INFO - Found 10 related chunks with scores above threshold\n",
      "2025-02-25 19:35:54,549 - INFO - Retrieval completed in 4.58 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "142262a2ba95476c899079dbf81d80e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:35:54,578 - INFO - Using combined index for initial retrieval\n",
      "2025-02-25 19:35:54,671 - INFO - Reranking top 10 candidates using cross-encoder\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46c1c8b0742f489a8f61c03e09f1ac90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:35:55,814 - INFO - Performing hierarchical exploration\n",
      "2025-02-25 19:35:55,815 - INFO - Found 29 related chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c30b23d9d1304d41ae2f7e27f192e8f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:35:59,177 - INFO - Found 10 related chunks with scores above threshold\n",
      "2025-02-25 19:35:59,178 - INFO - Retrieval completed in 4.63 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b6b8284c894a4896ede4c8c3720d53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:35:59,204 - INFO - Getting initial candidates using BM25 (pool size: 100)\n",
      "2025-02-25 19:35:59,300 - INFO - Reranking 100 candidates using dense embeddings\n",
      "2025-02-25 19:35:59,314 - INFO - Reranking top 10 candidates using cross-encoder\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b89cf60e464656b0310e72bcb43d69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:36:00,539 - INFO - Performing hierarchical exploration\n",
      "2025-02-25 19:36:00,540 - INFO - Found 29 related chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb81e9da7b740c7953add2da11616b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:36:03,720 - INFO - Found 10 related chunks with scores above threshold\n",
      "2025-02-25 19:36:03,721 - INFO - Retrieval completed in 4.54 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RETRIEVAL SCORE STATISTICS ===\n",
      "\n",
      "CONTENT RETRIEVER:\n",
      "  Average Vector Score: 0.6401\n",
      "  Average BM25 Score: 0.3753\n",
      "  Average Reranker Score: -5.5701\n",
      "\n",
      "COMBINED RETRIEVER:\n",
      "  Average Vector Score: 0.6401\n",
      "  Average BM25 Score: 0.3753\n",
      "  Average Reranker Score: -5.5701\n",
      "\n",
      "HYBRID RETRIEVER:\n",
      "  Average Vector Score: 0.4549\n",
      "  Average BM25 Score: 0.6920\n",
      "  Average Reranker Score: -5.1143\n",
      "\n",
      "CONTENT RETRIEVER TOP RESULTS:\n",
      "  1. chunk_185\n",
      "     Amazon Simple Storage Service User Guide\n",
      "    bucket_name, object_key, download_file_path, file_size_...\n",
      "  2. chunk_202\n",
      "     Amazon Simple Storage Service User Guide\n",
      "        { \n",
      "            s3Client = new AmazonS3Client(bucket...\n",
      "  3. chunk_272\n",
      "     Amazon Simple Storage Service User Guide\n",
      "Before making any updates to your S3 Lifecycle conguratio...\n",
      "\n",
      "COMBINED RETRIEVER TOP RESULTS:\n",
      "  1. chunk_185\n",
      "     Amazon Simple Storage Service User Guide\n",
      "    bucket_name, object_key, download_file_path, file_size_...\n",
      "  2. chunk_202\n",
      "     Amazon Simple Storage Service User Guide\n",
      "        { \n",
      "            s3Client = new AmazonS3Client(bucket...\n",
      "  3. chunk_272\n",
      "     Amazon Simple Storage Service User Guide\n",
      "Before making any updates to your S3 Lifecycle conguratio...\n",
      "\n",
      "HYBRID RETRIEVER TOP RESULTS:\n",
      "  1. chunk_29\n",
      "     Amazon Simple Storage Service User Guide\n",
      "The AWS SDKs provide a convenient way to create programmati...\n",
      "  2. chunk_185\n",
      "     Amazon Simple Storage Service User Guide\n",
      "    bucket_name, object_key, download_file_path, file_size_...\n",
      "  3. chunk_262\n",
      "     Amazon Simple Storage Service User Guide\n",
      "            downloadLargeFileBracketedByChecksum(s3Client);...\n",
      "\n",
      "\n",
      "====== OVERALL AVERAGE SCORES ======\n",
      "\n",
      "CONTENT RETRIEVER:\n",
      "  Overall Average Vector Score: 0.6793\n",
      "  Overall Average BM25 Score: 0.4171\n",
      "  Overall Average Reranker Score: -3.0439\n",
      "\n",
      "COMBINED RETRIEVER:\n",
      "  Overall Average Vector Score: 0.6793\n",
      "  Overall Average BM25 Score: 0.4171\n",
      "  Overall Average Reranker Score: -3.0439\n",
      "\n",
      "HYBRID RETRIEVER:\n",
      "  Overall Average Vector Score: 0.5350\n",
      "  Overall Average BM25 Score: 0.6485\n",
      "  Overall Average Reranker Score: -0.2297\n",
      "\n",
      "\n",
      "=== HIERARCHICAL EXPLORATION STATS ===\n",
      "\n",
      "CONTENT RETRIEVER:\n",
      "  Queries processed: 4\n",
      "  Improvements: 0\n",
      "  Improvement rate: 0.00%\n",
      "\n",
      "COMBINED RETRIEVER:\n",
      "  Queries processed: 4\n",
      "  Improvements: 0\n",
      "  Improvement rate: 0.00%\n",
      "\n",
      "HYBRID RETRIEVER:\n",
      "  Queries processed: 4\n",
      "  Improvements: 0\n",
      "  Improvement rate: 0.00%\n"
     ]
    }
   ],
   "source": [
    "run_retrieval_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c04d01d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(retrievers, query):\n",
    "    # Run each retriever\n",
    "    results = {}\n",
    "    stats = {}\n",
    "    \n",
    "    for name, retriever in retrievers.items():\n",
    "        results[name] = retriever.retrieve(query, k=5)\n",
    "        \n",
    "        # Calculate average scores\n",
    "        vector_scores = [r['scores'].get('vector', 0) for r in results[name] if 'scores' in r]\n",
    "        bm25_scores = [r['scores'].get('bm25', 0) for r in results[name] if 'scores' in r]\n",
    "        reranker_scores = [r['scores'].get('reranker', 0) for r in results[name] if 'scores' in r]\n",
    "        \n",
    "        stats[name] = {\n",
    "            'avg_vector': sum(vector_scores) / len(vector_scores) if vector_scores else 0,\n",
    "            'avg_bm25': sum(bm25_scores) / len(bm25_scores) if bm25_scores else 0,\n",
    "            'avg_reranker': sum(reranker_scores) / len(reranker_scores) if reranker_scores else 0,\n",
    "        }\n",
    "    \n",
    "    # Print stats for this query\n",
    "    print(f\"\\n=== SCORE STATISTICS FOR QUERY: {query} ===\")\n",
    "    for name, retriever_stats in stats.items():\n",
    "        print(f\"\\n{name.upper()} RETRIEVER:\")\n",
    "        print(f\"  Average Vector Score: {retriever_stats['avg_vector']:.4f}\")\n",
    "        print(f\"  Average BM25 Score: {retriever_stats['avg_bm25']:.4f}\")\n",
    "        print(f\"  Average Reranker Score: {retriever_stats['avg_reranker']:.4f}\")\n",
    "    \n",
    "    return results, stats\n",
    "\n",
    "def run_retrieval_analysis():\n",
    "    # Create all three retrievers\n",
    "    retrievers = RetrieverFactory.create_retrievers(\n",
    "        model_name='all-MiniLM-L6-v2',\n",
    "        reranker_model='cross-encoder/ms-marco-MiniLM-L-6-v2',\n",
    "        candidate_pool_size=100,\n",
    "        base_path='NEW_faiss'\n",
    "    )\n",
    "    \n",
    "    # Check that retrievers are using different indices\n",
    "    print(\"Verifying retriever configurations:\")\n",
    "    print(f\"Content retriever using index: {retrievers['content'].primary_index}\")\n",
    "    print(f\"Combined retriever using index: {retrievers['combined'].primary_index}\")\n",
    "    print(f\"Hybrid retriever using index: {retrievers['hybrid'].primary_index}\")\n",
    "    \n",
    "    # Example queries to test\n",
    "    # Example queries to test\n",
    "    test_queries = [\n",
    "        \"What are the prerequisites for locking a vault in S3 Glacier, and how does this feature enforce compliance?\",\n",
    "        \"How does S3 Object Lock help meet regulatory compliance, and what are the prerequisites for enabling it?\",\n",
    "        \"What are the different checksum algorithms supported by S3, and how can they be used to ensure data integrity?\",\n",
    "        \"A developer's Java SDK script to upload an archive to S3 Glacier fails due to a `Missing Authentication Token` error. What could be the cause, and how can it be resolved?\"\n",
    "    ]\n",
    "    \n",
    "    # Analyze results for each query\n",
    "    all_stats = {}\n",
    "    for query in test_queries:\n",
    "        print(f\"\\n\\n======= QUERY: {query} =======\")\n",
    "        results, stats = analyze_results(retrievers, query)\n",
    "        \n",
    "        # Store stats for this query\n",
    "        all_stats[query] = stats\n",
    "        \n",
    "        # Print chunk IDs to quickly check if results are different\n",
    "        for name, retriever_results in results.items():\n",
    "            chunk_ids = [r.get('id', r.get('chunk_id', 'unknown')) for r in retriever_results[:3]]\n",
    "            print(f\"\\n{name.upper()} TOP 3 CHUNKS: {', '.join(chunk_ids)}\")\n",
    "    \n",
    "    # Calculate overall average scores across all queries\n",
    "    print(\"\\n\\n====== OVERALL AVERAGE SCORES ======\")\n",
    "    overall_stats = {}\n",
    "    \n",
    "    for retriever_name in retrievers.keys():\n",
    "        vector_sum = sum(all_stats[q][retriever_name]['avg_vector'] for q in test_queries)\n",
    "        bm25_sum = sum(all_stats[q][retriever_name]['avg_bm25'] for q in test_queries)\n",
    "        reranker_sum = sum(all_stats[q][retriever_name]['avg_reranker'] for q in test_queries)\n",
    "        \n",
    "        overall_stats[retriever_name] = {\n",
    "            'avg_vector': vector_sum / len(test_queries),\n",
    "            'avg_bm25': bm25_sum / len(test_queries),\n",
    "            'avg_reranker': reranker_sum / len(test_queries)\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{retriever_name.upper()} RETRIEVER:\")\n",
    "        print(f\"  Overall Average Vector Score: {overall_stats[retriever_name]['avg_vector']:.4f}\")\n",
    "        print(f\"  Overall Average BM25 Score: {overall_stats[retriever_name]['avg_bm25']:.4f}\")\n",
    "        print(f\"  Overall Average Reranker Score: {overall_stats[retriever_name]['avg_reranker']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd0307f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:34:28,595 - INFO - Use pytorch device_name: cpu\n",
      "2025-02-25 21:34:28,597 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2025-02-25 21:34:29,851 - INFO - Use pytorch device: cpu\n",
      "2025-02-25 21:34:31,330 - INFO - Use pytorch device_name: cpu\n",
      "2025-02-25 21:34:31,330 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2025-02-25 21:34:32,356 - INFO - Use pytorch device: cpu\n",
      "2025-02-25 21:34:33,620 - INFO - Use pytorch device_name: cpu\n",
      "2025-02-25 21:34:33,622 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2025-02-25 21:34:34,597 - INFO - Use pytorch device: cpu\n",
      "2025-02-25 21:34:35,910 - INFO - Loading index s3_metadata\n",
      "2025-02-25 21:34:35,959 - INFO - Loaded index s3_metadata with 6214 chunks\n",
      "2025-02-25 21:34:35,960 - INFO - Loading index s3_content\n",
      "2025-02-25 21:34:36,045 - INFO - Loaded index s3_content with 6214 chunks\n",
      "2025-02-25 21:34:36,046 - INFO - Loading index s3_combined\n",
      "2025-02-25 21:34:36,137 - INFO - Loaded index s3_combined with 6214 chunks\n",
      "2025-02-25 21:34:36,137 - INFO - Initializing content retriever with primary index: s3_content\n",
      "2025-02-25 21:34:36,141 - INFO - Indexing 6214 chunks for BM25 scoring\n",
      "2025-02-25 21:34:37,044 - INFO - BM25 indexing complete. Average document length: 80.52\n",
      "2025-02-25 21:34:37,047 - INFO - content retriever initialized successfully\n",
      "2025-02-25 21:34:37,047 - INFO - Loading index s3_metadata\n",
      "2025-02-25 21:34:37,112 - INFO - Loaded index s3_metadata with 6214 chunks\n",
      "2025-02-25 21:34:37,112 - INFO - Loading index s3_content\n",
      "2025-02-25 21:34:38,130 - INFO - Loaded index s3_content with 6214 chunks\n",
      "2025-02-25 21:34:38,130 - INFO - Loading index s3_combined\n",
      "2025-02-25 21:34:38,264 - INFO - Loaded index s3_combined with 6214 chunks\n",
      "2025-02-25 21:34:38,264 - INFO - Initializing combined retriever with primary index: s3_combined\n",
      "2025-02-25 21:34:38,271 - INFO - Indexing 6214 chunks for BM25 scoring\n",
      "2025-02-25 21:34:39,121 - INFO - BM25 indexing complete. Average document length: 82.43\n",
      "2025-02-25 21:34:39,122 - INFO - combined retriever initialized successfully\n",
      "2025-02-25 21:34:39,124 - INFO - Loading index s3_metadata\n",
      "2025-02-25 21:34:39,196 - INFO - Loaded index s3_metadata with 6214 chunks\n",
      "2025-02-25 21:34:39,201 - INFO - Loading index s3_content\n",
      "2025-02-25 21:34:39,279 - INFO - Loaded index s3_content with 6214 chunks\n",
      "2025-02-25 21:34:39,279 - INFO - Loading index s3_combined\n",
      "2025-02-25 21:34:39,367 - INFO - Loaded index s3_combined with 6214 chunks\n",
      "2025-02-25 21:34:39,374 - INFO - Initializing hybrid retriever with primary index: s3_combined\n",
      "2025-02-25 21:34:39,375 - INFO - Indexing 6214 chunks for BM25 scoring\n",
      "2025-02-25 21:34:40,232 - INFO - BM25 indexing complete. Average document length: 82.43\n",
      "2025-02-25 21:34:40,232 - INFO - hybrid retriever initialized successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying retriever configurations:\n",
      "Content retriever using index: s3_content\n",
      "Combined retriever using index: s3_combined\n",
      "Hybrid retriever using index: s3_combined\n",
      "\n",
      "\n",
      "======= QUERY: What are the prerequisites for locking a vault in S3 Glacier, and how does this feature enforce compliance? =======\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b51288b0b324a25a208ceb32fe4522f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:34:40,343 - INFO - Using content index for initial retrieval\n",
      "2025-02-25 21:34:40,589 - INFO - Reranking top 10 candidates using cross-encoder\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddabfb20b482409aac74a2c9fe94e9fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:34:43,587 - INFO - Performing hierarchical exploration\n",
      "2025-02-25 21:34:43,587 - INFO - Found 28 related chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ffeeb2237d47e99172e389ce723ab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:34:52,174 - INFO - Found 20 related chunks with scores above threshold\n",
      "2025-02-25 21:34:52,176 - INFO - Retrieval completed in 11.94 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8ea4c89c50c4a8eae4566f16fea8288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:34:52,263 - INFO - Using combined index for initial retrieval\n",
      "2025-02-25 21:34:52,503 - INFO - Reranking top 10 candidates using cross-encoder\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef5f309115be450b958b682d91c36d48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:34:55,367 - INFO - Performing hierarchical exploration\n",
      "2025-02-25 21:34:55,370 - INFO - Found 28 related chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e9295e1ead4a969aa43da5a1fa156b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:35:03,869 - INFO - Found 20 related chunks with scores above threshold\n",
      "2025-02-25 21:35:03,873 - INFO - Retrieval completed in 11.69 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1bc0bee97da4824a4caea1914ff446c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:35:03,969 - INFO - Getting initial candidates using BM25 (pool size: 100)\n",
      "2025-02-25 21:35:04,177 - INFO - Reranking 100 candidates using dense embeddings\n",
      "2025-02-25 21:35:04,203 - INFO - Reranking top 10 candidates using cross-encoder\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ee2017158848daab9c0fdf7f828f62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:35:07,580 - INFO - Performing hierarchical exploration\n",
      "2025-02-25 21:35:07,585 - INFO - Found 28 related chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "314b294fa8f742fcbfa035cf4be764ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:35:17,195 - INFO - Found 3 related chunks with scores above threshold\n",
      "2025-02-25 21:35:17,199 - INFO - Retrieval completed in 13.32 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SCORE STATISTICS FOR QUERY: What are the prerequisites for locking a vault in S3 Glacier, and how does this feature enforce compliance? ===\n",
      "\n",
      "CONTENT RETRIEVER:\n",
      "  Average Vector Score: 0.7814\n",
      "  Average BM25 Score: 0.5665\n",
      "  Average Reranker Score: -9.5260\n",
      "\n",
      "COMBINED RETRIEVER:\n",
      "  Average Vector Score: 0.7814\n",
      "  Average BM25 Score: 0.5335\n",
      "  Average Reranker Score: -9.5260\n",
      "\n",
      "HYBRID RETRIEVER:\n",
      "  Average Vector Score: 0.4424\n",
      "  Average BM25 Score: 0.7421\n",
      "  Average Reranker Score: -3.0669\n",
      "\n",
      "CONTENT TOP 3 CHUNKS: chunk_411, chunk_109, chunk_375\n",
      "\n",
      "COMBINED TOP 3 CHUNKS: chunk_411, chunk_109, chunk_375\n",
      "\n",
      "HYBRID TOP 3 CHUNKS: chunk_17, chunk_1429, chunk_2483\n",
      "\n",
      "\n",
      "======= QUERY: How does S3 Object Lock help meet regulatory compliance, and what are the prerequisites for enabling it? =======\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4099b5cb15344bea7df39270f06daec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:35:17,271 - INFO - Using content index for initial retrieval\n",
      "2025-02-25 21:35:17,495 - INFO - Reranking top 10 candidates using cross-encoder\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa356bece3ad418685e5313e10360199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:35:20,477 - INFO - Performing hierarchical exploration\n",
      "2025-02-25 21:35:20,481 - INFO - Found 23 related chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6224745aafdb47c4ba97965c4e0106ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:35:27,509 - INFO - Found 2 related chunks with scores above threshold\n",
      "2025-02-25 21:35:27,516 - INFO - Retrieval completed in 10.31 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75cc47d015864b8cbc290b9fc8cc05de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:35:27,585 - INFO - Using combined index for initial retrieval\n",
      "2025-02-25 21:35:27,827 - INFO - Reranking top 10 candidates using cross-encoder\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b92e8f4f2c6541aaa8c47ea6f0d1c788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:35:30,762 - INFO - Performing hierarchical exploration\n",
      "2025-02-25 21:35:30,767 - INFO - Found 23 related chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79487bbb375c42bf90a4e21059660bfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:35:37,784 - INFO - Found 2 related chunks with scores above threshold\n",
      "2025-02-25 21:35:37,787 - INFO - Retrieval completed in 10.27 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60ef8ca631b4407e9cf74213e0987718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:35:37,858 - INFO - Getting initial candidates using BM25 (pool size: 100)\n",
      "2025-02-25 21:35:38,083 - INFO - Reranking 100 candidates using dense embeddings\n",
      "2025-02-25 21:35:38,115 - INFO - Reranking top 10 candidates using cross-encoder\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f74c7c5da2494112a1d8f2d3e32eeb5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:35:40,764 - INFO - Performing hierarchical exploration\n",
      "2025-02-25 21:35:40,768 - INFO - Found 30 related chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c43d4c888904748967ebbe71666a6c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:35:50,281 - INFO - Found 3 related chunks with scores above threshold\n",
      "2025-02-25 21:35:50,281 - INFO - Retrieval completed in 12.49 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SCORE STATISTICS FOR QUERY: How does S3 Object Lock help meet regulatory compliance, and what are the prerequisites for enabling it? ===\n",
      "\n",
      "CONTENT RETRIEVER:\n",
      "  Average Vector Score: 0.6165\n",
      "  Average BM25 Score: 0.1502\n",
      "  Average Reranker Score: 1.5524\n",
      "\n",
      "COMBINED RETRIEVER:\n",
      "  Average Vector Score: 0.6165\n",
      "  Average BM25 Score: 0.1693\n",
      "  Average Reranker Score: 1.5524\n",
      "\n",
      "HYBRID RETRIEVER:\n",
      "  Average Vector Score: 0.5787\n",
      "  Average BM25 Score: 0.4151\n",
      "  Average Reranker Score: 1.8069\n",
      "\n",
      "CONTENT TOP 3 CHUNKS: chunk_1426, chunk_1716, chunk_1720\n",
      "\n",
      "COMBINED TOP 3 CHUNKS: chunk_1426, chunk_1716, chunk_1720\n",
      "\n",
      "HYBRID TOP 3 CHUNKS: chunk_1426, chunk_533, chunk_1719\n",
      "\n",
      "\n",
      "======= QUERY: What are the different checksum algorithms supported by S3, and how can they be used to ensure data integrity? =======\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b29b26666864e4ba641e9cd5620ff16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:35:50,352 - INFO - Using content index for initial retrieval\n",
      "2025-02-25 21:35:50,576 - INFO - Reranking top 10 candidates using cross-encoder\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4456e06ea024449bc546ddbac3baffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:35:54,032 - INFO - Performing hierarchical exploration\n",
      "2025-02-25 21:35:54,035 - INFO - Found 30 related chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91dfca4d489f489180c68eb0a32e6563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:36:02,745 - INFO - Found 25 related chunks with scores above threshold\n",
      "2025-02-25 21:36:02,747 - INFO - Retrieval completed in 12.46 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52fca08a54347c48b10171ba67f9482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:36:02,782 - INFO - Using combined index for initial retrieval\n",
      "2025-02-25 21:36:02,897 - INFO - Reranking top 10 candidates using cross-encoder\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb53572b90714741a41d44ea2c85d6e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:36:04,240 - INFO - Performing hierarchical exploration\n",
      "2025-02-25 21:36:04,241 - INFO - Found 30 related chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3929899313c14424a3b6b8ddc513a942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:36:07,811 - INFO - Found 25 related chunks with scores above threshold\n",
      "2025-02-25 21:36:07,812 - INFO - Retrieval completed in 5.06 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18dce0a7c46c427ab0f6f3d2438c5c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:36:07,837 - INFO - Getting initial candidates using BM25 (pool size: 100)\n",
      "2025-02-25 21:36:07,928 - INFO - Reranking 100 candidates using dense embeddings\n",
      "2025-02-25 21:36:07,944 - INFO - Reranking top 10 candidates using cross-encoder\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3568e8b86374e17b86263a2ce466ea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:36:09,001 - INFO - Performing hierarchical exploration\n",
      "2025-02-25 21:36:09,002 - INFO - Found 26 related chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "033f3483281a46968a4642d3fb806811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:36:11,863 - INFO - Retrieval completed in 4.05 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SCORE STATISTICS FOR QUERY: What are the different checksum algorithms supported by S3, and how can they be used to ensure data integrity? ===\n",
      "\n",
      "CONTENT RETRIEVER:\n",
      "  Average Vector Score: 0.6792\n",
      "  Average BM25 Score: 0.5896\n",
      "  Average Reranker Score: 1.3683\n",
      "\n",
      "COMBINED RETRIEVER:\n",
      "  Average Vector Score: 0.6792\n",
      "  Average BM25 Score: 0.5904\n",
      "  Average Reranker Score: 1.3683\n",
      "\n",
      "HYBRID RETRIEVER:\n",
      "  Average Vector Score: 0.6638\n",
      "  Average BM25 Score: 0.7448\n",
      "  Average Reranker Score: 5.4556\n",
      "\n",
      "CONTENT TOP 3 CHUNKS: chunk_740, chunk_808, chunk_259\n",
      "\n",
      "COMBINED TOP 3 CHUNKS: chunk_740, chunk_808, chunk_259\n",
      "\n",
      "HYBRID TOP 3 CHUNKS: chunk_740, chunk_808, chunk_258\n",
      "\n",
      "\n",
      "======= QUERY: A developer's Java SDK script to upload an archive to S3 Glacier fails due to a `Missing Authentication Token` error. What could be the cause, and how can it be resolved? =======\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3630a3876d94451865eb731121b6f0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:36:11,892 - INFO - Using content index for initial retrieval\n",
      "2025-02-25 21:36:11,980 - INFO - Reranking top 10 candidates using cross-encoder\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cdc3dd315f14f318cf08d666107d0f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:36:13,082 - INFO - Performing hierarchical exploration\n",
      "2025-02-25 21:36:13,083 - INFO - Found 29 related chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97d1f51a62c6425498d01462ac20863a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:36:16,357 - INFO - Found 10 related chunks with scores above threshold\n",
      "2025-02-25 21:36:16,358 - INFO - Retrieval completed in 4.49 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b1b3d918c51419d89e8f9c8310627c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:36:16,384 - INFO - Using combined index for initial retrieval\n",
      "2025-02-25 21:36:16,472 - INFO - Reranking top 10 candidates using cross-encoder\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8c3e9fcae7645d3ab07cbcdc3919b1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:36:17,608 - INFO - Performing hierarchical exploration\n",
      "2025-02-25 21:36:17,609 - INFO - Found 29 related chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f62f6bf178514fd5a4722ab23eeb38c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:36:20,897 - INFO - Found 10 related chunks with scores above threshold\n",
      "2025-02-25 21:36:20,898 - INFO - Retrieval completed in 4.54 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e169c2585f43749282df27c92e98b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:36:20,926 - INFO - Getting initial candidates using BM25 (pool size: 100)\n",
      "2025-02-25 21:36:21,011 - INFO - Reranking 100 candidates using dense embeddings\n",
      "2025-02-25 21:36:21,023 - INFO - Reranking top 10 candidates using cross-encoder\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db0ff652923549a495029e15eb7b1273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:36:22,104 - INFO - Performing hierarchical exploration\n",
      "2025-02-25 21:36:22,106 - INFO - Found 29 related chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "330ee071cf714d2694838750713a1221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:36:25,239 - INFO - Found 10 related chunks with scores above threshold\n",
      "2025-02-25 21:36:25,240 - INFO - Retrieval completed in 4.34 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SCORE STATISTICS FOR QUERY: A developer's Java SDK script to upload an archive to S3 Glacier fails due to a `Missing Authentication Token` error. What could be the cause, and how can it be resolved? ===\n",
      "\n",
      "CONTENT RETRIEVER:\n",
      "  Average Vector Score: 0.6401\n",
      "  Average BM25 Score: 0.2432\n",
      "  Average Reranker Score: -5.5701\n",
      "\n",
      "COMBINED RETRIEVER:\n",
      "  Average Vector Score: 0.6401\n",
      "  Average BM25 Score: 0.3753\n",
      "  Average Reranker Score: -5.5701\n",
      "\n",
      "HYBRID RETRIEVER:\n",
      "  Average Vector Score: 0.4549\n",
      "  Average BM25 Score: 0.6920\n",
      "  Average Reranker Score: -5.1143\n",
      "\n",
      "CONTENT TOP 3 CHUNKS: chunk_185, chunk_202, chunk_272\n",
      "\n",
      "COMBINED TOP 3 CHUNKS: chunk_185, chunk_202, chunk_272\n",
      "\n",
      "HYBRID TOP 3 CHUNKS: chunk_29, chunk_185, chunk_262\n",
      "\n",
      "\n",
      "====== OVERALL AVERAGE SCORES ======\n",
      "\n",
      "CONTENT RETRIEVER:\n",
      "  Overall Average Vector Score: 0.6793\n",
      "  Overall Average BM25 Score: 0.3874\n",
      "  Overall Average Reranker Score: -3.0439\n",
      "\n",
      "COMBINED RETRIEVER:\n",
      "  Overall Average Vector Score: 0.6793\n",
      "  Overall Average BM25 Score: 0.4171\n",
      "  Overall Average Reranker Score: -3.0439\n",
      "\n",
      "HYBRID RETRIEVER:\n",
      "  Overall Average Vector Score: 0.5350\n",
      "  Overall Average BM25 Score: 0.6485\n",
      "  Overall Average Reranker Score: -0.2297\n"
     ]
    }
   ],
   "source": [
    "run_retrieval_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3953e0f7",
   "metadata": {},
   "source": [
    "# New approach March 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5774757",
   "metadata": {},
   "source": [
    "## Content - Combined - same +elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7988b3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Dict, Any, Tuple, Set\n",
    "import logging\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import spacy\n",
    "import faiss\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "from abc import ABC, abstractmethod\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class BaseRetriever(ABC):\n",
    "    \"\"\"Base abstract class for all retrievers\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_manager=None):\n",
    "        self.manager = embedding_manager\n",
    "        self.name = \"base\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def retrieve(self, query: str, k: int = 3) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant chunks for a given query\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            k: Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List of chunk results with relevance scores\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "# Mock Elasticsearch functionality since we don't have a live ES instance\n",
    "class MockElasticsearch:\n",
    "    \"\"\"Mock Elasticsearch for testing\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.documents = {}\n",
    "        self.index_name = None\n",
    "    \n",
    "    def indices(self):\n",
    "        return self\n",
    "    \n",
    "    def exists(self, index):\n",
    "        return index in self.documents\n",
    "    \n",
    "    def create(self, index, body=None):\n",
    "        self.documents[index] = []\n",
    "        return {\"acknowledged\": True}\n",
    "    \n",
    "    def bulk(self, body, refresh=False):\n",
    "        # Process bulk operations\n",
    "        current_index = self.index_name\n",
    "        for i in range(0, len(body), 2):\n",
    "            if i+1 < len(body):\n",
    "                action = body[i]\n",
    "                doc = body[i+1]\n",
    "                if 'index' in action:\n",
    "                    self.documents.setdefault(action['index']['_index'], []).append(doc)\n",
    "        return {\"errors\": False, \"took\": 10}\n",
    "    \n",
    "    def search(self, index, body):\n",
    "        # Simple BM25-like scoring using term frequency\n",
    "        results = []\n",
    "        query_terms = body[\"query\"][\"multi_match\"][\"query\"].lower().split()\n",
    "        \n",
    "        # Get documents for the specified index\n",
    "        docs = self.documents.get(index, [])\n",
    "        \n",
    "        for doc in docs:\n",
    "            # Simple scoring based on term frequency\n",
    "            score = 0\n",
    "            content = doc.get('content', '').lower()\n",
    "            topics = ' '.join(doc.get('topics', [])).lower()\n",
    "            summary = doc.get('summary', '').lower()\n",
    "            \n",
    "            # Score based on term frequency in content\n",
    "            for term in query_terms:\n",
    "                score += content.count(term) * 0.8\n",
    "                score += topics.count(term) * 0.5\n",
    "                score += summary.count(term) * 0.6\n",
    "            \n",
    "            if score > 0:\n",
    "                results.append({\n",
    "                    '_source': doc,\n",
    "                    '_score': score,\n",
    "                    'highlight': {'content': [f\"<em>{content[:100]}</em>\"]}\n",
    "                })\n",
    "        \n",
    "        # Sort by score\n",
    "        results.sort(key=lambda x: x['_score'], reverse=True)\n",
    "        \n",
    "        # Limit to size from request\n",
    "        size = body.get('size', 10)\n",
    "        results = results[:size]\n",
    "        \n",
    "        return {\n",
    "            'hits': {\n",
    "                'total': {'value': len(results)},\n",
    "                'hits': results\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "class ElasticSearchManager:\n",
    "    \"\"\"Manager for Elasticsearch integration\"\"\"\n",
    "    \n",
    "    def __init__(self, index_name: str = \"s3_documents\"):\n",
    "        \"\"\"\n",
    "        Initialize Elasticsearch manager\n",
    "        \n",
    "        Args:\n",
    "            index_name: Name of the Elasticsearch index\n",
    "        \"\"\"\n",
    "        self.index_name = index_name\n",
    "        self.es = MockElasticsearch()  # Using mock instead of real Elasticsearch\n",
    "        \n",
    "    def create_index(self, mapping: Dict = None):\n",
    "        \"\"\"\n",
    "        Create Elasticsearch index with specified mapping\n",
    "        \n",
    "        Args:\n",
    "            mapping: Index mapping for Elasticsearch\n",
    "        \"\"\"\n",
    "        # Default mapping if none provided\n",
    "        if mapping is None:\n",
    "            mapping = {\n",
    "                \"mappings\": {\n",
    "                    \"properties\": {\n",
    "                        \"chunk_id\": {\"type\": \"keyword\"},\n",
    "                        \"content\": {\"type\": \"text\", \"analyzer\": \"english\"},\n",
    "                        \"topics\": {\"type\": \"text\", \"analyzer\": \"english\"},\n",
    "                        \"summary\": {\"type\": \"text\", \"analyzer\": \"english\"},\n",
    "                        \"parent\": {\"type\": \"keyword\"},\n",
    "                        \"children\": {\"type\": \"keyword\"}\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        # Create index if it doesn't exist\n",
    "        if not self.es.indices().exists(index=self.index_name):\n",
    "            self.es.indices().create(index=self.index_name, body=mapping)\n",
    "            logging.info(f\"Created Elasticsearch index: {self.index_name}\")\n",
    "        else:\n",
    "            logging.info(f\"Elasticsearch index {self.index_name} already exists\")\n",
    "            \n",
    "    def index_chunks(self, chunks: List[Dict[str, Any]]):\n",
    "        \"\"\"\n",
    "        Index chunks in Elasticsearch\n",
    "        \n",
    "        Args:\n",
    "            chunks: List of chunks to index\n",
    "        \"\"\"\n",
    "        logging.info(f\"Indexing {len(chunks)} chunks in Elasticsearch\")\n",
    "        \n",
    "        # Ensure index exists\n",
    "        if not self.es.indices().exists(index=self.index_name):\n",
    "            self.create_index()\n",
    "        \n",
    "        # Index chunks in bulk\n",
    "        bulk_data = []\n",
    "        for chunk in chunks:\n",
    "            # Create Elasticsearch document ID from chunk ID\n",
    "            chunk_id = chunk.get('id', chunk.get('chunk_id'))\n",
    "            if not chunk_id:\n",
    "                continue\n",
    "                \n",
    "            # Prepare the index action\n",
    "            index_action = {\n",
    "                \"_index\": self.index_name,\n",
    "                \"_id\": chunk_id,\n",
    "                \"_source\": {\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"content\": chunk.get('content', ''),\n",
    "                    \"topics\": chunk.get('topics', []),\n",
    "                    \"summary\": chunk.get('summary', ''),\n",
    "                    \"parent\": chunk.get('parent', []),\n",
    "                    \"children\": chunk.get('children', [])\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            bulk_data.append({\"index\": {\"_index\": self.index_name, \"_id\": chunk_id}})\n",
    "            bulk_data.append(index_action[\"_source\"])\n",
    "            \n",
    "            # Process in batches of 500\n",
    "            if len(bulk_data) >= 1000:\n",
    "                self.es.bulk(body=bulk_data, refresh=True)\n",
    "                bulk_data = []\n",
    "                \n",
    "        # Index any remaining chunks\n",
    "        if bulk_data:\n",
    "            self.es.bulk(body=bulk_data, refresh=True)\n",
    "            \n",
    "        logging.info(f\"Finished indexing {len(chunks)} chunks in Elasticsearch\")\n",
    "        \n",
    "    def search(self, query: str, k: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Search Elasticsearch index\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            k: Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List of search results\n",
    "        \"\"\"\n",
    "        # Define search query\n",
    "        body = {\n",
    "            \"size\": k,\n",
    "            \"query\": {\n",
    "                \"multi_match\": {\n",
    "                    \"query\": query,\n",
    "                    \"fields\": [\"content^1.0\", \"topics^0.5\", \"summary^0.7\"],\n",
    "                    \"type\": \"best_fields\",\n",
    "                    \"fuzziness\": \"AUTO\"\n",
    "                }\n",
    "            },\n",
    "            \"highlight\": {\n",
    "                \"fields\": {\n",
    "                    \"content\": {},\n",
    "                    \"topics\": {},\n",
    "                    \"summary\": {}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Execute search\n",
    "        try:\n",
    "            response = self.es.search(index=self.index_name, body=body)\n",
    "            \n",
    "            # Process results\n",
    "            results = []\n",
    "            for hit in response['hits']['hits']:\n",
    "                result = hit['_source'].copy()\n",
    "                result['score'] = hit['_score']\n",
    "                if 'highlight' in hit:\n",
    "                    result['highlights'] = hit['highlight']\n",
    "                results.append(result)\n",
    "                \n",
    "            return results\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Elasticsearch search error: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "\n",
    "class BM25Scorer:\n",
    "    \"\"\"BM25 scoring implementation for keyword-based relevance\"\"\"\n",
    "    \n",
    "    def __init__(self, k1=1.5, b=0.75):\n",
    "        \"\"\"\n",
    "        Initialize BM25 scorer\n",
    "        \n",
    "        Args:\n",
    "            k1: Term frequency saturation parameter\n",
    "            b: Length normalization parameter\n",
    "        \"\"\"\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.doc_freqs = defaultdict(int)  # Document frequencies\n",
    "        self.doc_lengths = []  # Length of each document\n",
    "        self.avg_doc_length = 0\n",
    "        self.total_docs = 0\n",
    "        self.chunk_keywords = []  # Processed keywords for each chunk\n",
    "        self.chunk_ids = []  # Original chunk IDs\n",
    "        self.id_to_idx = {}  # Mapping from chunk_id to index\n",
    "        \n",
    "    def extract_keywords(self, chunk: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract keywords from a chunk\n",
    "        \n",
    "        Args:\n",
    "            chunk: The chunk to process\n",
    "            \n",
    "        Returns:\n",
    "            List of keywords\n",
    "        \"\"\"\n",
    "        keywords = []\n",
    "        \n",
    "        # Add topics if available\n",
    "        if 'topics' in chunk and chunk['topics']:\n",
    "            keywords.extend(chunk['topics'])\n",
    "            \n",
    "        # Add entity texts if available\n",
    "        if 'entities' in chunk and chunk['entities']:\n",
    "            keywords.extend([entity['text'].lower() for entity in chunk['entities']])\n",
    "            \n",
    "        # Add important words from content\n",
    "        if 'content' in chunk and chunk['content']:\n",
    "            # Simple content-based keywords\n",
    "            content_words = chunk['content'].lower().split()\n",
    "            # Filter out short words and common stopwords\n",
    "            content_words = [w for w in content_words if len(w) > 3]\n",
    "            keywords.extend(content_words)\n",
    "            \n",
    "        return list(set(keywords))  # Remove duplicates\n",
    "        \n",
    "    def index_chunks(self, chunks: List[Dict[str, Any]]):\n",
    "        \"\"\"\n",
    "        Index chunks for BM25 scoring\n",
    "        \n",
    "        Args:\n",
    "            chunks: List of chunks to index\n",
    "        \"\"\"\n",
    "        logging.info(f\"Indexing {len(chunks)} chunks for BM25 scoring\")\n",
    "        self.total_docs = len(chunks)\n",
    "        self.chunk_ids = [chunk.get('id', chunk.get('chunk_id', f'unknown_{i}')) for i, chunk in enumerate(chunks)]\n",
    "        \n",
    "        # Create mapping from chunk_id to index\n",
    "        self.id_to_idx = {chunk_id: i for i, chunk_id in enumerate(self.chunk_ids)}\n",
    "        \n",
    "        # Process each chunk\n",
    "        for chunk in chunks:\n",
    "            # Extract keywords\n",
    "            keywords = self.extract_keywords(chunk)\n",
    "            self.chunk_keywords.append(keywords)\n",
    "            \n",
    "            # Update document frequencies\n",
    "            for keyword in set(keywords):  # Use set to count each term once per document\n",
    "                self.doc_freqs[keyword] += 1\n",
    "            \n",
    "            # Update document lengths\n",
    "            self.doc_lengths.append(len(keywords))\n",
    "            \n",
    "        # Calculate average document length\n",
    "        self.avg_doc_length = sum(self.doc_lengths) / max(1, self.total_docs)\n",
    "        logging.info(f\"BM25 indexing complete. Average document length: {self.avg_doc_length:.2f}\")\n",
    "        \n",
    "    def score(self, query_keywords: List[str], doc_idx: int) -> float:\n",
    "        \"\"\"\n",
    "        Calculate BM25 score for a document\n",
    "        \n",
    "        Args:\n",
    "            query_keywords: List of keywords from the query\n",
    "            doc_idx: Index of the document to score\n",
    "            \n",
    "        Returns:\n",
    "            BM25 score\n",
    "        \"\"\"\n",
    "        score = 0\n",
    "        doc_length = self.doc_lengths[doc_idx]\n",
    "        doc_keywords = self.chunk_keywords[doc_idx]\n",
    "        \n",
    "        # Count query terms in document\n",
    "        doc_term_counts = Counter(doc_keywords)\n",
    "        \n",
    "        for query_term in query_keywords:\n",
    "            if query_term not in doc_term_counts:\n",
    "                continue\n",
    "                \n",
    "            # Calculate IDF\n",
    "            df = self.doc_freqs.get(query_term, 0)\n",
    "            if df == 0:\n",
    "                continue\n",
    "            idf = math.log((self.total_docs - df + 0.5) / (df + 0.5) + 1)\n",
    "            \n",
    "            # Calculate TF with saturation and length normalization\n",
    "            tf = doc_term_counts[query_term]\n",
    "            tf_normalized = ((tf * (self.k1 + 1)) / \n",
    "                           (tf + self.k1 * (1 - self.b + self.b * doc_length / self.avg_doc_length)))\n",
    "            \n",
    "            score += idf * tf_normalized\n",
    "            \n",
    "        return score\n",
    "    \n",
    "    def get_scores(self, query: str) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Get BM25 scores for all documents\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            \n",
    "        Returns:\n",
    "            List of (chunk_id, score) tuples\n",
    "        \"\"\"\n",
    "        # Process query keywords\n",
    "        query_keywords = query.lower().split()\n",
    "        \n",
    "        # Calculate scores for all documents\n",
    "        scores = []\n",
    "        for i in range(len(self.chunk_keywords)):\n",
    "            score = self.score(query_keywords, i)\n",
    "            scores.append((self.chunk_ids[i], score))\n",
    "        \n",
    "        # Normalize scores\n",
    "        max_score = max([score for _, score in scores]) if scores else 1\n",
    "        if max_score > 0:\n",
    "            normalized_scores = [(chunk_id, score/max_score) for chunk_id, score in scores]\n",
    "        else:\n",
    "            normalized_scores = scores\n",
    "            \n",
    "        return normalized_scores\n",
    "    \n",
    "    def get_top_k(self, query: str, k: int = 5) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Get top-k chunks by BM25 score\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            k: Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List of (chunk_id, score) tuples for top-k results\n",
    "        \"\"\"\n",
    "        scores = self.get_scores(query)\n",
    "        # Sort by score (descending)\n",
    "        sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "        return sorted_scores[:k]\n",
    "\n",
    "\n",
    "class ContentRetriever(BaseRetriever):\n",
    "    \"\"\"Simple content-based retriever\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_manager):\n",
    "        super().__init__(embedding_manager)\n",
    "        self.name = \"content\"\n",
    "        \n",
    "    def retrieve(self, query: str, k: int = 3) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant chunks using content-based retrieval\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            k: Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List of chunk results with relevance scores\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Get query embedding\n",
    "        query_embedding = self.manager.model.encode([query])[0]\n",
    "        \n",
    "        # Search in FAISS index\n",
    "        distances, indices = self.manager.index.search(query_embedding.reshape(1, -1), k)\n",
    "        \n",
    "        # Get results\n",
    "        results = []\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            if idx != -1:\n",
    "                result = self.manager.chunk_metadata[idx].copy()\n",
    "                similarity = 1 - (distances[0][i] / 2)  # Convert L2 distance to similarity\n",
    "                result['scores'] = {\n",
    "                    'rank': i + 1,\n",
    "                    'vector': similarity\n",
    "                }\n",
    "                result['distance'] = float(distances[0][i])  # For compatibility\n",
    "                results.append(result)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        logging.info(f\"Content retrieval completed in {elapsed:.2f} seconds\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "class CombinedRetriever(BaseRetriever):\n",
    "    \"\"\"Combined content and metadata retriever\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_manager):\n",
    "        super().__init__(embedding_manager)\n",
    "        self.name = \"combined\"\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "    def retrieve(self, query: str, k: int = 3) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant chunks using combined retrieval\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            k: Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List of chunk results with relevance scores\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Get query embedding\n",
    "        query_embedding = self.manager.model.encode([query])[0]\n",
    "        \n",
    "        # Search in FAISS index\n",
    "        distances, indices = self.manager.index.search(query_embedding.reshape(1, -1), k * 2)\n",
    "        \n",
    "        # Process results with metadata scoring\n",
    "        combined_results = []\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            if idx != -1:\n",
    "                chunk = self.manager.chunk_metadata[idx].copy()\n",
    "                \n",
    "                # Content score (from embedding similarity)\n",
    "                content_score = 1 - (distances[0][i] / 2)  # Convert L2 distance to similarity\n",
    "                \n",
    "                # Metadata score\n",
    "                metadata_score = 0.0\n",
    "                \n",
    "                # Extract entities from query\n",
    "                doc = self.nlp(query)\n",
    "                query_entities = set([ent.text.lower() for ent in doc.ents])\n",
    "                \n",
    "                # Check topic matches\n",
    "                if 'topics' in chunk and chunk['topics']:\n",
    "                    topics = set([t.lower() for t in chunk['topics']])\n",
    "                    topic_matches = len(query_entities.intersection(topics))\n",
    "                    if topic_matches > 0:\n",
    "                        metadata_score += 0.5 * (topic_matches / len(query_entities) if query_entities else 0)\n",
    "                \n",
    "                # Check summary match if available\n",
    "                if 'summary' in chunk and chunk['summary']:\n",
    "                    summary_doc = self.nlp(chunk['summary'].lower())\n",
    "                    summary_entities = set([ent.text.lower() for ent in summary_doc.ents])\n",
    "                    entity_matches = len(query_entities.intersection(summary_entities))\n",
    "                    if entity_matches > 0:\n",
    "                        metadata_score += 0.3 * (entity_matches / len(query_entities) if query_entities else 0)\n",
    "                \n",
    "                # Calculate combined score (70% content, 30% metadata)\n",
    "                combined_score = 0.7 * content_score + 0.3 * metadata_score\n",
    "                \n",
    "                chunk['scores'] = {\n",
    "                    'vector': content_score,\n",
    "                    'metadata': metadata_score,\n",
    "                    'combined': combined_score\n",
    "                }\n",
    "                chunk['distance'] = 1 - combined_score  # For compatibility\n",
    "                combined_results.append((combined_score, chunk))\n",
    "        \n",
    "        # Sort by combined score and take top k\n",
    "        combined_results.sort(reverse=True)\n",
    "        \n",
    "        final_results = []\n",
    "        for i, (score, chunk) in enumerate(combined_results[:k]):\n",
    "            chunk['scores']['rank'] = i + 1\n",
    "            final_results.append(chunk)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        logging.info(f\"Combined retrieval completed in {elapsed:.2f} seconds\")\n",
    "        \n",
    "        return final_results\n",
    "\n",
    "\n",
    "class ContentESRetriever(BaseRetriever):\n",
    "    \"\"\"Content retriever enhanced with Elasticsearch and reranking\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                embedding_manager,\n",
    "                es_manager: ElasticSearchManager,\n",
    "                reranker_model: str = 'cross-encoder/ms-marco-MiniLM-L-6-v2',\n",
    "                content_weight: float = 0.5,\n",
    "                es_weight: float = 0.3,\n",
    "                rerank_weight: float = 0.2):\n",
    "        \"\"\"\n",
    "        Initialize the content retriever with Elasticsearch and reranking\n",
    "        \n",
    "        Args:\n",
    "            embedding_manager: FAISS embedding manager\n",
    "            es_manager: Elasticsearch manager\n",
    "            reranker_model: Name of cross-encoder model for reranking\n",
    "            content_weight: Weight for content-based retrieval\n",
    "            es_weight: Weight for Elasticsearch results\n",
    "            rerank_weight: Weight for reranker scores\n",
    "        \"\"\"\n",
    "        super().__init__(embedding_manager)\n",
    "        self.name = \"content_es\"\n",
    "        self.es_manager = es_manager\n",
    "        self.reranker = CrossEncoder(reranker_model)\n",
    "        self.content_weight = content_weight\n",
    "        self.es_weight = es_weight\n",
    "        self.rerank_weight = rerank_weight\n",
    "        \n",
    "    def retrieve(self, query: str, k: int = 3) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant chunks using content-based retrieval enhanced with ES\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            k: Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List of chunk results with relevance scores\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Stage 1: Get content-based results (from FAISS)\n",
    "        query_embedding = self.manager.model.encode([query])[0]\n",
    "        distances, indices = self.manager.index.search(query_embedding.reshape(1, -1), k * 2)\n",
    "        \n",
    "        # Process content results\n",
    "        content_results = {}\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            if idx != -1:\n",
    "                chunk_id = self.manager.chunk_metadata[idx]['chunk_id']\n",
    "                similarity = 1 - (distances[0][i] / 2)  # Convert L2 distance to similarity\n",
    "                content_results[chunk_id] = {\n",
    "                    'content_score': similarity,\n",
    "                    'metadata': self.manager.chunk_metadata[idx]\n",
    "                }\n",
    "        \n",
    "        # Stage 2: Get Elasticsearch results\n",
    "        es_results = self.es_manager.search(query, k * 2)\n",
    "        \n",
    "        # Process ES results\n",
    "        es_scores = {}\n",
    "        for result in es_results:\n",
    "            chunk_id = result['chunk_id']\n",
    "            es_scores[chunk_id] = result['score']\n",
    "        \n",
    "        # Normalize ES scores (if any results)\n",
    "        if es_scores:\n",
    "            max_es_score = max(es_scores.values())\n",
    "            es_scores = {chunk_id: score / max_es_score for chunk_id, score in es_scores.items()}\n",
    "        \n",
    "        # Stage 3: Combine unique results from both sources\n",
    "        all_candidates = {}\n",
    "        \n",
    "        # Add combined results\n",
    "        for chunk_id, data in combined_results.items():\n",
    "            all_candidates[chunk_id] = {\n",
    "                'metadata': data['metadata'],\n",
    "                'combined_score': data['combined_score'],\n",
    "                'vector': data['vector'],\n",
    "                'es_score': es_scores.get(chunk_id, 0.0)\n",
    "            }\n",
    "        \n",
    "        # Add ES results that weren't in combined results\n",
    "        for result in es_results:\n",
    "            chunk_id = result['chunk_id']\n",
    "            if chunk_id not in all_candidates:\n",
    "                # Find this chunk in FAISS manager's metadata\n",
    "                metadata = None\n",
    "                for idx, meta in self.manager.chunk_metadata.items():\n",
    "                    if meta['chunk_id'] == chunk_id:\n",
    "                        metadata = meta\n",
    "                        break\n",
    "                \n",
    "                if metadata:\n",
    "                    # Calculate combined score for this ES result\n",
    "                    combined_score = self._get_combined_score(query, metadata)\n",
    "                    all_candidates[chunk_id] = {\n",
    "                        'metadata': metadata,\n",
    "                        'combined_score': combined_score,\n",
    "                        'vector': 0.0,  # Was not in top vector results\n",
    "                        'es_score': es_scores[chunk_id]\n",
    "                    }\n",
    "        \n",
    "        # Stage 4: Rerank top candidates\n",
    "        rerank_candidates = list(all_candidates.items())\n",
    "        # Sort by preliminary combined score\n",
    "        rerank_candidates.sort(\n",
    "            key=lambda x: self.combined_weight * x[1]['combined_score'] + self.es_weight * x[1]['es_score'],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        # Take top candidates for reranking\n",
    "        rerank_candidates = rerank_candidates[:k * 2]\n",
    "        \n",
    "        if rerank_candidates:\n",
    "            # Prepare texts for reranker\n",
    "            texts = []\n",
    "            chunk_ids = []\n",
    "            for chunk_id, data in rerank_candidates:\n",
    "                chunk_ids.append(chunk_id)\n",
    "                texts.append(data['metadata']['content'])\n",
    "            \n",
    "            # Rerank\n",
    "            rerank_inputs = [[query, text] for text in texts]\n",
    "            rerank_scores = self.reranker.predict(rerank_inputs)\n",
    "            \n",
    "            # Normalize reranker scores\n",
    "            min_score = min(rerank_scores)\n",
    "            max_score = max(rerank_scores)\n",
    "            if max_score > min_score:\n",
    "                normalized_rerank_scores = [(score - min_score) / (max_score - min_score) for score in rerank_scores]\n",
    "            else:\n",
    "                normalized_rerank_scores = [1.0 for _ in rerank_scores]\n",
    "            \n",
    "            # Update candidates with reranker scores\n",
    "            for i, (chunk_id, score) in enumerate(zip(chunk_ids, normalized_rerank_scores)):\n",
    "                all_candidates[chunk_id]['rerank_score'] = score\n",
    "                \n",
    "                # Calculate final score\n",
    "                all_candidates[chunk_id]['final_score'] = (\n",
    "                    self.combined_weight * all_candidates[chunk_id]['combined_score'] +\n",
    "                    self.es_weight * all_candidates[chunk_id]['es_score'] +\n",
    "                    self.rerank_weight * score\n",
    "                )\n",
    "        \n",
    "        # Stage 5: Prepare final results\n",
    "        final_candidates = [(chunk_id, data) for chunk_id, data in all_candidates.items()]\n",
    "        final_candidates.sort(key=lambda x: x[1].get('final_score', 0.0), reverse=True)\n",
    "        \n",
    "        # Take top k results\n",
    "        final_results = []\n",
    "        for i, (chunk_id, data) in enumerate(final_candidates[:k]):\n",
    "            result = data['metadata'].copy()\n",
    "            result['scores'] = {\n",
    "                'rank': i + 1,\n",
    "                'vector': data['vector'],\n",
    "                'combined_score': data['combined_score'],\n",
    "                'es_score': data['es_score'],\n",
    "                'rerank_score': data.get('rerank_score', 0.0),\n",
    "                'final_score': data.get('final_score', 0.0)\n",
    "            }\n",
    "            result['distance'] = 1 - data.get('final_score', 0.0)  # For compatibility\n",
    "            final_results.append(result)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        logging.info(f\"Combined+ES retrieval completed in {elapsed:.2f} seconds\")\n",
    "        \n",
    "        return final_results_scores = {chunk_id: score / max_es_score for chunk_id, score in es_scores.items()}\n",
    "        \n",
    "        # Stage 3: Combine unique results from both sources\n",
    "        combined_candidates = {}\n",
    "        \n",
    "        # Add content results\n",
    "        for chunk_id, data in content_results.items():\n",
    "            combined_candidates[chunk_id] = {\n",
    "                'metadata': data['metadata'],\n",
    "                'content_score': data['content_score'],\n",
    "                'es_score': es_scores.get(chunk_id, 0.0)\n",
    "            }\n",
    "        \n",
    "        # Add ES results that weren't in content results\n",
    "        for result in es_results:\n",
    "            chunk_id = result['chunk_id']\n",
    "            if chunk_id not in combined_candidates:\n",
    "                # Find this chunk in FAISS manager's metadata\n",
    "                metadata = None\n",
    "                for idx, meta in self.manager.chunk_metadata.items():\n",
    "                    if meta['chunk_id'] == chunk_id:\n",
    "                        metadata = meta\n",
    "                        break\n",
    "                \n",
    "                if metadata:\n",
    "                    combined_candidates[chunk_id] = {\n",
    "                        'metadata': metadata,\n",
    "                        'content_score': 0.0,  # Was not in top content results\n",
    "                        'es_score': es_scores[chunk_id]\n",
    "                    }\n",
    "        \n",
    "        # Stage 4: Rerank top candidates\n",
    "        rerank_candidates = list(combined_candidates.items())\n",
    "        # Sort by preliminary combined score\n",
    "        rerank_candidates.sort(\n",
    "            key=lambda x: self.content_weight * x[1]['content_score'] + self.es_weight * x[1]['es_score'],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        # Take top candidates for reranking\n",
    "        rerank_candidates = rerank_candidates[:k * 2]\n",
    "        \n",
    "        if rerank_candidates:\n",
    "            # Prepare texts for reranker\n",
    "            texts = []\n",
    "            chunk_ids = []\n",
    "            for chunk_id, data in rerank_candidates:\n",
    "                chunk_ids.append(chunk_id)\n",
    "                texts.append(data['metadata']['content'])\n",
    "            \n",
    "            # Rerank\n",
    "            rerank_inputs = [[query, text] for text in texts]\n",
    "            rerank_scores = self.reranker.predict(rerank_inputs)\n",
    "            \n",
    "            # Normalize reranker scores\n",
    "            min_score = min(rerank_scores)\n",
    "            max_score = max(rerank_scores)\n",
    "            if max_score > min_score:\n",
    "                normalized_rerank_scores = [(score - min_score) / (max_score - min_score) for score in rerank_scores]\n",
    "            else:\n",
    "                normalized_rerank_scores = [1.0 for _ in rerank_scores]\n",
    "            \n",
    "            # Update candidates with reranker scores\n",
    "            for i, (chunk_id, score) in enumerate(zip(chunk_ids, normalized_rerank_scores)):\n",
    "                combined_candidates[chunk_id]['rerank_score'] = score\n",
    "                \n",
    "                # Calculate final score\n",
    "                combined_candidates[chunk_id]['final_score'] = (\n",
    "                    self.content_weight * combined_candidates[chunk_id]['content_score'] +\n",
    "                    self.es_weight * combined_candidates[chunk_id]['es_score'] +\n",
    "                    self.rerank_weight * score\n",
    "                )\n",
    "        \n",
    "        # Stage 5: Prepare final results\n",
    "        final_candidates = [(chunk_id, data) for chunk_id, data in combined_candidates.items()]\n",
    "        final_candidates.sort(key=lambda x: x[1].get('final_score', 0.0), reverse=True)\n",
    "        \n",
    "        # Take top k results\n",
    "        final_results = []\n",
    "        for i, (chunk_id, data) in enumerate(final_candidates[:k]):\n",
    "            result = data['metadata'].copy()\n",
    "            result['scores'] = {\n",
    "                'rank': i + 1,\n",
    "                'vector': data['content_score'],  # For consistency in reporting\n",
    "                'content_score': data['content_score'],\n",
    "                'es_score': data['es_score'],\n",
    "                'rerank_score': data.get('rerank_score', 0.0),\n",
    "                'final_score': data.get('final_score', 0.0)\n",
    "            }\n",
    "            result['distance'] = 1 - data.get('final_score', 0.0)  # For compatibility\n",
    "            final_results.append(result)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        logging.info(f\"Content+ES retrieval completed in {elapsed:.2f} seconds\")\n",
    "        \n",
    "        return final_results\n",
    "\n",
    "\n",
    "class CombinedESRetriever(BaseRetriever):\n",
    "    \"\"\"Combined retriever enhanced with Elasticsearch and reranking\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                embedding_manager,\n",
    "                es_manager: ElasticSearchManager,\n",
    "                reranker_model: str = 'cross-encoder/ms-marco-MiniLM-L-6-v2',\n",
    "                combined_weight: float = 0.5,\n",
    "                es_weight: float = 0.3,\n",
    "                rerank_weight: float = 0.2):\n",
    "        \"\"\"\n",
    "        Initialize the combined retriever with Elasticsearch and reranking\n",
    "        \n",
    "        Args:\n",
    "            embedding_manager: FAISS embedding manager\n",
    "            es_manager: Elasticsearch manager\n",
    "            reranker_model: Name of cross-encoder model for reranking\n",
    "            combined_weight: Weight for combined retrieval\n",
    "            es_weight: Weight for Elasticsearch results\n",
    "            rerank_weight: Weight for reranker scores\n",
    "        \"\"\"\n",
    "        super().__init__(embedding_manager)\n",
    "        self.name = \"combined_es\"\n",
    "        self.es_manager = es_manager\n",
    "        self.reranker = CrossEncoder(reranker_model)\n",
    "        self.combined_weight = combined_weight\n",
    "        self.es_weight = es_weight\n",
    "        self.rerank_weight = rerank_weight\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "    def _get_combined_score(self, query: str, chunk: Dict[str, Any]) -> float:\n",
    "        \"\"\"\n",
    "        Calculate combined score for a chunk (content + metadata)\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            chunk: Chunk to score\n",
    "            \n",
    "        Returns:\n",
    "            Combined score\n",
    "        \"\"\"\n",
    "        # Content score (via embedding similarity)\n",
    "        query_embedding = self.manager.model.encode([query])[0]\n",
    "        content_embedding = self.manager.model.encode([chunk['content']])[0]\n",
    "        content_similarity = np.dot(query_embedding, content_embedding) / (\n",
    "            np.linalg.norm(query_embedding) * np.linalg.norm(content_embedding)\n",
    "        )\n",
    "        \n",
    "        # Metadata score\n",
    "        metadata_score = 0.0\n",
    "        \n",
    "        # Extract entities from query\n",
    "        doc = self.nlp(query)\n",
    "        query_entities = set([ent.text.lower() for ent in doc.ents])\n",
    "        \n",
    "        # Check topic matches\n",
    "        if 'topics' in chunk and chunk['topics']:\n",
    "            topics = set([t.lower() for t in chunk['topics']])\n",
    "            topic_matches = len(query_entities.intersection(topics))\n",
    "            if topic_matches > 0:\n",
    "                metadata_score += 0.5 * (topic_matches / len(query_entities) if query_entities else 0)\n",
    "        \n",
    "        # Check summary match if available\n",
    "        if 'summary' in chunk and chunk['summary']:\n",
    "            summary_doc = self.nlp(chunk['summary'].lower())\n",
    "            summary_entities = set([ent.text.lower() for ent in summary_doc.ents])\n",
    "            entity_matches = len(query_entities.intersection(summary_entities))\n",
    "            if entity_matches > 0:\n",
    "                metadata_score += 0.3 * (entity_matches / len(query_entities) if query_entities else 0)\n",
    "        \n",
    "        # Combined score (70% content, 30% metadata)\n",
    "        return 0.7 * content_similarity + 0.3 * metadata_score\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 3) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant chunks using combined retrieval enhanced with ES\n",
    "\n",
    "        Args:\n",
    "            query: The search query\n",
    "            k: Number of results to return\n",
    "\n",
    "        Returns:\n",
    "            List of chunk results with relevance scores\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Stage 1: Get combined results (content + metadata) from FAISS\n",
    "        query_embedding = self.manager.model.encode([query])[0]\n",
    "        distances, indices = self.manager.index.search(query_embedding.reshape(1, -1), k * 2)\n",
    "\n",
    "        # Process combined results with metadata scoring\n",
    "        combined_results = {}\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            if idx != -1:\n",
    "                chunk = self.manager.chunk_metadata[idx]\n",
    "                chunk_id = chunk['chunk_id']\n",
    "                # Calculate combined score\n",
    "                combined_score = self._get_combined_score(query, chunk)\n",
    "                combined_results[chunk_id] = {\n",
    "                    'combined_score': combined_score,\n",
    "                    'metadata': chunk,\n",
    "                    'vector': 1 - (distances[0][i] / 2)  # For reporting\n",
    "                }\n",
    "\n",
    "        # Stage 2: Get Elasticsearch results\n",
    "        es_results = self.es_manager.search(query, k * 2)\n",
    "\n",
    "        # Process ES results\n",
    "        es_scores = {}\n",
    "        for result in es_results:\n",
    "            chunk_id = result['chunk_id']\n",
    "            es_scores[chunk_id] = result['score']\n",
    "\n",
    "        # Normalize ES scores (if any results)\n",
    "        if es_scores:\n",
    "            max_es_score = max(es_scores.values())\n",
    "            es_scores = {chunk_id: score / max_es_score for chunk_id, score in es_scores.items()}\n",
    "\n",
    "        # Stage 3: Combine unique results from both sources\n",
    "        all_candidates = {}\n",
    "\n",
    "        # Add combined results\n",
    "        for chunk_id, data in combined_results.items():\n",
    "            all_candidates[chunk_id] = {\n",
    "                'metadata': data['metadata'],\n",
    "                'combined_score': data['combined_score'],\n",
    "                'vector': data['vector'],\n",
    "                'es_score': es_scores.get(chunk_id, 0.0)\n",
    "            }\n",
    "\n",
    "        # Add ES results that weren't in combined results\n",
    "        for result in es_results:\n",
    "            chunk_id = result['chunk_id']\n",
    "            if chunk_id not in all_candidates:\n",
    "                # Find this chunk in FAISS manager's metadata\n",
    "                metadata = None\n",
    "                for idx, meta in self.manager.chunk_metadata.items():\n",
    "                    if meta['chunk_id'] == chunk_id:\n",
    "                        metadata = meta\n",
    "                        break\n",
    "\n",
    "                if metadata:\n",
    "                    # Calculate combined score for this ES result\n",
    "                    combined_score = self._get_combined_score(query, metadata)\n",
    "                    all_candidates[chunk_id] = {\n",
    "                        'metadata': metadata,\n",
    "                        'combined_score': combined_score,\n",
    "                        'vector': 0.0,  # Was not in top vector results\n",
    "                        'es_score': es_scores[chunk_id]\n",
    "                    }\n",
    "\n",
    "        # Stage 4: Rerank top candidates\n",
    "        rerank_candidates = list(all_candidates.items())\n",
    "        # Sort by preliminary combined score\n",
    "        rerank_candidates.sort(\n",
    "            key=lambda x: self.combined_weight * x[1]['combined_score'] + self.es_weight * x[1]['es_score'],\n",
    "            reverse=True\n",
    "        )\n",
    "\n",
    "        # Take top candidates for reranking\n",
    "        rerank_candidates = rerank_candidates[:k * 2]\n",
    "\n",
    "        if rerank_candidates:\n",
    "            # Prepare texts for reranker\n",
    "            texts = []\n",
    "            chunk_ids = []\n",
    "            for chunk_id, data in rerank_candidates:\n",
    "                chunk_ids.append(chunk_id)\n",
    "                texts.append(data['metadata']['content'])\n",
    "\n",
    "            # Rerank\n",
    "            rerank_inputs = [[query, text] for text in texts]\n",
    "            rerank_scores = self.reranker.predict(rerank_inputs)\n",
    "\n",
    "            # Normalize reranker scores\n",
    "            min_score = min(rerank_scores)\n",
    "            max_score = max(rerank_scores)\n",
    "            if max_score > min_score:\n",
    "                normalized_rerank_scores = [(score - min_score) / (max_score - min_score) for score in rerank_scores]\n",
    "            else:\n",
    "                normalized_rerank_scores = [1.0 for _ in rerank_scores]\n",
    "\n",
    "            # Update candidates with reranker scores\n",
    "            for i, (chunk_id, score) in enumerate(zip(chunk_ids, normalized_rerank_scores)):\n",
    "                all_candidates[chunk_id]['rerank_score'] = score\n",
    "\n",
    "                # Calculate final score\n",
    "                all_candidates[chunk_id]['final_score'] = (\n",
    "                    self.combined_weight * all_candidates[chunk_id]['combined_score'] +\n",
    "                    self.es_weight * all_candidates[chunk_id]['es_score'] +\n",
    "                    self.rerank_weight * score\n",
    "                )\n",
    "\n",
    "        # Stage 5: Prepare final results\n",
    "        final_candidates = [(chunk_id, data) for chunk_id, data in all_candidates.items()]\n",
    "        final_candidates.sort(key=lambda x: x[1].get('final_score', 0.0), reverse=True)\n",
    "\n",
    "        # Take top k results\n",
    "        final_results = []\n",
    "        for i, (chunk_id, data) in enumerate(final_candidates[:k]):\n",
    "            result = data['metadata'].copy()\n",
    "            result['scores'] = {\n",
    "                'rank': i + 1,\n",
    "                'vector': data['vector'],\n",
    "                'combined_score': data['combined_score'],\n",
    "                'es_score': data['es_score'],\n",
    "                'rerank_score': data.get('rerank_score', 0.0),\n",
    "                'final_score': data.get('final_score', 0.0)\n",
    "            }\n",
    "            result['distance'] = 1 - data.get('final_score', 0.0)  # For compatibility\n",
    "            final_results.append(result)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        logging.info(f\"Combined+ES retrieval completed in {elapsed:.2f} seconds\")\n",
    "\n",
    "        return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54655c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 02:28:42,398 - INFO - Use pytorch device_name: cpu\n",
      "2025-03-03 02:28:42,398 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "C:\\Users\\prana\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "NodeConfig.__init__() missing 1 required positional argument: 'scheme'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m retrievers, results \u001b[38;5;241m=\u001b[39m setup_and_use_retrievers()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRetrieval complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 789\u001b[0m, in \u001b[0;36msetup_and_use_retrievers\u001b[1;34m()\u001b[0m\n\u001b[0;32m    785\u001b[0m embedding_manager \u001b[38;5;241m=\u001b[39m SimpleFAISSManager()\n\u001b[0;32m    787\u001b[0m \u001b[38;5;66;03m# 2. Initialize Elasticsearch and index documents\u001b[39;00m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# (in a real scenario, you would index your chunks here)\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m es_manager \u001b[38;5;241m=\u001b[39m ElasticSearchManager(index_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3_documents\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    791\u001b[0m \u001b[38;5;66;03m# 3. Create all four retrievers\u001b[39;00m\n\u001b[0;32m    792\u001b[0m retrievers \u001b[38;5;241m=\u001b[39m EnhancedRetrieverFactory\u001b[38;5;241m.\u001b[39mcreate_retrievers(\n\u001b[0;32m    793\u001b[0m     embedding_manager\u001b[38;5;241m=\u001b[39membedding_manager,\n\u001b[0;32m    794\u001b[0m     es_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocalhost\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    795\u001b[0m     es_port\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m9200\u001b[39m\n\u001b[0;32m    796\u001b[0m )\n",
      "Cell \u001b[1;32mIn[6], line 55\u001b[0m, in \u001b[0;36mElasticSearchManager.__init__\u001b[1;34m(self, index_name, host, port)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;124;03mInitialize Elasticsearch manager\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m    port: Elasticsearch port\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_name \u001b[38;5;241m=\u001b[39m index_name\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mes \u001b[38;5;241m=\u001b[39m Elasticsearch([{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhost\u001b[39m\u001b[38;5;124m'\u001b[39m: host, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mport\u001b[39m\u001b[38;5;124m'\u001b[39m: port}])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\elasticsearch\\_sync\\client\\__init__.py:339\u001b[0m, in \u001b[0;36mElasticsearch.__init__\u001b[1;34m(self, hosts, cloud_id, api_key, basic_auth, bearer_auth, opaque_id, headers, connections_per_node, http_compress, verify_certs, ca_certs, client_cert, client_key, ssl_assert_hostname, ssl_assert_fingerprint, ssl_version, ssl_context, ssl_show_warn, transport_class, request_timeout, node_class, node_pool_class, randomize_nodes_in_pool, node_selector_class, dead_node_backoff_factor, max_dead_node_backoff, serializer, serializers, default_mimetype, max_retries, retry_on_status, retry_on_timeout, sniff_on_start, sniff_before_requests, sniff_on_node_failure, sniff_timeout, min_delay_between_sniffing, sniffed_node_callback, meta_header, timeout, randomize_hosts, host_info_callback, sniffer_timeout, sniff_on_connection_fail, http_auth, maxsize, _transport)\u001b[0m\n\u001b[0;32m    336\u001b[0m         requests_session_auth \u001b[38;5;241m=\u001b[39m http_auth\n\u001b[0;32m    337\u001b[0m         http_auth \u001b[38;5;241m=\u001b[39m DEFAULT\n\u001b[1;32m--> 339\u001b[0m node_configs \u001b[38;5;241m=\u001b[39m client_node_configs(\n\u001b[0;32m    340\u001b[0m     hosts,\n\u001b[0;32m    341\u001b[0m     cloud_id\u001b[38;5;241m=\u001b[39mcloud_id,\n\u001b[0;32m    342\u001b[0m     requests_session_auth\u001b[38;5;241m=\u001b[39mrequests_session_auth,\n\u001b[0;32m    343\u001b[0m     connections_per_node\u001b[38;5;241m=\u001b[39mconnections_per_node,\n\u001b[0;32m    344\u001b[0m     http_compress\u001b[38;5;241m=\u001b[39mhttp_compress,\n\u001b[0;32m    345\u001b[0m     verify_certs\u001b[38;5;241m=\u001b[39mverify_certs,\n\u001b[0;32m    346\u001b[0m     ca_certs\u001b[38;5;241m=\u001b[39mca_certs,\n\u001b[0;32m    347\u001b[0m     client_cert\u001b[38;5;241m=\u001b[39mclient_cert,\n\u001b[0;32m    348\u001b[0m     client_key\u001b[38;5;241m=\u001b[39mclient_key,\n\u001b[0;32m    349\u001b[0m     ssl_assert_hostname\u001b[38;5;241m=\u001b[39mssl_assert_hostname,\n\u001b[0;32m    350\u001b[0m     ssl_assert_fingerprint\u001b[38;5;241m=\u001b[39mssl_assert_fingerprint,\n\u001b[0;32m    351\u001b[0m     ssl_version\u001b[38;5;241m=\u001b[39mssl_version,\n\u001b[0;32m    352\u001b[0m     ssl_context\u001b[38;5;241m=\u001b[39mssl_context,\n\u001b[0;32m    353\u001b[0m     ssl_show_warn\u001b[38;5;241m=\u001b[39mssl_show_warn,\n\u001b[0;32m    354\u001b[0m )\n\u001b[0;32m    355\u001b[0m transport_kwargs: t\u001b[38;5;241m.\u001b[39mDict[\u001b[38;5;28mstr\u001b[39m, t\u001b[38;5;241m.\u001b[39mAny] \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m node_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m DEFAULT:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\elasticsearch\\_sync\\client\\utils.py:119\u001b[0m, in \u001b[0;36mclient_node_configs\u001b[1;34m(hosts, cloud_id, requests_session_auth, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m hosts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     node_configs \u001b[38;5;241m=\u001b[39m hosts_to_node_configs(hosts)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# Remove all values which are 'DEFAULT' to avoid overwriting actual defaults.\u001b[39;00m\n\u001b[0;32m    122\u001b[0m node_options \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m DEFAULT}\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\elasticsearch\\_sync\\client\\utils.py:168\u001b[0m, in \u001b[0;36mhosts_to_node_configs\u001b[1;34m(hosts)\u001b[0m\n\u001b[0;32m    165\u001b[0m     node_configs\u001b[38;5;241m.\u001b[39mappend(url_to_node_config(host))\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(host, Mapping):\n\u001b[1;32m--> 168\u001b[0m     node_configs\u001b[38;5;241m.\u001b[39mappend(host_mapping_to_node_config(host))\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhosts\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a list of URLs, NodeConfigs, or dictionaries\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    172\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\elasticsearch\\_sync\\client\\utils.py:235\u001b[0m, in \u001b[0;36mhost_mapping_to_node_config\u001b[1;34m(host)\u001b[0m\n\u001b[0;32m    228\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    229\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl_prefix\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m option is deprecated in favor of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath_prefix\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    230\u001b[0m         category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[0;32m    231\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mwarn_stacklevel(),\n\u001b[0;32m    232\u001b[0m     )\n\u001b[0;32m    233\u001b[0m     options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath_prefix\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl_prefix\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m NodeConfig(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "\u001b[1;31mTypeError\u001b[0m: NodeConfig.__init__() missing 1 required positional argument: 'scheme'"
     ]
    }
   ],
   "source": [
    "\n",
    "retrievers, results = setup_and_use_retrievers()\n",
    "print(\"\\nRetrieval complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf688ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting elasticsearch\n",
      "  Downloading elasticsearch-8.17.1-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting elastic-transport<9,>=8.15.1 (from elasticsearch)\n",
      "  Downloading elastic_transport-8.17.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in c:\\users\\prana\\anaconda3\\lib\\site-packages (from elastic-transport<9,>=8.15.1->elasticsearch) (1.26.20)\n",
      "Requirement already satisfied: certifi in c:\\users\\prana\\anaconda3\\lib\\site-packages (from elastic-transport<9,>=8.15.1->elasticsearch) (2023.7.22)\n",
      "Downloading elasticsearch-8.17.1-py3-none-any.whl (653 kB)\n",
      "   ---------------------------------------- 0.0/654.0 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 262.1/654.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 654.0/654.0 kB 2.1 MB/s eta 0:00:00\n",
      "Downloading elastic_transport-8.17.0-py3-none-any.whl (64 kB)\n",
      "Installing collected packages: elastic-transport, elasticsearch\n",
      "Successfully installed elastic-transport-8.17.0 elasticsearch-8.17.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~illow (C:\\Users\\prana\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~rotobuf (C:\\Users\\prana\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~yarrow (C:\\Users\\prana\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~illow (C:\\Users\\prana\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~rotobuf (C:\\Users\\prana\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~yarrow (C:\\Users\\prana\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~illow (C:\\Users\\prana\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~rotobuf (C:\\Users\\prana\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~yarrow (C:\\Users\\prana\\anaconda3\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc868f8",
   "metadata": {},
   "source": [
    "Let me explain how the four retrievers work and how their scores are calculated, all using the same underlying index but with different retrieval strategies.\n",
    "\n",
    "## Overview of the Four Retrievers\n",
    "\n",
    "I've implemented four different retrievers that all use the same FAISS index but apply different retrieval strategies:\n",
    "\n",
    "1. **ContentRetriever**: Pure vector similarity search on document content\n",
    "2. **CombinedRetriever**: Vector search with added metadata influence \n",
    "3. **ContentESRetriever**: Content-based retrieval enhanced with Elasticsearch and reranking\n",
    "4. **CombinedESRetriever**: Combined retrieval enhanced with Elasticsearch and reranking\n",
    "\n",
    "## Score Calculation for Each Retriever\n",
    "\n",
    "### 1. ContentRetriever\n",
    "- **Score Components**:\n",
    "  - `vector`: Cosine similarity between query and document embeddings\n",
    "- **Final Score Calculation**:\n",
    "  - Uses only vector similarity from FAISS index\n",
    "  - No weights or combinations - pure semantic search\n",
    "\n",
    "### 2. CombinedRetriever\n",
    "- **Score Components**:\n",
    "  - `vector`: Cosine similarity between query and document embeddings (70%)\n",
    "  - `metadata`: Score based on topic and entity matches between query and document metadata (30%)\n",
    "- **Final Score Calculation**:\n",
    "  - `combined = 0.7 * vector_score + 0.3 * metadata_score`\n",
    "\n",
    "### 3. ContentESRetriever\n",
    "- **Score Components**:\n",
    "  - `content_score`: Vector similarity from FAISS (weight = 0.5)\n",
    "  - `es_score`: BM25 score from Elasticsearch (weight = 0.3)\n",
    "  - `rerank_score`: Score from cross-encoder reranking (weight = 0.2)\n",
    "- **Final Score Calculation**:\n",
    "  - `final_score = 0.5 * content_score + 0.3 * es_score + 0.2 * rerank_score`\n",
    "\n",
    "### 4. CombinedESRetriever\n",
    "- **Score Components**:\n",
    "  - `combined_score`: Combined vector and metadata score (weight = 0.5)\n",
    "  - `es_score`: BM25 score from Elasticsearch (weight = 0.3)\n",
    "  - `rerank_score`: Score from cross-encoder reranking (weight = 0.2)\n",
    "- **Final Score Calculation**:\n",
    "  - `final_score = 0.5 * combined_score + 0.3 * es_score + 0.2 * rerank_score`\n",
    "  - Where `combined_score = 0.7 * vector_score + 0.3 * metadata_score`\n",
    "\n",
    "## Retrieval Process Summary\n",
    "\n",
    "Here's a step-by-step breakdown of how each retriever works:\n",
    "\n",
    "### 1. ContentRetriever\n",
    "1. Encode query into vector embedding\n",
    "2. Search FAISS index for nearest neighbors\n",
    "3. Return top-k chunks with vector similarity scores\n",
    "\n",
    "### 2. CombinedRetriever\n",
    "1. Encode query into vector embedding\n",
    "2. Search FAISS index for nearest neighbors\n",
    "3. Calculate metadata score for each candidate (topic/entity matching)\n",
    "4. Calculate combined score by weighting vector and metadata scores\n",
    "5. Rank results and return top-k chunks\n",
    "\n",
    "### 3. ContentESRetriever\n",
    "1. Encode query into vector embedding\n",
    "2. Search FAISS index for nearest neighbors (content-based)\n",
    "3. Search Elasticsearch index with the same query\n",
    "4. Combine unique results from both sources\n",
    "5. Rerank top candidates using cross-encoder\n",
    "6. Calculate final weighted score across all three methods\n",
    "7. Return top-k chunks\n",
    "\n",
    "### 4. CombinedESRetriever\n",
    "1. Encode query into vector embedding\n",
    "2. Search FAISS index for nearest neighbors (combined approach)\n",
    "3. Calculate metadata score for each candidate\n",
    "4. Search Elasticsearch index with the same query\n",
    "5. Combine unique results from both sources\n",
    "6. Rerank top candidates using cross-encoder\n",
    "7. Calculate final weighted score across all three methods\n",
    "8. Return top-k chunks\n",
    "\n",
    "Each retriever uses the same FAISS index and chunk data but applies different algorithms and weights to calculate relevance scores. This approach allows you to compare different retrieval strategies while maintaining a single source of truth for your document data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac5ab32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
