{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "053673bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Set, Tuple\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from bert_score import score as bert_score\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import defaultdict, Counter\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pickle\n",
    "\n",
    "class RougeCalculator:\n",
    "    \"\"\"Custom implementation of ROUGE metrics\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_ngrams(text: str, n: int) -> Counter:\n",
    "        \"\"\"Get n-gram counts from text\"\"\"\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        ngrams = Counter()\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = tuple(tokens[i:i + n])\n",
    "            ngrams[ngram] += 1\n",
    "        return ngrams\n",
    "        \n",
    "    @staticmethod\n",
    "    def _lcs(str1: List[str], str2: List[str]) -> int:\n",
    "        \"\"\"Calculate Longest Common Subsequence length\"\"\"\n",
    "        m, n = len(str1), len(str2)\n",
    "        dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "        \n",
    "        for i in range(1, m + 1):\n",
    "            for j in range(1, n + 1):\n",
    "                if str1[i-1] == str2[j-1]:\n",
    "                    dp[i][j] = dp[i-1][j-1] + 1\n",
    "                else:\n",
    "                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
    "        \n",
    "        return dp[m][n]\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_rouge_scores(generated: str, reference: str) -> Dict[str, float]:\n",
    "        \"\"\"Calculate ROUGE-1, ROUGE-2, and ROUGE-L scores\"\"\"\n",
    "        # Convert to lowercase and tokenize\n",
    "        gen_tokens = word_tokenize(generated.lower())\n",
    "        ref_tokens = word_tokenize(reference.lower())\n",
    "        \n",
    "        # Get n-grams\n",
    "        gen_1grams = RougeCalculator._get_ngrams(generated, 1)\n",
    "        ref_1grams = RougeCalculator._get_ngrams(reference, 1)\n",
    "        gen_2grams = RougeCalculator._get_ngrams(generated, 2)\n",
    "        ref_2grams = RougeCalculator._get_ngrams(reference, 2)\n",
    "        \n",
    "        # Calculate ROUGE-1\n",
    "        overlap_1grams = sum((gen_1grams & ref_1grams).values())\n",
    "        if len(gen_1grams) == 0:\n",
    "            rouge1_precision = 0\n",
    "        else:\n",
    "            rouge1_precision = overlap_1grams / sum(gen_1grams.values())\n",
    "            \n",
    "        if len(ref_1grams) == 0:\n",
    "            rouge1_recall = 0\n",
    "        else:\n",
    "            rouge1_recall = overlap_1grams / sum(ref_1grams.values())\n",
    "            \n",
    "        rouge1_f1 = (2 * rouge1_precision * rouge1_recall) / (rouge1_precision + rouge1_recall + 1e-8)\n",
    "        \n",
    "        # Calculate ROUGE-2\n",
    "        overlap_2grams = sum((gen_2grams & ref_2grams).values())\n",
    "        if len(gen_2grams) == 0:\n",
    "            rouge2_precision = 0\n",
    "        else:\n",
    "            rouge2_precision = overlap_2grams / sum(gen_2grams.values())\n",
    "            \n",
    "        if len(ref_2grams) == 0:\n",
    "            rouge2_recall = 0\n",
    "        else:\n",
    "            rouge2_recall = overlap_2grams / sum(ref_2grams.values())\n",
    "            \n",
    "        rouge2_f1 = (2 * rouge2_precision * rouge2_recall) / (rouge2_precision + rouge2_recall + 1e-8)\n",
    "        \n",
    "        # Calculate ROUGE-L\n",
    "        lcs_length = RougeCalculator._lcs(gen_tokens, ref_tokens)\n",
    "        if len(gen_tokens) == 0:\n",
    "            rougeL_precision = 0\n",
    "        else:\n",
    "            rougeL_precision = lcs_length / len(gen_tokens)\n",
    "            \n",
    "        if len(ref_tokens) == 0:\n",
    "            rougeL_recall = 0\n",
    "        else:\n",
    "            rougeL_recall = lcs_length / len(ref_tokens)\n",
    "            \n",
    "        rougeL_f1 = (2 * rougeL_precision * rougeL_recall) / (rougeL_precision + rougeL_recall + 1e-8)\n",
    "        \n",
    "        return {\n",
    "            'rouge1_f': rouge1_f1,\n",
    "            'rouge2_f': rouge2_f1,\n",
    "            'rougeL_f': rougeL_f1\n",
    "        }\n",
    "\n",
    "class RAGEvaluator:\n",
    "    def __init__(self, ground_truth_path: str, rag_results_path: str, output_dir: str = \"rag_evaluation\"):\n",
    "        \"\"\"Initialize RAG evaluator with paths to result files\"\"\"\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        logging.basicConfig(\n",
    "            filename=self.output_dir / 'evaluation.log',\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        self.ground_truth = self._load_json(ground_truth_path)\n",
    "        self.rag_results = self._load_json(rag_results_path)\n",
    "        self.ground_truth_map = {item['question']: item['answer'] for item in self.ground_truth}\n",
    "        \n",
    "        try:\n",
    "            nltk.download('punkt', quiet=True)\n",
    "            nltk.download('wordnet', quiet=True)\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"NLTK download error: {str(e)}\")\n",
    "        \n",
    "        self.rouge_calculator = RougeCalculator()\n",
    "        self.bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.bert_model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "        self.sim_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "            \n",
    "    def _load_json(self, path: str) -> List[Dict]:\n",
    "        \"\"\"Load JSON file with error handling\"\"\"\n",
    "        try:\n",
    "            with open(path, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading {path}: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def _save_checkpoint(self, results: Dict, name: str):\n",
    "        \"\"\"Save evaluation checkpoint\"\"\"\n",
    "        checkpoint_path = self.output_dir / f\"{name}_checkpoint.pkl\"\n",
    "        with open(checkpoint_path, 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "        self.logger.info(f\"Saved checkpoint: {checkpoint_path}\")\n",
    "            \n",
    "    def calculate_text_metrics(self, generated: str, reference: str) -> Dict[str, float]:\n",
    "        \"\"\"Calculate BLEU, ROUGE scores\"\"\"\n",
    "        try:\n",
    "            # BLEU\n",
    "            smoothie = SmoothingFunction().method1\n",
    "            bleu_score = sentence_bleu(\n",
    "                [word_tokenize(reference.lower())],\n",
    "                word_tokenize(generated.lower()),\n",
    "                smoothing_function=smoothie\n",
    "            )\n",
    "            \n",
    "            # ROUGE (using custom implementation)\n",
    "            rouge_scores = self.rouge_calculator.calculate_rouge_scores(generated, reference)\n",
    "            \n",
    "            return {\n",
    "                'bleu': bleu_score,\n",
    "                **rouge_scores\n",
    "            }\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error calculating text metrics: {str(e)}\")\n",
    "            return {'bleu': 0, 'rouge1_f': 0, 'rouge2_f': 0, 'rougeL_f': 0}\n",
    "            \n",
    "    def calculate_faithfulness(self, answer: str, retrieved_docs: List[Dict]) -> Dict[str, float]:\n",
    "        \"\"\"Calculate faithfulness and hallucination metrics\"\"\"\n",
    "        try:\n",
    "            # Combine all retrieved documents\n",
    "            context = \" \".join([doc['content'] for doc in retrieved_docs])\n",
    "            \n",
    "            # Tokenize answer and context\n",
    "            answer_tokens = set(word_tokenize(answer.lower()))\n",
    "            context_tokens = set(word_tokenize(context.lower()))\n",
    "            \n",
    "            # Calculate metrics\n",
    "            supported_tokens = answer_tokens.intersection(context_tokens)\n",
    "            hallucinated_tokens = answer_tokens - context_tokens\n",
    "            \n",
    "            if len(answer_tokens) == 0:\n",
    "                return {'faithfulness': 0, 'hallucination_rate': 1.0}\n",
    "            \n",
    "            faithfulness = len(supported_tokens) / len(answer_tokens)\n",
    "            hallucination_rate = len(hallucinated_tokens) / len(answer_tokens)\n",
    "            \n",
    "            return {\n",
    "                'faithfulness': faithfulness,\n",
    "                'hallucination_rate': hallucination_rate\n",
    "            }\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error calculating faithfulness: {str(e)}\")\n",
    "            return {'faithfulness': 0, 'hallucination_rate': 1.0}\n",
    "            \n",
    "    def calculate_coverage(self, answer: str, retrieved_docs: List[Dict]) -> float:\n",
    "        \"\"\"Calculate coverage using sentence embeddings similarity\"\"\"\n",
    "        try:\n",
    "            # Combine all retrieved documents\n",
    "            context = \" \".join([doc['content'] for doc in retrieved_docs])\n",
    "            \n",
    "            # Get embeddings\n",
    "            answer_emb = self.sim_model.encode([answer])\n",
    "            context_emb = self.sim_model.encode([context])\n",
    "            \n",
    "            # Calculate cosine similarity\n",
    "            similarity = torch.nn.functional.cosine_similarity(\n",
    "                torch.tensor(answer_emb), \n",
    "                torch.tensor(context_emb)\n",
    "            ).item()\n",
    "            \n",
    "            return float(similarity)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error calculating coverage: {str(e)}\")\n",
    "            return 0.0\n",
    "            \n",
    "    def calculate_retrieval_metrics(self) -> Dict[str, float]:\n",
    "        \"\"\"Calculate average retrieval time and distance\"\"\"\n",
    "        times = []\n",
    "        distances = []\n",
    "        \n",
    "        for result in self.rag_results:\n",
    "            if result.get('retrieval_time'):\n",
    "                times.append(float(result['retrieval_time']))\n",
    "            if result.get('avg_distance'):\n",
    "                distances.append(float(result['avg_distance']))\n",
    "                \n",
    "        return {\n",
    "            'avg_retrieval_time': np.mean(times) if times else 0,\n",
    "            'avg_distance': np.mean(distances) if distances else 0\n",
    "        }\n",
    "        \n",
    "    def evaluate(self) -> Dict[str, Any]:\n",
    "        \"\"\"Run full evaluation\"\"\"\n",
    "        self.logger.info(\"Starting RAG evaluation...\")\n",
    "        \n",
    "        results = {\n",
    "            'per_query': [],\n",
    "            'aggregated': defaultdict(list)\n",
    "        }\n",
    "        \n",
    "        # Process each query\n",
    "        for rag_result in tqdm(self.rag_results, desc=\"Evaluating queries\"):\n",
    "            question = rag_result['question']\n",
    "            ground_truth = self.ground_truth_map.get(question)\n",
    "            \n",
    "            if not ground_truth:\n",
    "                self.logger.warning(f\"No ground truth found for question: {question[:50]}...\")\n",
    "                continue\n",
    "                \n",
    "            # Calculate metrics\n",
    "            text_metrics = self.calculate_text_metrics(\n",
    "                rag_result['answer'], \n",
    "                ground_truth\n",
    "            )\n",
    "            \n",
    "            faithfulness_metrics = self.calculate_faithfulness(\n",
    "                rag_result['answer'],\n",
    "                rag_result['retrieved_docs']\n",
    "            )\n",
    "            \n",
    "            coverage = self.calculate_coverage(\n",
    "                rag_result['answer'],\n",
    "                rag_result['retrieved_docs']\n",
    "            )\n",
    "            \n",
    "            # Store results\n",
    "            query_result = {\n",
    "                'question': question,\n",
    "                'metrics': {\n",
    "                    **text_metrics,\n",
    "                    **faithfulness_metrics,\n",
    "                    'coverage': coverage\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            results['per_query'].append(query_result)\n",
    "            \n",
    "            # Aggregate metrics\n",
    "            for metric, value in query_result['metrics'].items():\n",
    "                results['aggregated'][metric].append(value)\n",
    "                \n",
    "            # Save checkpoint periodically\n",
    "            if len(results['per_query']) % 10 == 0:\n",
    "                self._save_checkpoint(results, 'evaluation')\n",
    "                \n",
    "        # Calculate retrieval metrics\n",
    "        retrieval_metrics = self.calculate_retrieval_metrics()\n",
    "        results['retrieval_metrics'] = retrieval_metrics\n",
    "        \n",
    "        # Calculate final aggregated metrics\n",
    "        results['final_metrics'] = {\n",
    "            metric: float(np.mean(values))\n",
    "            for metric, values in results['aggregated'].items()\n",
    "        }\n",
    "        \n",
    "        # Save final results\n",
    "        final_path = self.output_dir / \"evaluation_results.json\"\n",
    "        with open(final_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "            \n",
    "        self.logger.info(\"Evaluation complete!\")\n",
    "        return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "618ffd7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating queries:   0%|                                                                       | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72b6453f49a14e79a91d473c8d3ab1fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a395073be394df3bd38509261f552c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating queries:   2%|█▌                                                             | 1/40 [00:00<00:14,  2.63it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0113ff9650df4d42acc8522dd97c49b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3741b6c45568428fb01931beece69e1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating queries:   5%|███▏                                                           | 2/40 [00:00<00:15,  2.53it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43d7e898eab94197bc877d401a55d1b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00356096d1bc4f2196850b5a3b868ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating queries:   8%|████▋                                                          | 3/40 [00:01<00:15,  2.37it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54532b51c68a4f6d85f0b053bcdc423b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f89118aa28f9480eb8a2e1214c2b1de2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating queries:  10%|██████▎                                                        | 4/40 [00:01<00:16,  2.22it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b2a26bd5e7a4d589dd817a8ebe87fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e632b8bc8fed4ab4bf6c4d04eb5027ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████████████████████████████████████████████████████████| 40/40 [00:01<00:00, 20.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Metrics:\n",
      "bleu: 0.0105\n",
      "rouge1_f: 0.1108\n",
      "rouge2_f: 0.0415\n",
      "rougeL_f: 0.0888\n",
      "faithfulness: 0.4880\n",
      "hallucination_rate: 0.5120\n",
      "coverage: 0.6806\n",
      "\n",
      "Retrieval Metrics:\n",
      "avg_retrieval_time: 6.8266\n",
      "avg_distance: 0.5459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "\n",
    "evaluator = RAGEvaluator(\n",
    "    ground_truth_path='ground_truth.json',\n",
    "    rag_results_path='hybrid_evaluation/qna_content.json',\n",
    "    output_dir='rag_evaluation'\n",
    ")\n",
    "results = evaluator.evaluate()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nFinal Metrics:\")\n",
    "for metric, value in results['final_metrics'].items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nRetrieval Metrics:\")\n",
    "for metric, value in results['retrieval_metrics'].items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a797492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8cfb8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating queries:   0%|                                                                       | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b54519003c741e09e155c08dae1804b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e85c46a9a247f691b2b922283d806d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating queries:   2%|█▌                                                             | 1/40 [00:00<00:10,  3.62it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8496e6fb661742099ad2715530564687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3239a814adf48afaab89c535618dd53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating queries:   5%|███▏                                                           | 2/40 [00:00<00:14,  2.67it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0bd2d94298c43e5b4b2e03c4281a28e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed53b8af9a4646758a15738e2ec460fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating queries:   8%|████▋                                                          | 3/40 [00:00<00:11,  3.14it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab091dd7c4e41c6a47df958f71587e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c99c401f5f2413e8b54b5249a1afc34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating queries:  10%|██████▎                                                        | 4/40 [00:01<00:10,  3.36it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f12ef006fc9417696b2be747056c05d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb1cbcb800f4ce48a41fb063e9c6f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████████████████████████████████████████████████████████| 40/40 [00:01<00:00, 26.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Metrics:\n",
      "bleu: 0.0040\n",
      "rouge1_f: 0.0992\n",
      "rouge2_f: 0.0214\n",
      "rougeL_f: 0.0933\n",
      "faithfulness: 0.4555\n",
      "hallucination_rate: 0.5445\n",
      "coverage: 0.0342\n",
      "\n",
      "Retrieval Metrics:\n",
      "avg_retrieval_time: 5.8719\n",
      "avg_distance: 0.7510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "\n",
    "evaluator = RAGEvaluator(\n",
    "    ground_truth_path='ground_truth.json',\n",
    "    rag_results_path='hybrid_evaluation/qna_combined.json',\n",
    "    output_dir='rag_evaluation'\n",
    ")\n",
    "results = evaluator.evaluate()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nFinal Metrics:\")\n",
    "for metric, value in results['final_metrics'].items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nRetrieval Metrics:\")\n",
    "for metric, value in results['retrieval_metrics'].items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15bbd2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "import logging\n",
    "\n",
    "class RAGResultsPreprocessor:\n",
    "    def __init__(self, input_path: str, output_dir: str = \"preprocessed_data\"):\n",
    "        \"\"\"Initialize preprocessor with paths\"\"\"\n",
    "        self.input_path = Path(input_path)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Setup logging\n",
    "        logging.basicConfig(\n",
    "            filename=self.output_dir / 'preprocessing.log',\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def _load_json(self, path: str) -> List[Dict]:\n",
    "        \"\"\"Load JSON file with error handling\"\"\"\n",
    "        try:\n",
    "            with open(path, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading {path}: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _extract_qa_pairs(self, data: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Extract just Q&A pairs for ground truth\"\"\"\n",
    "        qa_pairs = []\n",
    "        for item in data:\n",
    "            try:\n",
    "                qa_pairs.append({\n",
    "                    'question': item['question'],\n",
    "                    'answer': item['answer'].split('###')[0].strip()  # Take only the main answer, before any headers\n",
    "                })\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Error processing QA pair: {str(e)}\")\n",
    "                continue\n",
    "        return qa_pairs\n",
    "    \n",
    "    def _extract_rag_results(self, data: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Extract RAG results in the expected format\"\"\"\n",
    "        rag_results = []\n",
    "        for item in data:\n",
    "            try:\n",
    "                result = {\n",
    "                    'question': item['question'],\n",
    "                    'answer': item.get('answer', ''),\n",
    "                    'retrieval_time': float(item.get('retrieval_time', 0)),\n",
    "                    'avg_distance': float(item.get('avg_distance', 1.0)),\n",
    "                    'retrieved_docs': []\n",
    "                }\n",
    "                \n",
    "                # Process retrieved docs if present\n",
    "                if 'retrieved_docs' in item:\n",
    "                    for doc in item['retrieved_docs']:\n",
    "                        result['retrieved_docs'].append({\n",
    "                            'content': str(doc.get('content', '')),\n",
    "                            'distance': float(doc.get('distance', 1.0)),\n",
    "                            'chunk_id': str(doc.get('chunk_id', ''))\n",
    "                        })\n",
    "                \n",
    "                rag_results.append(result)\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Error processing RAG result: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return rag_results\n",
    "    \n",
    "    def preprocess(self) -> Dict[str, str]:\n",
    "        \"\"\"Preprocess the data and save both ground truth and RAG results\"\"\"\n",
    "        try:\n",
    "            # Load input data\n",
    "            data = self._load_json(self.input_path)\n",
    "            self.logger.info(f\"Loaded {len(data)} items from {self.input_path}\")\n",
    "            \n",
    "            # Extract and save ground truth\n",
    "            qa_pairs = self._extract_qa_pairs(data)\n",
    "            ground_truth_path = self.output_dir / \"ground_truth.json\"\n",
    "            with open(ground_truth_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(qa_pairs, f, indent=2, ensure_ascii=False)\n",
    "            self.logger.info(f\"Saved {len(qa_pairs)} QA pairs to {ground_truth_path}\")\n",
    "            \n",
    "            # Extract and save RAG results\n",
    "            rag_results = self._extract_rag_results(data)\n",
    "            rag_results_path = self.output_dir / \"rag_results.json\"\n",
    "            with open(rag_results_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(rag_results, f, indent=2, ensure_ascii=False)\n",
    "            self.logger.info(f\"Saved {len(rag_results)} RAG results to {rag_results_path}\")\n",
    "            \n",
    "            return {\n",
    "                'ground_truth_path': str(ground_truth_path),\n",
    "                'rag_results_path': str(rag_results_path)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during preprocessing: {str(e)}\")\n",
    "            raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b15fb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\prana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\prana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Evaluating queries:   0%|                                                                       | 0/40 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating queries:   2%|█▌                                                             | 1/40 [00:07<04:40,  7.20s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating queries:   5%|███▏                                                           | 2/40 [00:13<04:02,  6.38s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating queries:   8%|████▋                                                          | 3/40 [00:19<03:57,  6.43s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating queries:  10%|██████▎                                                        | 4/40 [00:25<03:51,  6.43s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating queries:  12%|███████▉                                                       | 5/40 [00:32<03:47,  6.51s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating queries:  15%|█████████▍                                                     | 6/40 [00:39<03:49,  6.74s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating queries:  18%|███████████                                                    | 7/40 [00:47<03:57,  7.19s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating queries:  20%|████████████▌                                                  | 8/40 [00:54<03:44,  7.03s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating queries:  22%|██████████████▏                                                | 9/40 [01:00<03:24,  6.60s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating queries:  25%|███████████████▌                                              | 10/40 [01:06<03:12,  6.41s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating queries:  28%|█████████████████                                             | 11/40 [01:11<02:59,  6.20s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating queries:  30%|██████████████████▌                                           | 12/40 [01:18<02:52,  6.17s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating queries:  32%|████████████████████▏                                         | 13/40 [01:24<02:50,  6.32s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating queries:  35%|█████████████████████▋                                        | 14/40 [01:30<02:39,  6.13s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating queries:  38%|███████████████████████▎                                      | 15/40 [01:36<02:31,  6.08s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating queries:  40%|████████████████████████▊                                     | 16/40 [01:42<02:26,  6.09s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating queries:  42%|██████████████████████████▎                                   | 17/40 [01:48<02:20,  6.12s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating queries:  45%|███████████████████████████▉                                  | 18/40 [01:55<02:19,  6.34s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries:  48%|█████████████████████████████▍                                | 19/40 [02:01<02:10,  6.21s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating queries:  50%|███████████████████████████████                               | 20/40 [02:06<01:59,  5.97s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating queries:  52%|████████████████████████████████▌                             | 21/40 [02:13<01:57,  6.19s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating queries:  55%|██████████████████████████████████                            | 22/40 [02:21<02:01,  6.73s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating queries:  57%|███████████████████████████████████▋                          | 23/40 [02:26<01:47,  6.35s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating queries:  60%|█████████████████████████████████████▏                        | 24/40 [02:33<01:42,  6.39s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating queries:  62%|██████████████████████████████████████▊                       | 25/40 [02:39<01:34,  6.30s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating queries:  65%|████████████████████████████████████████▎                     | 26/40 [02:45<01:24,  6.06s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating queries:  68%|█████████████████████████████████████████▊                    | 27/40 [02:51<01:21,  6.29s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating queries:  70%|███████████████████████████████████████████▍                  | 28/40 [02:58<01:15,  6.31s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating queries:  72%|████████████████████████████████████████████▉                 | 29/40 [03:04<01:07,  6.16s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating queries:  75%|██████████████████████████████████████████████▌               | 30/40 [03:09<01:00,  6.09s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating queries:  78%|████████████████████████████████████████████████              | 31/40 [03:15<00:53,  5.89s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating queries:  80%|█████████████████████████████████████████████████▌            | 32/40 [03:21<00:46,  5.87s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating queries:  82%|███████████████████████████████████████████████████▏          | 33/40 [03:26<00:40,  5.79s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating queries:  85%|████████████████████████████████████████████████████▋         | 34/40 [03:31<00:33,  5.60s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating queries:  88%|██████████████████████████████████████████████████████▎       | 35/40 [03:37<00:28,  5.63s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating queries:  90%|███████████████████████████████████████████████████████▊      | 36/40 [03:44<00:23,  5.98s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating queries:  92%|█████████████████████████████████████████████████████████▎    | 37/40 [03:49<00:17,  5.69s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating queries:  95%|██████████████████████████████████████████████████████████▉   | 38/40 [03:54<00:11,  5.53s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries:  98%|████████████████████████████████████████████████████████████▍ | 39/40 [03:59<00:05,  5.44s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating queries: 100%|██████████████████████████████████████████████████████████████| 40/40 [04:05<00:00,  6.14s/it]\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = RAGResultsPreprocessor(\n",
    "    input_path='hybrid_evaluation/qna_hybrid.json',\n",
    "    output_dir='preprocessed_data'\n",
    ")\n",
    "\n",
    "# Preprocess data\n",
    "paths = preprocessor.preprocess()\n",
    "\n",
    "# Use the preprocessed data with the evaluator\n",
    "evaluator = RAGEvaluator(\n",
    "    ground_truth_path=paths['ground_truth_path'],\n",
    "    rag_results_path=paths['rag_results_path'],\n",
    "    output_dir='rag_evaluation'\n",
    ")\n",
    "results = evaluator.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25535a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Metrics:\n",
      "bleu: 0.3026\n",
      "rouge1_f: 0.3608\n",
      "rouge2_f: 0.3593\n",
      "rougeL_f: 0.3608\n",
      "faithfulness: 0.4075\n",
      "hallucination_rate: 0.5925\n",
      "coverage: 0.8204\n",
      "\n",
      "Retrieval Metrics:\n",
      "avg_retrieval_time: 14.1316\n",
      "avg_distance: 0.8642\n"
     ]
    }
   ],
   "source": [
    "# Print summary\n",
    "print(\"\\nFinal Metrics:\")\n",
    "for metric, value in results['final_metrics'].items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nRetrieval Metrics:\")\n",
    "for metric, value in results['retrieval_metrics'].items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50f9e19",
   "metadata": {},
   "source": [
    "# Hallucination n Faithfullness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9976e9",
   "metadata": {},
   "source": [
    "### by measuring the RAG answer against retreived chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d41ba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Dict, List, Set\n",
    "from tqdm import tqdm\n",
    "\n",
    "class FaithfulnessCalculator:\n",
    "    def __init__(self, rag_results_path: str):\n",
    "        \"\"\"Initialize calculator with path to RAG results\"\"\"\n",
    "        self.rag_results_path = Path(rag_results_path)\n",
    "        \n",
    "        # Setup logging\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        self.sim_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "        # Initialize NLTK\n",
    "        try:\n",
    "            nltk.download('punkt', quiet=True)\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"NLTK download error: {str(e)}\")\n",
    "        \n",
    "        # Common words to ignore\n",
    "        self.stop_words = set(['i', 'cannot', 'find', 'the', 'answer', 'in', 'provided', 'context', \n",
    "                             'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'has', 'he',\n",
    "                             'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the', 'to', 'was', \n",
    "                             'were', 'will', 'with', '.', ',', ':', ';', '(', ')', '[', ']'])\n",
    "        \n",
    "    def clean_text(self, text: str) -> Set[str]:\n",
    "        \"\"\"Clean and tokenize text, returning set of meaningful tokens\"\"\"\n",
    "        # Convert to lowercase and tokenize\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        \n",
    "        # Remove stop words and punctuation, keep only meaningful words\n",
    "        return {token for token in tokens \n",
    "                if token not in self.stop_words \n",
    "                and token.isalnum() \n",
    "                and len(token) > 1}  # Filter out single characters\n",
    "    \n",
    "    def calculate_metrics_for_response(self, \n",
    "                                     answer: str, \n",
    "                                     retrieved_docs: List[Dict]) -> Dict[str, float]:\n",
    "        \"\"\"Calculate faithfulness metrics for a single response\"\"\"\n",
    "        # Handle \"cannot find answer\" case\n",
    "        if \"cannot find the answer in the provided context\" in answer.lower():\n",
    "            return {\n",
    "                'faithfulness': 1.0,\n",
    "                'hallucination_rate': 0.0,\n",
    "                'supported_tokens': set(),\n",
    "                'unsupported_tokens': set()\n",
    "            }\n",
    "\n",
    "        # Get answer tokens\n",
    "        answer_tokens = self.clean_text(answer)\n",
    "        \n",
    "        # Get context tokens from all retrieved docs\n",
    "        context = \" \".join(doc['content'] for doc in retrieved_docs)\n",
    "        context_tokens = self.clean_text(context)\n",
    "        \n",
    "        # Calculate supported and unsupported tokens\n",
    "        supported_tokens = answer_tokens.intersection(context_tokens)\n",
    "        unsupported_tokens = answer_tokens - context_tokens\n",
    "        \n",
    "        # Calculate metrics\n",
    "        total_tokens = len(answer_tokens) if answer_tokens else 1\n",
    "        faithfulness = len(supported_tokens) / total_tokens\n",
    "        hallucination_rate = len(unsupported_tokens) / total_tokens\n",
    "        \n",
    "        return {\n",
    "            'faithfulness': faithfulness,\n",
    "            'hallucination_rate': hallucination_rate,\n",
    "            'supported_tokens': supported_tokens,\n",
    "            'unsupported_tokens': unsupported_tokens\n",
    "        }\n",
    "    \n",
    "    def calculate_coverage(self, answer: str, retrieved_docs: List[Dict]) -> float:\n",
    "    \"\"\"Calculate coverage using sentence embeddings similarity\"\"\"\n",
    "    try:\n",
    "        context = \" \".join([doc['content'] for doc in retrieved_docs])\n",
    "        answer_emb = self.sim_model.encode([answer])\n",
    "        context_emb = self.sim_model.encode([context])\n",
    "        similarity = torch.nn.functional.cosine_similarity(\n",
    "            torch.tensor(answer_emb), \n",
    "            torch.tensor(context_emb)\n",
    "        ).item()\n",
    "        return float(similarity)\n",
    "    except Exception as e:\n",
    "        self.logger.error(f\"Error calculating coverage: {str(e)}\")\n",
    "        return 0.0\n",
    "    \n",
    "    def analyze_rag_results(self) -> Dict:\n",
    "        \"\"\"Analyze entire RAG results file and calculate average metrics\"\"\"\n",
    "        try:\n",
    "            # Load RAG results\n",
    "            with open(self.rag_results_path, 'r', encoding='utf-8') as f:\n",
    "                rag_results = json.load(f)\n",
    "            \n",
    "            self.logger.info(f\"Loaded {len(rag_results)} RAG results\")\n",
    "            \n",
    "            # Track metrics\n",
    "            total_faithfulness = 0\n",
    "            total_hallucination = 0\n",
    "            results_with_details = []\n",
    "            \n",
    "            # Process each result\n",
    "            for result in tqdm(rag_results, desc=\"Analyzing responses\"):\n",
    "                metrics = self.calculate_metrics_for_response(\n",
    "                    result['answer'],\n",
    "                    result.get('retrieved_docs', [])\n",
    "                )\n",
    "                \n",
    "                # Add to totals\n",
    "                total_faithfulness += metrics['faithfulness']\n",
    "                total_hallucination += metrics['hallucination_rate']\n",
    "                \n",
    "                # Store detailed results\n",
    "                results_with_details.append({\n",
    "                    'question': result['question'],\n",
    "                    'metrics': {\n",
    "                        'faithfulness': metrics['faithfulness'],\n",
    "                        'hallucination_rate': metrics['hallucination_rate'],\n",
    "                        'supported_count': len(metrics['supported_tokens']),\n",
    "                        'unsupported_count': len(metrics['unsupported_tokens'])\n",
    "                    }\n",
    "                })\n",
    "            \n",
    "            # Calculate averages\n",
    "            num_results = len(rag_results)\n",
    "            avg_metrics = {\n",
    "                'average_faithfulness': total_faithfulness / num_results,\n",
    "                'average_hallucination': total_hallucination / num_results\n",
    "            }\n",
    "            \n",
    "            self.logger.info(f\"Average Faithfulness: {avg_metrics['average_faithfulness']:.4f}\")\n",
    "            self.logger.info(f\"Average Hallucination: {avg_metrics['average_hallucination']:.4f}\")\n",
    "            \n",
    "            return {\n",
    "                'summary': avg_metrics,\n",
    "                'detailed_results': results_with_details\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error analyzing RAG results: {str(e)}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40fa59a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing responses: 100%|████████████████████████████████████████████████████████████| 40/40 [00:00<00:00, 312.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary Metrics:\n",
      "Average Faithfulness: 0.8691\n",
      "Average Hallucination: 0.1309\n",
      "\n",
      "Detailed Results Sample (first 3):\n",
      "\n",
      "Question: How does S3 on Outposts differ from standard Amazon S3, and what are the key considerations for its ...\n",
      "Faithfulness: 1.0000\n",
      "Hallucination Rate: 0.0000\n",
      "Supported Words: 0\n",
      "Unsupported Words: 0\n",
      "\n",
      "Question: Debug the following issue: An S3 on Outposts bucket policy returns an `Access Denied` error for cros...\n",
      "Faithfulness: 0.3828\n",
      "Hallucination Rate: 0.6172\n",
      "Supported Words: 49\n",
      "Unsupported Words: 79\n",
      "\n",
      "Question: A user is unable to access an S3 on Outposts bucket from a different VPC using PrivateLink. Outline ...\n",
      "Faithfulness: 1.0000\n",
      "Hallucination Rate: 0.0000\n",
      "Supported Words: 0\n",
      "Unsupported Words: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage:\n",
    "\n",
    "calculator = FaithfulnessCalculator('hybrid_evaluation/qna_combined.json')\n",
    "results = calculator.analyze_rag_results()\n",
    "\n",
    "print(\"\\nSummary Metrics:\")\n",
    "print(f\"Average Faithfulness: {results['summary']['average_faithfulness']:.4f}\")\n",
    "print(f\"Average Hallucination: {results['summary']['average_hallucination']:.4f}\")\n",
    "\n",
    "print(\"\\nDetailed Results Sample (first 3):\")\n",
    "for result in results['detailed_results'][:3]:\n",
    "    print(f\"\\nQuestion: {result['question'][:100]}...\")\n",
    "    print(f\"Faithfulness: {result['metrics']['faithfulness']:.4f}\")\n",
    "    print(f\"Hallucination Rate: {result['metrics']['hallucination_rate']:.4f}\")\n",
    "    print(f\"Supported Words: {result['metrics']['supported_count']}\")\n",
    "    print(f\"Unsupported Words: {result['metrics']['unsupported_count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d519439",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing responses: 100%|████████████████████████████████████████████████████████████| 40/40 [00:00<00:00, 131.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary Metrics:\n",
      "Average Faithfulness: 0.6282\n",
      "Average Hallucination: 0.3718\n",
      "\n",
      "Detailed Results Sample (first 3):\n",
      "\n",
      "Question: How does S3 on Outposts differ from standard Amazon S3, and what are the key considerations for its ...\n",
      "Faithfulness: 0.4792\n",
      "Hallucination Rate: 0.5208\n",
      "Supported Words: 115\n",
      "Unsupported Words: 125\n",
      "\n",
      "Question: Debug the following issue: An S3 on Outposts bucket policy returns an `Access Denied` error for cros...\n",
      "Faithfulness: 0.3354\n",
      "Hallucination Rate: 0.6646\n",
      "Supported Words: 55\n",
      "Unsupported Words: 109\n",
      "\n",
      "Question: A user is unable to access an S3 on Outposts bucket from a different VPC using PrivateLink. Outline ...\n",
      "Faithfulness: 0.5311\n",
      "Hallucination Rate: 0.4689\n",
      "Supported Words: 94\n",
      "Unsupported Words: 83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage:\n",
    "\n",
    "calculator = FaithfulnessCalculator('hybrid_evaluation/qna_content.json')\n",
    "results = calculator.analyze_rag_results()\n",
    "\n",
    "print(\"\\nSummary Metrics:\")\n",
    "print(f\"Average Faithfulness: {results['summary']['average_faithfulness']:.4f}\")\n",
    "print(f\"Average Hallucination: {results['summary']['average_hallucination']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef3e5ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing responses: 100%|█████████████████████████████████████████████████████████████| 40/40 [00:00<00:00, 99.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary Metrics:\n",
      "Average Faithfulness: 0.3773\n",
      "Average Hallucination: 0.6227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage:\n",
    "\n",
    "calculator = FaithfulnessCalculator('hybrid_evaluation/qna_hybrid.json')\n",
    "results = calculator.analyze_rag_results()\n",
    "\n",
    "print(\"\\nSummary Metrics:\")\n",
    "print(f\"Average Faithfulness: {results['summary']['average_faithfulness']:.4f}\")\n",
    "print(f\"Average Hallucination: {results['summary']['average_hallucination']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83beb175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c91431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93d143c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import logging\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfec36e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
